{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9c0744",
   "metadata": {},
   "source": [
    "### creating chat memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/_t6p92zx3t528nckxnsfts4c0000gn/T/ipykernel_33777/218750787.py:59: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import re\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# -------------------------------------\n",
    "# Step 1: Q&A Data from JSON\n",
    "# -------------------------------------\n",
    "with open(\"merged_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "# Combine each Q&A pair into a single document\n",
    "documents = []\n",
    "for qa in qa_data:\n",
    "    content = f\"Question: {qa[0]}\\nAnswer: {qa[1]}\"\n",
    "    documents.append(Document(page_content=content))\n",
    "\n",
    "# 1. Configure a splitter that only subdivides if a chunk exceeds 1000 chars.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # target <=1 000 chars per chunk\n",
    "    chunk_overlap=200,      # keep 200 chars overlap for context\n",
    "    length_function=len,    # measure by character count\n",
    ")\n",
    "\n",
    "# 2. Apply it—short docs (under 1000 chars) remain intact; long ones get split.\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(documents=chunked_docs, embedding=embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.4,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33440c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "import re\n",
    "\n",
    "def print_answer(answer):\n",
    "    # Split on end‑of‑sentence punctuation (keeping the punctuation)\n",
    "    sentences = re.split(r'(?<=[\\?\\.\\!])\\s+', answer.strip())\n",
    "\n",
    "    # Print each sentence on its own line\n",
    "    for s in sentences:\n",
    "        print(s)\n",
    "def gen_llm():\n",
    "    contextualize_q_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\{context}\"\"\"\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"\n",
    "        You are a senior technical interviewer.\n",
    "\n",
    "        Ask from user an interview question, based on the Relevant Knowledge‑Base Excerpts and given input:\n",
    "        {input}\n",
    "        if you don't find a question related to the input, just say that you don't know.\n",
    "        {context}\n",
    "        Question:\n",
    "        \"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm():\n",
    "    contextualize_q_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.{context}'\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")])\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "    qa_system_prompt = \"\"\"\n",
    "        You are a senior technical interviewer assistant.\n",
    "\n",
    "        Interview Question:\n",
    "        {input}\n",
    "\n",
    "        Candidate’s Answer:\n",
    "        {user_answer}\n",
    "\n",
    "        Relevant Knowledge‑Base Excerpts:\n",
    "        {context}\n",
    "        Evaluate the candidate’s answer based on the following criteria:\n",
    "            a) Score the candidate’s answer from 1 (poor) to 10 (excellent).  \n",
    "            b) Provide concise, actionable feedback on how to improve.  \n",
    "            c) Supply the “model” (ideal) answer.  \n",
    "        Format your response as:\n",
    "            Score: <number>  \n",
    "            Feedback: <text>  \n",
    "            Model Answer: <text>\n",
    "\n",
    "        Begin now.\n",
    "        \"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"system\", \"{input}\"),\n",
    "        (\"human\", \"Candidate’s Answer: {user_answer}\")])\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25b58c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "import re\n",
    "\n",
    "def print_answer(answer):\n",
    "    # Split on end‑of‑sentence punctuation (keeping the punctuation)\n",
    "    sentences = re.split(r'(?<=[\\?\\.\\!])\\s+', answer.strip())\n",
    "\n",
    "    # Print each sentence on its own line\n",
    "    for s in sentences:\n",
    "        print(s)\n",
    "def comunicate_llm():\n",
    "    contextualize_q_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\{context}\"\"\"\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"\n",
    "        You are a senior technical interviewer.\n",
    "        communicate with the user, based on the Relevant Knowledge‑Base Excerpts and given input and return Answer to the users:\n",
    "        {input}\n",
    "        if you don't find a question related to the input, just say that you don't know.\n",
    "        {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31010bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "question = \"Generate new question about data science\"\n",
    "generated_question = gen_llm().invoke({\"input\":question,\"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), generated_question[\"answer\"]])\n",
    "question = generated_question['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some common techniques for handling missing data in a dataset?\n"
     ]
    }
   ],
   "source": [
    "print_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd3d339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 3  \n",
      "Feedback: The candidate's answer does not address the question about handling missing data techniques.\n",
      "Instead, they listed libraries without explaining how they relate to the topic.\n",
      "To improve, the candidate should focus on specific techniques such as imputation methods, removal of missing values, or using algorithms that support missing data.\n",
      "Model Answer: Common techniques for handling missing data include:  \n",
      "1.\n",
      "**Removing Rows/Columns**: If the missing data is minimal, you can remove the affected rows or columns.\n",
      "2.\n",
      "**Mean/Median/Mode Imputation**: Filling in missing values with the mean, median, or mode of the column.\n",
      "3.\n",
      "**Predictive Imputation**: Using machine learning algorithms to predict and fill in missing values based on other data.\n",
      "4.\n",
      "**K-Nearest Neighbors (KNN)**: Using the KNN algorithm to impute missing values based on the nearest neighbors.\n",
      "5.\n",
      "**Multiple Imputation**: Creating multiple datasets with different imputed values and averaging the results.\n",
      "6.\n",
      "**Flagging Missing Values**: Creating a new feature that indicates whether a value was missing or not.\n"
     ]
    }
   ],
   "source": [
    "answer = \"\"\"Ptorch, scikit learn, numpy and pandas Library.\"\"\"\n",
    "evaluate = evaluate_llm().invoke({\"input\": question, \"context\": \"\", \"user_answer\": answer, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=answer), evaluate[\"answer\"]])\n",
    "print_answer(evaluate[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ece5d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Generate new question about deep learning\"\n",
    "generated_question = gen_llm().invoke({\"input\":question,\"chat_history\": chat_history, \"context\": \"\"})\n",
    "chat_history.extend([HumanMessage(content=question), generated_question[\"answer\"]])\n",
    "question = generated_question['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76ce2887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the main challenges associated with training deep learning models, and how can they be addressed?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ee0fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Generate new question about data science', additional_kwargs={}, response_metadata={}),\n",
       " 'What are some common techniques for handling missing data in a dataset?',\n",
       " HumanMessage(content='Ptorch, scikit learn, numpy and pandas Library.', additional_kwargs={}, response_metadata={}),\n",
       " \"Score: 3  \\nFeedback: The candidate's answer does not address the question about handling missing data techniques. Instead, they listed libraries without explaining how they relate to the topic. To improve, the candidate should focus on specific techniques such as imputation methods, removal of missing values, or using algorithms that support missing data.  \\nModel Answer: Common techniques for handling missing data include:  \\n1. **Removing Rows/Columns**: If the missing data is minimal, you can remove the affected rows or columns.  \\n2. **Mean/Median/Mode Imputation**: Filling in missing values with the mean, median, or mode of the column.  \\n3. **Predictive Imputation**: Using machine learning algorithms to predict and fill in missing values based on other data.  \\n4. **K-Nearest Neighbors (KNN)**: Using the KNN algorithm to impute missing values based on the nearest neighbors.  \\n5. **Multiple Imputation**: Creating multiple datasets with different imputed values and averaging the results.  \\n6. **Flagging Missing Values**: Creating a new feature that indicates whether a value was missing or not.\",\n",
       " HumanMessage(content='Generate new question about deep learning', additional_kwargs={}, response_metadata={}),\n",
       " 'What are the key differences between Convolutional Neural Networks (CNNs) and Fully Connected Neural Networks (FCNs), and in what scenarios would you choose one over the other?',\n",
       " HumanMessage(content='Generate new question about deep learning', additional_kwargs={}, response_metadata={}),\n",
       " 'What are the main challenges associated with training deep learning models, and how can they be addressed?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "151b33d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, LingoNet is a lightweight transformer‑based language model designed specifically for on‑device text autocompletion on mobile.\n",
      "During training, LingoNet was fine‑tuned on a multilingual corpus spanning English, Spanish, and Mandarin to support cross‑language suggestions.\n"
     ]
    }
   ],
   "source": [
    "question = \"do you know about LingoNet?\"\n",
    "generated_question = comunicate_llm().invoke({\"input\":question,\"chat_history\": chat_history, \"context\": \"\"})\n",
    "chat_history.extend([HumanMessage(content=question), generated_question[\"answer\"]])\n",
    "response = generated_question['answer']\n",
    "print_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6d48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"can I test LingoNet on a France dataset?\"\n",
    "generated_question = comunicate_llm().invoke({\"input\":question,\"chat_history\": chat_history, \"context\": \"\"})\n",
    "chat_history.extend([HumanMessage(content=question), generated_question[\"answer\"]])\n",
    "response = generated_question['answer']\n",
    "print_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5e8ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, LingoPer is similar to LingoNet but additionally fine-tuned on Persian language data.\n"
     ]
    }
   ],
   "source": [
    "question = \"do you know anything about LingoPer?\"\n",
    "generated_question = comunicate_llm().invoke({\"input\":question,\"chat_history\": chat_history, \"context\": \"\"})\n",
    "chat_history.extend([HumanMessage(content=question), generated_question[\"answer\"]])\n",
    "response = generated_question['answer']\n",
    "print_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf8bf3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LingoPer is a language model that builds upon the architecture of LingoNet, which is designed for text autocompletion.\n",
      "The key distinction is that LingoPer has been specifically fine-tuned on data in the Persian language.\n",
      "This fine-tuning allows it to better understand and generate text in Persian, making it more effective for applications that require Persian language processing.\n",
      "It leverages the transformer architecture to provide accurate and contextually relevant suggestions for text input, similar to how LingoNet operates for other languages.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain more LingoPer\"\n",
    "generated_question = comunicate_llm().invoke({\"input\":question,\"chat_history\": chat_history, \"context\": \"\"})\n",
    "chat_history.extend([HumanMessage(content=question), generated_question[\"answer\"]])\n",
    "response = generated_question['answer']\n",
    "print_answer(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
