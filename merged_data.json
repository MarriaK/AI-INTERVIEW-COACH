[
    [
        "What are some real-life applications of clustering algorithms?",
        "Clustering algorithms are used in various real-life applications such as:Customer segmentation for targeted marketingRecommendation systems for personalized suggestionsAnomaly detection in fraud preventionImage compression to reduce storage Healthcare for grouping patients with similar conditionsDocument categorization in search engines"
    ],
    [
        "How to choose an optimal number of clusters?",
        "Elbow Method: Plot the explained variance or within-cluster sum of squares (WCSS) against the number of clusters. The “elbow” point, where the curve starts to flatten, indicates the optimal number of clusters.Silhouette Score: Measures how similar each point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters. The optimal number of clusters is the one with the highest average silhouette score.Gap Statistic: Compares the clustering result with a random clustering of the same data. A larger gap between the real and random clustering suggests a more appropriate number of clusters."
    ],
    [
        "What is feature engineering? How does it affect the model’s performance?",
        "Feature engineering refers to developing some new features by using existing features. Sometimes there is a very subtle mathematical relation between some features which if explored properly then the new features can be developed using those mathematical operations. Also, there are times when multiple pieces of information are clubbed and provided as a single data column. At those times developing new features and using them help us to gain deeper insights into the data as well as if the features derived are significant enough helps to improve the model’s performance a lot."
    ],
    [
        "What is overfitting in machine learning and how can it be avoided?",
        "Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. To avoid overfitting there are multiple methods that we can use:Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on.Using regularization methods like L1 or L2 regularization which is used to penalize the model’s weights to avoid overfitting."
    ],
    [
        "Why we cannot use linear regression for a classification task?",
        "The main reason why we cannot use linear regression for a classification task is that the output of linear regression is continuous and unbounded, while classification requires discrete and bounded output values. If we use linear regression for the classification task the error function graph will not be convex. A convex graph has only one minimum which is also known as the global minima but in the case of the non-convex graph, there are chances of our model getting stuck at some local minima which may not be the global minima. To avoid this situation of getting stuck at the local minima we do not use the linear regression algorithm for a classification task."
    ],
    [
        "Why do we perform normalization?",
        "To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values. If we do not perform normalization then there are chances that the gradient will not converge to the global or local minima and end up oscillating back and forth."
    ],
    [
        "What is the difference between precision and recall?",
        "Precision is the ratio between the true positives(TP) and all the positive examples (TP+FP) predicted by the model. In other words, precision measures how many of the predicted positive examples are actually true positives. It is a measure of the model’s ability to avoid false positives and make accurate positive predictions.[Tex]\\text{Precision}=\\frac{TP}{TP\\; +\\; FP}[/Tex]In recall, we calculate the ratio of true positives (TP) and the total number of examples (TP+FN) that actually fall in the positive class. Recall measures how many of the actual positive examples are correctly identified by the model. It is a measure of the model’s ability to avoid false negatives and identify all positive examples correctly.[Tex]\\text{Recall}=\\frac{TP}{TP\\; +\\; FN}[/Tex]"
    ],
    [
        "What is the difference between upsampling and downsampling?",
        "In upsampling method, we increase the number of samples in the minority class by randomly selecting some points from the minority class and adding them to the dataset repeat this process till the dataset gets balanced for each class. But, here is a disadvantage the training accuracy becomes high as in each epoch model trained more than once in each epoch but the same high accuracy is not observed in the validation accuracy. In downsampling, we decrease the number of samples in the majority class by selecting some random number of points that are equal to the number of data points in the minority class so that the distribution becomes balanced. In this case, we have to suffer from data loss which may lead to the loss of some critical information as well."
    ],
    [
        "What is data leakage and how can we identify it?",
        "If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage. This is because when we train our model with that highly correlated feature then the model gets most of the target variable’s information in the training process only and it has to do very little to achieve high accuracy. In this situation, the model gives pretty decent performance both on the training as well as the validation data but as we use that model to make actual predictions then the model’s performance is not up to the mark. This is how we can identify data leakage."
    ],
    [
        "Explain the classification report and the metrics it includes.",
        "The classification report provides key metrics to evaluate a model’s performance, including:Precision: The proportion of true positives to all predicted positives, measuring accuracy of positive predictions.Recall: The proportion of true positives to all actual positives, indicating how well the model finds positive instances.F1-Score: The harmonic mean of precision and recall, balancing the two metrics.Support: The number of true instances for each class in the dataset.Accuracy: The overall proportion of correct predictions.Macro Average: The average of precision, recall, and F1-score across all classes, treating them equally.Weighted Average: The average of metrics, weighted by class support, giving more importance to frequent classes."
    ],
    [
        "What are some of the hyperparameters of the random forest regressor which help to avoid overfitting?",
        "The most important hyperparameters of a Random Forest are:max_depth: Sometimes the larger depth of the tree can create overfitting. To overcome it, the depth should be limited.n-estimator: It is the number of decision trees we want in our forest.min_sample_split: It is the minimum number of samples an internal node must hold in order to split into further nodes.max_leaf_nodes: It helps the model to control the splitting of the nodes and in turn, the depth of the model is also restricted."
    ],
    [
        "What is the bias-variance tradeoff?",
        "First, let’s understand what is bias and variance:Bias refers to the difference between the actual values and the predicted values by the model. Low bias means the model has learned the pattern in the data and high bias means the model is unable to learn the patterns present in the data i.e the underfitting.Variance refers to the change in accuracy of the model’s prediction on which the model has not been trained. Low variance is a good case but high variance means that the performance of the training data and the validation data vary a lot.If the bias is too low but the variance is too high then that case is known as overfitting. So, finding a balance between these two situations is known as the bias-variance trade-off."
    ],
    [
        "Is it always necessary to use an 80:20 ratio for the train test split?",
        "No, there is no such necessary condition that the data must be split into 80:20 ratio. The main purpose of the splitting is to have some data which the model has not seen previously so, that we can evaluate the performance of the model. If the dataset contains let’s say 50,000 rows of data then only 1000 or maybe 2000 rows of data is enough to evaluate the model’s performance."
    ],
    [
        "What is Principal Component Analysis?",
        "PCA(Principal Component Analysis) is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly. In this algorithm, we try to preserve the variance of the original dataset up to a great extent let’s say 95%. For very high dimensional data sometimes even at the loss of 1% of the variance, we can reduce the data size significantly. By using this algorithm we can perform image compression, visualize high-dimensional data as well as make data visualization easy."
    ],
    [
        "What is one-shot learning?",
        "One-shot learning is a concept in machine learning where the model is trained to recognize the patterns in datasets from a single example instead of training on large datasets. This is useful when we haven’t large datasets. It is applied to find the similarity and dissimilarities between the two images."
    ],
    [
        "What is the difference between Manhattan Distance and Euclidean distance?",
        "Both Manhattan Distance and Euclidean distance are two distance measurement techniques. Manhattan Distance (MD) is calculated as the sum of absolute differences between the coordinates of two points along each dimension. [Tex]MD = \\left| x_1 – x_2\\right| + \\left| y_1-y_2\\right|[/Tex]Euclidean Distance (ED) is calculated as the square root of the sum of squared differences between the coordinates of two points along each dimension.[Tex]ED = \\sqrt{\\left ( x_1 – x_2 \\right )^2 + \\left ( y_1-y_2 \\right )^2}[/Tex]Generally, these two metrics are used to evaluate the effectiveness of the clusters formed by a clustering algorithm."
    ],
    [
        "What is the difference between one hot encoding and ordinal encoding?",
        "One Hot encoding and ordinal encoding both are different methods to convert categorical features to numeric ones the difference is in the way they are implemented. In one hot encoding, we create a separate column for each category and add 0 or 1 as per the value corresponding to that row. In ordinal encoding, we replace the categories with numbers from 0 to n-1 based on the order or rank where n is the number of unique categories present in the dataset. The main difference between one-hot encoding and ordinal encoding is that one-hot encoding results in a binary matrix representation of the data in the form of 0 and 1, it is used when there is no order or ranking between the dataset whereas ordinal encoding represents categories as ordinal values."
    ],
    [
        "How can you conclude about the model’s performance using the confusion matrix?",
        "Confusion matrix summarizes the performance of a classification model. In a confusion matrix, we get four types of output (in case of a binary classification problem) which are TP, TN, FP, and FN. As we know that there are two diagonals possible in a square, and one of these two diagonals represents the numbers for which our model’s prediction and the true labels are the same. Our target is also to maximize the values along these diagonals. From the confusion matrix, we can calculate various evaluation metrics like accuracy, precision, recall, F1 score, etc.Machine Learning Interview Questions and Answers"
    ],
    [
        "Explain the working principle of SVM.",
        "A data set that is not separable in different classes in one plane may be separable in another plane. This is exactly the idea behind the SVMin this a low dimensional data is mapped to high dimensional data so, that it becomes separable in the different classes. A hyperplane is determined after mapping the data into a higher dimension which can separate the data into categories. SVM model can even learn non-linear boundaries with the objective that there should be as much margin as possible between the categories in which the data has been categorized. To perform this mapping different types of kernels are used like radial basis kernel, gaussian kernel, polynomial kernel, and many others."
    ],
    [
        "What is the difference between the k-means and k-means++ algorithms?",
        "The only difference between the two is in the way centroids are initialized. In the k-means algorithm, the centroids are initialized randomly from the given points. There is a drawback in this method that sometimes this random initialization leads to non-optimized clusters due to maybe initialization of two clusters close to each other. To overcome this problem k-means++ algorithm was formed. In k-means++, the first centroid is selected randomly from the data points. The selection of subsequent centroids is based on their separation from the initial centroids. The probability of a point being selected as the next centroid is proportional to the squared distance between the point and the closest centroid that has already been selected. This guarantees that the centroids are evenly spread apart and lowers the possibility of convergence to less-than-ideal clusters. This helps the algorithm reach the global minima instead of getting stuck at some local minima. Read more about it here."
    ],
    [
        "Explain some measures of similarity which are generally used in Machine learning.",
        "Some of the most commonly used similarity measures are as follows:Cosine Similarity: By considering the two vectors in n – dimension we evaluate the cosine of the angle between the two. The range of this similarity measure varies from [-1, 1] where the value 1 represents that the two vectors are highly similar and -1 represents that the two vectors are completely different from each other.Euclidean or Manhattan Distance: These two values represent the distances between the two points in an n-dimensional plane. The only difference between the two is in the way the two are calculated.Jaccard Similarity: It is also known as IoU or Intersection over union it is widely used in the field of object detection to evaluate the overlap between the predicted bounding box and the ground truth bounding box."
    ],
    [
        "Whether decision tree or random forest is more robust to the outliers.",
        "Decision trees and random forests are both relatively robust to outliers. A random forest model is an ensemble of multiple decision trees so, the output of a random forest model is an aggregate of multiple decision trees. So, when we average the results the chances of overfitting get reduced. Hence we can say that the random forest models are more robust to outliers."
    ],
    [
        "What is the difference between L1 and L2 regularization?",
        "What is their significance?L1 regularization (Lasso regularization)adds the sum of the absolute values of the model’s weights to the loss function. This penalty encourages sparsity in the model by pushing the weights of less important features to exactly zero. As a result, L1 regularization automatically performs feature selection, removing irrelevant or redundant features from the model, which can improve interpretability and reduce overfitting.L2 regularization (Ridge regularization) in which we add the square of the weights to the loss function. In both of these regularization methods, weights are penalized but there is a subtle difference between the objective they help to achieve. In L2 regularization the weights are not penalized to 0 but they are near zero for irrelevant features. It is often used to prevent overfitting by shrinking the weights towards zero, especially when there are many features and the data is noisy."
    ],
    [
        "What is a radial basis function?",
        "RBF (radial basis function) is a real-valued function used in machine learning whose value only depends upon the input and fixed point called the center. The formula for the radial basis function is as follows:[Tex]K\\left ( x,\\; {x}^{ }\\right )=exp\\left ( -\\frac{\\left\\|x-{x}^{ } \\right\\|^2}{2\\sigma ^2} \\right )[/Tex]Machine learning systems frequently use the RBF function for a variety of functions, including:RBF networks can be used to approximate complex functions. By training the network’s weights to suit a set of input-output pairs, RBF networks can be used for unsupervised learning to locate data groups. By treating the RBF centers as cluster centers,RBF networks can be used for classification tasks by training the network’s weights to divide inputs into groups based on how far from the RBF nodes they are.It is one of the very famous kernels which is generally used in the SVM algorithm to map low dimensional data to a higher dimensional plane so, we can determine a boundary that can separate the classes in different regions of those planes with as much margin as possible."
    ],
    [
        "Explain SMOTE method used to handle data imbalance.",
        "In SMOTE, we synthesize new data points using the existing ones from the minority classes by using linear interpolation. The advantage of using this method is that the model does not get trained on the same data. But the disadvantage of using this method is that it adds undesired noise to the dataset and can lead to a negative effect on the model’s performance."
    ],
    [
        "Does the accuracy score always a good metric to measure the performance of a classification model?",
        "No, there are times when we train our model on an imbalanced dataset the accuracy score is not a good metric to measure the performance of the model. In such cases, we use precision and recall to measure the performance of a classification model. Also, f1-score is another metric that can be used to measure performance but in the end, f1-score is also calculated using precision and recall as the f1-score is nothing but the harmonic mean of the precision and recall."
    ],
    [
        "What is KNN Imputer and how does it work?",
        "KNN Imputer imputes missing values in a dataset compared to traditional methods like using mean, median, or mode. It is based on the K-Nearest Neighbors (KNN) algorithm, which fills missing values by referencing the values of the nearest neighbors.Here’s how it works:Neighborhood-based Imputation: The KNN Imputer identifies the k nearest neighbors to the data point with the missing value, based on a distance metric (e.g., Euclidean distance).Imputation Process: Once the nearest neighbors are found, the missing value is imputed (filled) using a statistical measure, such as the mean or median, of the values from these neighbors.Distance Parameter: The k parameter is used to define how many neighbors to consider when imputing a missing value, and the distance metric controls how similarity is measured between data points."
    ],
    [
        "Explain the working procedure of the XGBoost model.",
        "XGBoost model is an ensemble technique of machine learning in this method weights are optimized in a sequential manner by passing them to the decision trees. After each pass, the weights become better and better as each tree tries to optimize the weights, and finally, we obtain the best weights for the problem at hand. Techniques like regularized gradient and mini-batch gradient descent have been used to implement this algorithm so, that it works in a very fast and optimized manner."
    ],
    [
        "What is the purpose of splitting a given dataset into training and validation data?",
        "The main purpose is to keep some data left over on which the model has not been trained so, that we can evaluate the performance of our machine learning model after training. Also, sometimes we use the validation dataset to choose among the multiple state-of-the-art machine learning models. Like we first train some models let’s say LogisticRegression, XGBoost, or any other than test their performance using validation data and choose the model which has less difference between the validation and the training accuracy."
    ],
    [
        "Explain some methods to handle missing values in that data.",
        "Some of the methods to handle missing values are as follows:Removing the rows with null values may lead to the loss of some important information.Removing the column having null values if it has very less valuable information. it may lead to the loss of some important information.Imputing null values with descriptive statistical measures like mean, mode, and median.Using methods like KNN Imputer to impute the null values in a more sophisticated way."
    ],
    [
        "What is the difference between k-means and the KNN algorithm?",
        "K-means algorithm is one of the popular unsupervised machine learning algorithms which is used for clustering purposes. But, KNN is a model which is generally used for the classification task and is a supervised machine learning algorithm. The k-means algorithm helps us to label the data by forming clusters within the dataset."
    ],
    [
        "What is Linear Discriminant Analysis?",
        "Linear Discriminant Analysis (LDA) is a supervised machine learning dimensionality reduction technique because it uses target variables also for dimensionality reduction. It is commonly used for classification problems. The LDA mainly works on two objectives:Maximize the distance between the means of the two classes.Minimize the variation within each class."
    ],
    [
        "How can we visualize high-dimensional data in 2-d?",
        "One of the most common and effective methods is by using the t-SNE algorithm which is a short form for t-Distributed Stochastic Neighbor Embedding. This algorithm uses some non-linear complex methods to reduce the dimensionality of the given data. We can also use PCA or LDA to convert n-dimensional data to 2 – dimensional so, that we can plot it to get visuals for better analysis. But the difference between the PCA and t-SNE is that the former tries to preserve the variance of the dataset but the t-SNE tries to preserve the local similarities in the dataset."
    ],
    [
        "What is the reason behind the curse of dimensionality?",
        "As the dimensionality of the input data increases the amount of data required to generalize or learn the patterns present in the data increases. For the model, it becomes difficult to identify the pattern for every feature from the limited number of datasets or we can say that the weights are not optimized properly due to the high dimensionality of the data and the limited number of examples used to train the model. Due to this after a certain threshold for the dimensionality of the input data, we have to face the curse of dimensionality."
    ],
    [
        "Which metric is more robust to outliers: MAE, MSE, or RMSE?",
        "Out of the three metrics—Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)—MAE is more robust to outliers.The reason behind this is the way each metric handles error values:MSE and RMSE both square the error values. When there are outliers, the error is typically large, and squaring it results in even larger error values. This causes outliers to disproportionately affect the overall error, leading to misleading results and potentially distorting the model’s performance.MAE, on the other hand, takes the absolute value of the errors. Since it does not square the error terms, the influence of large errors (outliers) is linear rather than exponential, making MAE less sensitive to outliers."
    ],
    [
        "Why removing highly correlated features are considered a good practice?",
        "When two features are highly correlated, they may provide similar information to the model, which may cause overfitting. If there are highly correlated features in the dataset then they unnecessarily increase the dimensionality of the feature space and sometimes create the problem of the curse of dimensionality. If the dimensionality of the feature space is high then the model training may take more time than expected, it will increase the complexity of the model and chances of error. This somehow also helps us to achieve data compression as the features have been removed without much loss of data."
    ],
    [
        "What is the difference between the content-based and collaborative filtering algorithms of recommendation systems?",
        "In a content-based recommendation system, similarities in the content and services are evaluated, and then by using these similarity measures from past data we recommend products to the user. But on the other hand in collaborative filtering, we recommend content and services based on the preferences of similar users. For example, if one user has taken A and B services in past and a new user has taken service A then service A will be recommended to him based on the other user’s preferences."
    ],
    [
        "How you would assess the goodness-of-fit for a linear regression model?",
        "Which metrics would you consider most important and why?To evaluate the performance of a linear regression model, important key metrics are: R-squared, Adjusted R-squared, RMSE, and F-Statistics. R-squared is particularly important as it reflects the proportion of variance in the dependent variable that can be explained by the independent variables, providing a measure of how well our model fits the data. However, Adjusted R-squared also plays a crucial role, especially when comparing models with different numbers of predictors. It adjusts for the complexity of the model, helping to prevent overfitting and ensuring the robustness of our findings.To learn more about regression metrics, check out: Regression Metrics"
    ],
    [
        "What is the null hypothesis in linear regression problem?",
        "In linear regression, the null hypothesis id that there is no relationship between the independent variable(s) and the dependent variable. This is formally represented as [Tex]H_0: \\beta_1 = 0[/Tex], where [Tex]\\beta_1[/Tex] is the coefficient of the independent variable. Essentially, the null hypothesis suggests that the predictor variable does not contribute to predicting the outcome. For instance, if the null hypothesis states that the slope of the regression line is zero, then a student’s score in an English class would not be a useful predictor of their overall grade-point average.The alternative hypothesis, denoted as [Tex]H_1: \\beta_1 \\neq 0[/Tex], proposes that changes in the independent variable are indeed associated with changes in the dependent variable, indicating a meaningful relationship."
    ],
    [
        "Can SVMs be used for both classification and regression tasks?",
        "Yes, Support Vector Machines (SVMs) can be used for both classification and regression. For classification, SVMs work by finding a hyperplane that separates different classes in the data with the largest gap possible. For regression, which involves predicting a continuous number, SVMs are adapted into a version called Support Vector Regression (SVR). SVR tries to fit as many data points as possible within a certain range of the predicted line, allowing some errors but penalizing those that are too large. This makes it useful for predicting values in situations where the data shows complex patterns.To learn how to implement Support Vector Regression, you can refer to: Support Vector Regression (SVR) using Linear and Non-Linear Kernels in Scikit Learn"
    ],
    [
        "Explain the concept of weighting in KNN?",
        "What are the different ways to assign weights, and how do they affect the model’s predictions?Weighting in KNN assigns different levels of importance to the neighbors based on their distance from the query point, influencing how each neighbor affects the model’s predictions.The weights can be assigned using: Uniform Weighting: All neighbors have equal weight regardless of their distance.Distance Weighting: Weights are inversely proportional to the distance, giving closer neighbors more influence.User-defined Weights: Weights are assigned based on domain knowledge or specific data characteristics.Effect on Model’s Prediction: Uniform Weighting: Simple but may not perform well with noisy data or varied distances.Distance Weighting: Improves accuracy by emphasizing closer neighbors, useful for irregular class boundaries but sensitive to anomalies.User-defined Weights: Optimizes performance when specific insights about the dataset are applied, though less generalizable."
    ],
    [
        "What are the assumptions behind the K-means algorithm?",
        "How do these assumptions affect the results?The assumptions of K-Means algorithm include: Cluster Shape: Assumes clusters are spherical and of similar size, affecting how well it handles non-spherical groups.Scale of Features: Assumes features are on similar scales; different ranges can distort the distance calculation.Clusters are Balanced: Assumes clusters have a roughly equal number of observations, which can bias results against smaller clusters.Similar Density: Assumes all clusters have similar density, impacting the algorithm’s effectiveness with clusters of varying densities.If these assumptions are not met, the model will perform poorly making difficult to process and select clustering techniques that align with the data characteristics. Check out the article: K Means Clustering Assumptions"
    ],
    [
        "Can you explain the concept of convergence in K-means?",
        "What conditions must be met for K-means to converge?Convergence in K-means occurs when the cluster centroids stabilize, and the assignment of data points to clusters no longer changes. This happens when the algorithm has minimized the sum of squared distances between points and their corresponding centroids.Conditions for K-means to Converge:Proper Initialization: The initial placement of centroids significantly impacts convergence. Techniques like k-means++ help ensure a better start. Data Characteristics: The algorithm converges more effectively if the data naturally clusters into well-separated groups. Overlapping or complex cluster shapes can hinder convergence.Correct Number of Clusters (k): Choosing the right number of clusters is critical; too many or too few can lead to slow convergence or convergence to suboptimal solutions.Algorithm Parameters: Setting a maximum number of iterations and a small tolerance for centroid change helps prevent infinite loops and determines when the algorithm should stop if centroids move minimally between iterations."
    ],
    [
        "What is the significance of tree pruning in XGBoost?",
        "How does it affect the model?Tree pruning in XGBoost is used to reduce model complexity and prevent overfitting. XGBoost implements a “pruning-as-you-grow” strategy where it starts by growing a full tree up to a maximum depth, then prunes back the branches that contribute minimal gains in terms of loss reduction. This is guided by the gamma parameter, which sets a minimum loss reduction required to make further partitions on a leaf node.Effect on the Model:Reduces Overfitting: By trimming unnecessary branches, pruning helps in creating simpler models that generalize better to unseen data, reducing the likelihood of overfitting.Improves Performance: Pruning helps in removing splits that have little impact, which can enhance the model’s performance by focusing on more significant attributes.Optimizes Computational Efficiency: It decreases the complexity of the final model, which can lead to faster training and prediction times as there are fewer nodes to traverse during decision making."
    ],
    [
        "How does Random Forest ensure diversity among the trees in the model?",
        "Random Forest ensures diversity among the trees in its ensemble through two main mechanisms:Bootstrap Aggregating (Bagging): Each tree in a Random Forest is trained on a different bootstrap sample, a random subset of the data. This sampling with replacement means that each tree sees different portions of the data, leading to variations in their learning and decision-making processes.Feature Randomness: When splitting a node during the construction of the tree, Random Forest randomly selects a subset of features instead of using all available features. This variation in the feature set ensures that trees do not follow the same paths or use the same splits, thereby increasing the diversity among the trees.The diversity among trees reduces the variance of the model without significantly increasing the bias."
    ],
    [
        "What is the concept of information gain in decision trees?",
        "How does it guide the creation of the tree structure?Information gain is a measure used in decision trees to select the best feature that splits the data into the most informative subsets. It is calculated based on the reduction in entropy or impurity after a dataset is split on an attribute. Entropy is a measure of the randomness or uncertainty in the data set, and information gain quantifies how much splitting on a particular attribute reduces that randomness."
    ],
    [
        "How does the independence assumption affect the accuracy of a Naive Bayes classifier?",
        "Naive Bayes classifier operates under the assumption that all features in the dataset are independent of each other given the class label. This assumption simplifies the computation of the classifier’s probability model, as it allows the conditional probability of the class given multiple features to be calculated as the product of the individual probabilities for each feature.Affect of accuracy on a Naive Bayes classifier: Strengths in High-Dimensional Data: In practice, the independence assumption can sometimes lead to good performance, especially in high-dimensional settings like text classification, despite the interdependencies among features. This is because the errors in probability estimates may cancel out across the large number of features.Limitations Due to Feature Dependency: The accuracy of Naive Bayes can be adversely affected when features are not independent, particularly if the dependencies between features are strong and critical to predicting the class. The model may underperform in such scenarios because it fails to capture the interactions between features.Generalization Capability: The simplistic nature of the independence assumption often allows Naive Bayes to perform well on smaller datasets or in cases where data for training is limited, as it does not require as complex a model as other classifiers."
    ],
    [
        "Why does PCA maximize the variance in the data?",
        "PCA aims to maximize the variance because variance represents how much information is spread out in a given direction. The higher the variance along a direction, the more information that direction holds about the data. By focusing on the directions of highest variance, PCA helps us:Preserve information while reducing the dimensionality.Simplify the data by eliminating less important features (those with low variance)"
    ],
    [
        "How do you evaluate the effectiveness of a machine learning model in an imbalanced dataset scenario?",
        "What metrics would you use instead of accuracy?We can use Precision, Recall, F1 score and ROC-AUC to evaluate the effectiveness of machine learning model in imbalanced dataset scenario. The best metric is F1 score as it combines both precision and recall into single metric that is important in imbalanced datasets where a high number of true negatives can skew accuracy. By focusing on both false positives and false negatives, the F1-score ensures that both the positive class detection and false positives are accounted for.If the cost of false positives (Type I errors) and false negatives (Type II errors) is similar, F1-Score strikes a good balance.It is especially useful when you need to prioritize performance in detecting the minority class (positive class).However, if you are more concerned about false positives or false negatives specifically, you may opt for:Precision (if false positives are more costly) orRecall (if false negatives are more costly)."
    ],
    [
        "How the One-Class SVM algorithm works for anomaly detection?",
        "One-Class SVM is an unsupervised anomaly detection algorithm. It is often used when only normal data is available. The model learns a decision boundary around normal data points using a kernel, typically an RBF, to map the data into a higher-dimensional space. The algorithm identifies support vectors—data points closest to the boundary—and any new data point outside this boundary is flagged as an anomaly. Key parameters like nu’ control the fraction of outliers allowed, while the kernel defines the boundary shape."
    ],
    [
        "Explain the concept of “concept drift” in anomaly detection.",
        "Concept driftrefers to the change in the underlying distribution or patterns in the data over time, which can make previously normal data points appear as anomalies. In anomaly detection, this is particularly challenging because a model trained on old data may not recognize new, evolving patterns as part of the normal data distribution. Concept drift can occur suddenly or gradually and needs to be monitored closely. To address this, models can be adapted through periodic retraining with new data or by using adaptive anomaly detection algorithms."
    ],
    [
        "How would you design a system for real-time recommendations for a large e-commerce platform?",
        "Firstly, I would consider using collaborative filtering techniques for recommendation. The system architecture would include Kafka for real-time data streaming from user interactions, and Apache Spark for processing these data streams. The model would be deployed on a scalable cloud environment like AWS. Data pipelines would be established to periodically retrain the model using the latest data."
    ],
    [
        "Explain how gradient boosting works and when you would use it in a machine learning system.",
        "Gradient boosting is an ensemble technique that builds models sequentially, each correcting errors made by previous models. Models are added until no further improvements can be made. It's particularly useful in scenarios of high variance and bias, such as predicting customer churn. I would use it in systems where predictive accuracy is crucial and computational resources are adequate."
    ],
    [
        "What is the difference between bagging and boosting in machine learning?",
        "Bagging involves training multiple models in parallel on different subsets of the data and then averaging their predictions to reduce variance. Boosting, however, trains models sequentially to correct the predecessor's errors, primarily reducing bias. I would choose bagging for stability and boosting for performance."
    ],
    [
        "Describe a system design for a machine learning model that predicts stock prices.",
        "The system would integrate with financial markets data through APIs to fetch real-time stock prices. I would use a combination of LSTM neural networks to predict future prices based on historical data. The model would run on a cloud platform with high-compute capabilities to ensure real-time processing and predictions."
    ],
    [
        "How would you ensure that your machine learning system is scalable?",
        "To ensure scalability, I would deploy the model in a containerized environment using Docker and orchestrate these containers with Kubernetes. This setup allows for dynamic scaling up or down based on system demand. Additionally, I would ensure data pipelines are built with scalability in mind, using technologies like Apache Kafka."
    ],
    [
        "Discuss the trade-offs between using SQL vs.",
        "NoSQL databases in machine learning systems. SQL databases provide structured data storage and powerful query capabilities, ideal for applications requiring complex queries and where data integrity is critical. NoSQL databases offer flexibility and scalability, better for unstructured data or when reading and writing speeds are prioritized. The choice depends on the specific needs of the machine learning application."
    ],
    [
        "What are some ways to handle missing data in a dataset during preprocessing?",
        "Handling missing data can be approached by: Imputation: Replacing missing values with the mean, median, or mode of the column. Deletion: Removing rows or columns with missing data, which is feasible if the loss of data isn't significant. Prediction: Using other data points to predict missing values, typically through regression or an ML model."
    ],
    [
        "How would you design a fraud detection system using machine learning?",
        "A fraud detection system design would involve: Data Collection: Aggregating transaction data in real-time. Feature Engineering: Creating features that effectively capture behavior indicative of fraud. Model Training: E mploying algorithms like Random Forest or Neural Networks to detect potential fraud. Deployment: Real-time scoring of transactions using a cloud-hosted model. Feedback Loop: Incorporating a system for feedback to continuously refine the model based on new fraud patterns."
    ],
    [
        "Explain the concept of \"Feature Importance\" in machine learning models.",
        "Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable. It is critical for understanding the model's decision-making and for reducing the model's complexity by eliminating unimportant features."
    ],
    [
        "How can you use machine learning to improve the accuracy of a demand forecasting system?",
        "To improve accuracy, I would: Data Enrichment: Incorporate external data sources like weather or economic indicators. Model Selection: Experiment with different models to find the best fit for the data complexity. Hyperparameter Tuning: Use techniques like grid search to find the optimal settings for the chosen model. Ensemble Methods: Combine predictions from multiple models to improve reliability."
    ],
    [
        "Describe the process of training a machine learning model on large datasets.",
        "Training on large datasets involves: Data Splitting: Dividing the data into training, validation, and test sets. Batch Processing: Using mini-batch gradient descent to optimize the learning process. Parallel Processing: Leveraging distributed computing frameworks like Apache Spark to train models in parallel across multiple nodes."
    ],
    [
        "How would you evaluate the performance of a machine learning model?",
        "Performance evaluation could be done by: Cross-Validation: Using techniques like k-fold cross-validation to ensure the model's effectiveness across different subsets of the data. Performance Metrics: Depending on the problem type (classification or regression), using appropriate metrics like accuracy, precision, recall, F1-score, RMSE. A/B Testing: Deploying the model in a controlled test to compare its predictions against known outcomes."
    ],
    [
        "Explain the use of convolutional neural networks in image recognition.",
        "Convolutional Neural Networks (CNNs) are particularly effective for image recognition tasks because they can automatically detect important features without any human supervision. The layers capture hierarchically higher-level features, making CNNs ideal for tasks like facial recognition and object detection."
    ],
    [
        "Discuss how you would implement a natural language processing system that can understand context.",
        "To implement a context-aware NLP system, I would use: BERT: A pre-trained model that uses transformers to understand the context of a word in a sentence. Fine-tuning: Adapting the BERT model to the specific context of the application by further training on a domain-specific dataset."
    ],
    [
        "What are the considerations when deploying a machine learning model into production?",
        "Key considerations include: Model Monitoring: Setting up systems to monitor model performance over time. Scalability: Ensuring the infrastructure can handle increased load. Data Drift: I mplementing mechanisms to detect and adapt to changes in the input data."
    ],
    [
        "How would you optimize a machine learning model that is underfitting the training data?",
        "To address underfitting, I would: Increase Model Complexity: Switching to a more complex model or adding more features. Reduce Regularization: Decreasing the regularization parameter to allow the model to fit the data more closely. Feature Engineering: Adding interaction terms or polynomial features to capture more complex relationships."
    ],
    [
        "Describe the steps to ensure the security of data in a machine learning pipeline.",
        "Ensuring data security involves: Data Encryption: Encrypting data both at rest and in transit. Access Controls: Implementing strict access controls and authentication protocols. Auditing: Regularly auditing access logs and data accesses."
    ],
    [
        "How do you handle data imbalance in a classification problem?",
        "To handle data imbalance, techniques include: Resampling: Either oversampling the minority class or undersampling the majority class. Synthetic Data Generation: Using methods like SMOTE to generate synthetic samples. Algorithmic Adjustments: Adjusting the decision threshold or using cost-sensitive learning."
    ],
    [
        "What are some challenges you face when using deep learning models in production?",
        "Challenges include: Computational Resources: Deep learning models require substantial computational resources, necessitating powerful GPUs and infrastructure. Model Interpretability: Deep learning models are often considered black boxes, making them difficult to interpret and troubleshoot. Data Requirements: They require large amounts of labeled data for training, which can be expensive and time-consuming to gather."
    ],
    [
        "How would you use reinforcement learning in a system that adapts to user behavior?",
        "In a reinforcement learning setup, the system would learn from interactions with users by receiving rewards for actions that lead to favorable outcomes. This approach is ideal for systems like personalized content recommendation, where the model adapatops based on continuous feedback from user interactions."
    ],
    [
        "How would you design a machine learning system to detect anomalies in network traffic?",
        "For designing a system to detect network anomalies, I would: Data Collection: Capture real-time network traffic data using packet sniffers and log aggregators. Feature Engineering: Extract features such as packet size, traffic volume, and connection rate. Model Selection: Use unsupervised learning algorithms like Isolation Forest or Autoencoders, which are effective for anomaly detection. Deployment: Deploy the model in a real-time environment where it continuously monitors network traffic. Alert System: Implement an alert system to notify network administrators when potential anomalies are detected."
    ],
    [
        "Explain how you would use machine learning to optimize supply chain operations.",
        "To optimize supply chain operations using machine learning, I would: Data Integration: Collect data from various points in the supply chain, including inventory levels, delivery times, and demand forecasts. Predictive Analytics: Use regression models or time series forecasting to predict future demand and supply conditions. Optimization Algorithms: Implement optimization algorithms to suggest optimal ordering quantities and routing of deliveries. Simulation: Use simulation techniques to test different supply chain scenarios and their outcomes. Continuous Learning: Set up the system to refine predictions and recommendations based on new data and outcomes."
    ],
    [
        "Describe a machine learning approach to improve customer retention.",
        "A machine learning approach to improving customer retention would include: Customer Data Analysis: Gather comprehensive data on customer interactions, purchase history, and feedback. Churn Prediction Model: Develop a classification model using algorithms like XGBoost or Random Forest to predict the likelihood of customers churning. Feature Importance: Analyze which features most significantly impact churn, such as customer service interactions, product usage frequency, and pricing sensitivity. Intervention Strategies: Use the model's predictions to implement targeted intervention strategies for customers at high risk of churn, such as personalized offers or proactive customer support. Model Monitoring: Regularly monitor and update the model to adapt to changing customer behavior and feedback."
    ],
    [
        "What machine learning techniques would you use to improve search engine relevance?",
        "Improving search engine relevance can be approached by: Data Collection: Gather data on user queries, clicks, and feedback to understand user intent. Natural Language Processing: Employ NLP techniques to process and understand the text within queries and documents. Relevance Models: Develop models such as RankNet, a pair-wise ranking model, or more advanced deep learning models to predict the relevance of documents to a query. Feature Engineering: Incorporate user-specific and context-specific features to personalize search results. Continuous Learning: Implement online learning algorithms that continuously update the model based on new user interactions."
    ],
    [
        "How would you design a system to predict and prevent churn in a subscription-based service?",
        "For predicting and preventing churn in a subscription-based service, the system design would include: Customer Data Analysis: Collect comprehensive data on customer usage patterns, subscription details, and previous interactions. Feature Engineering: Create predictive features from raw data, such as usage frequency, changes in usage patterns, and customer feedback. Predictive Modeling: Utilize survival analysis or classification models like XGBoost to predict the likelihood of churn for each customer. Intervention Strategies: Based on model predictions, implement targeted interventions such as personalized offers or proactive customer support to retain high-risk customers. Model Evaluation and Update: Regularly evaluate the model’s performance and update it with new data to maintain its accuracy over time. Comment More info Advertise with us Next Article Top 50+ Machine Learning Interview Questions and Answers S sherlowkey Follow Improve Article Tags : Machine Learning System Design AI-ML-DS Practice Tags : Machine Learning Similar Reads Top 25 Machine Learning System Design Interview Questions Machine Learning System Design Interviews are critical for evaluating a candidate's ability to design scalable and efficient machine learning systems. These interviews test a mix of technical skills, problem-solving abilities, and system thinking. Candidates might face questions about designing reco 9 min read Machine Learning Interview Question & Answers Machine Learning involves the development of algorithms and statistical models that enable computers to improve their performance in tasks through experience. Machine Learning is one of the booming careers in the present-day scenario. If you are preparing for machine learning interview, this intervi 15+ min read How to Answer a System Design Interview Problem/Question? System design interviews are crucial for software engineering roles, especially senior positions. These interviews assess your ability to architect scalable, efficient systems. Unlike coding interviews, they focus on overall design, problem-solving, and communication skills. You need to understand r 5 min read System Design Tutorial for Machine Learning System design in machine learning is vital for scalability, performance, and efficiency. It ensures effective data management, model deployment, monitoring, and resource optimization, while also addressing security, privacy, and regulatory compliance. A well-designed system enables seamless integrat 5 min read Deep Learning Interview Questions Deep learning is a part of machine learning that is based on the artificial neural network with multiple layers to learn from and make predictions on data. An artificial neural network is based on the structure and working of the Biological neuron which is found in the brain. This Deep Learning Inte 15+ min read Google System Design Interview Questions Preparing for a Google System Design interview requires a deep understanding of designing scalable, fault-tolerant, and efficient systems. This article explores key system design concepts, questions, and strategies to help you ace the interview and build scalable solutions across millions of users. 14 min read Most Commonly Asked System Design Interview Problems/Questions This System Design Interview Guide will provide the most commonly asked system design interview questions and equip you with the knowledge and techniques needed to design, build, and scale your robust applications, for professionals and newbies Below are a list of most commonly asked interview probl 2 min read How to Crack System Design Interview Round? In the System Design Interview round, You will have to give a clear explanation about designing large scalable distributed systems to the interviewer. This round may be challenging and complex for you because you are supposed to cover all the topics and tradeoffs within this limited time frame, whic 9 min read Design Patterns in Machine Learning for MLOps Machine learning (ML) is revolutionizing industries by enabling data-driven decision-making and automation. However, developing, deploying, and maintaining machine learning models in production environments presents a unique set of challenges. This is where MLOps (Machine Learning Operations) comes 5 min read 10 Must Have Machine Learning Engineer Skills in 2024 In Today's world we can see that machine learning is growing very rapidly. Machine Learning engineers are in high demand as more and more companies are adopting machine learning technology in their domain to remain competitive in the market. If you want to become a successful Machine Learning Engine 7 min read Like Corporate & Communications Address: A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305) Registered Address: K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305 Advertise with us Company About Us Legal Privacy Policy In Media Contact Us Advertise with us GFG Corporate Solution Placement Training Program GeeksforGeeks Community Languages Python Java C++ PHP GoLang SQL R Language Android Tutorial Tutorials Archive DSA Data Structures Algorithms DSA for Beginners Basic DSA Problems DSA Roadmap Top 100 DSA Interview Problems DSA Roadmap by Sandeep Jain All Cheat Sheets Data Science & ML Data Science With Python Data Science For Beginner Machine Learning ML Maths Data Visualisation Pandas NumPy NLP Deep Learning Web Technologies HTML CSS JavaScript TypeScript ReactJS NextJS Bootstrap Web Design Python Tutorial Python Programming Examples Python Projects Python Tkinter Web Scraping OpenCV Tutorial Python Interview Question Django Computer Science Operating Systems Computer Network Database Management System Software Engineering Digital Logic Design Engineering Maths Software Development Software Testing DevOps Git Linux AWS Docker Kubernetes Azure GCP DevOps Roadmap System Design High Level Design Low Level Design UML Diagrams Interview Guide Design Patterns OOAD System Design Bootcamp Interview Questions Inteview Preparation Competitive Programming Top DS or Algo for CP Company-Wise Recruitment Process Company-Wise Preparation Aptitude Preparation Puzzles School Subjects Mathematics Physics Chemistry Biology Social Science English Grammar Commerce World GK GeeksforGeeks Videos DSA Python Java C++ Web Development Data Science CS Subjects @GeeksforGeeks, Sanchhaya Education Private Limited , All rights reserved We use cookies to ensure you have the best browsing experience on our website. By using our site, you acknowledge that you have read and understood our Cookie Policy & Privacy Policy Got It ! Improvement Suggest changes Suggest Changes Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal. Create Improvement Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all. Suggest Changes min 4 words, max Words Limit:1000 Thank You! Your suggestions are valuable to us. What kind of Experience do you want to share? Interview Experiences Admission Experiences Career Journeys Work Experiences Campus Experiences Competitive Exam Experiences"
    ],
    [
        "Explain the difference between supervised and unsupervised machine learning",
        "In supervised machine learning algorithms, we have to provide labeled data, for example, prediction of stock market prices, whereas in unsupervised we need not have labeled data, for example, classification of emails into spam and non-spam."
    ],
    [
        "What are the parametric models",
        "Give an example. Parametric models are those with a finite number of parameters. To predict new data, you only need to know the parameters of the model. Examples include linear regression, logistic regression, and linear SVMs. Non-parametric models are those with an unbounded number of parameters, allowing for more flexibility. To predict new data, you need to know the parameters of the model and the state of the data that has been observed. Examples include decision trees, k-nearest neighbors, and topic models using latent Dirichlet analysis."
    ],
    [
        "What is the difference between classification and regression",
        "Classification is used to produce discrete results, classification is used to classify data into some specific categories. For example, classifying emails into spam and non-spam categories. Whereas, We use regression analysis when we are dealing with continuous data, for example predicting stock prices at a certain point in time."
    ],
    [
        "What Is Overfitting, and How Can You Avoid It",
        "Overfitting is a situation that occurs when a model learns the training set too well, taking up random fluctuations in the training data as concepts. These impact the model’s ability to generalize and don’t apply to new data. When a model is given the training data, it shows 100 percent accuracy—technically a slight loss. But, when we use the test data, there may be an error and low efficiency. This condition is known as overfitting. There are multiple ways of avoiding overfitting, such as:   Regularization. It involves a cost term for the features involved with the objecti ve function   Making a simple model. With lesser variables and parameters, the variance can be reduced   Cross-validation methods like k-folds can also be used   If some model parameters are likely to cause overfitting, techniques for regularization like LASSO can be used that penalize these parameters"
    ],
    [
        "What is meant by  Training set’ and  Test Set’",
        "We split the given data set into two different sections namely,’Training set’ and  Test Set’.  Training set’ is the portion of the dataset used to train the model.  Testing set’ is the portion of the dataset used to test the trained model."
    ],
    [
        "How Do You Handle Missing or Corrupted Data in a Dataset",
        "One of the easiest ways to handle missing or corrupted data is to drop those rows or columns or replace them entirely with some other value. There are two useful methods in Pandas:   IsNull() and dropna() will help to find the columns/rows with missing data and drop them   Fillna() will replace the wrong values with a placeholder value"
    ],
    [
        "Explain Ensemble learning",
        "In ensemble learning, many base models like classifiers and regressors are generated and combined together so that they give better results. It is used when we build component classifiers that are accurate and independent. There are sequential as well as parallel ensemble methods."
    ],
    [
        "Explain the Bias-Variance Tradeoff",
        "Predictive models have a tradeoff between bias (how well the model fits the data) and variance (how much the model changes based on changes in the inputs). Simpler models are stable (low variance) but they don't get close to the truth (high bias). More complex models are more prone to overfitting (high variance) but they are expressive enough to get close to the truth (low bias). The best model for a given problem usually lies somewhere in the middle."
    ],
    [
        "What is the difference between stochastic gradient descent (SGD) and gradient descent (GD)",
        "Both algorithms are methods for finding a set of parameters that minimize a loss function by evaluating parameters against data and then making adjustments. In standard gradient descent, you'll evaluate all training samples for each set of parameters. This is akin to taking big, slow steps toward the solution. In stochastic gradient descent, you'll evaluate only 1 training sample for the set of parameters before updating them. This is akin to taking small, quick steps toward the solution."
    ],
    [
        "How Can You Choose a Classifier Based on a Training Set Data Size",
        "When the training set is small, a model that has a right bias and low variance seems to work better because they are less likely to overfit. For example, Naive Bayes works best when the training set is large. Models with low bias and high variance tend to perform better as they work fine with complex relationships."
    ],
    [
        "What are 3 data preprocessing techniques to handle outliers",
        "1. Winsorize (cap at threshold). 2. Transform to reduce skew (using Box-Cox or similar). 3. Remove outliers if you're certain they are anomalies or measurement errors."
    ],
    [
        "How much data should you allocate for your training, validation, and test sets",
        "You have to find a balance, and there's no right answer for every problem. If your test set is too small, you'll have an unreliable estimation of model performance (performance statistic will have high variance). If your training set is too small, youractual model parameters will have a high variance. A good rule of thumb is to use an 80/20 train/test split. Then, your train set can be further split into train/validation or into partitions for cross-validation."
    ],
    [
        "What Is a False Positive and False Negative and How Are They Significant",
        "False positives are those cases which wrongly get classified as True but are False. False negatives are those cases which wrongly get classified as False but are True. In the term  False Positive,’ the word  Positive’ refers to the  Yes’ row of the predicted value in the confusion matrix. The complete term indicates that the system has predicted it as a positive, but the actual value is negative."
    ],
    [
        "Explain the difference between L1 and L2 regularization",
        "L2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean prior to the terms, while L2 corresponds to a Gaussian prior."
    ],
    [
        "What’s a Fourier transform",
        "A Fourier transform is a generic method to decompose generic functions into a superposition of symmetric functions. Or as this more intuitive tutorial puts it, given a smoothie, it’s how we find the recipe. The Fourier transform finds the set of cycle speeds, amplitudes, and phases to match any time signal. A Fourier transform converts a signal from time to frequency domain — it’s a very common way to extract features from audio signals or other time series such as sensor data."
    ],
    [
        "What is deep learning, and how does it contrast with other machine learning algorithms",
        "Deep learning is a subset of machine learning that is concerned with neural networks: how to use backpropagation and certain principles from neuroscience to more accurately model large sets of unlabelled or semi-structured data. In that sense, deep learning represents an unsupervised learning algorithm that learns representations of data through the use of neural nets."
    ],
    [
        "What’s the difference between a generative and discriminative model",
        "A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks."
    ],
    [
        "What Are the Applications of Supervised Machine Learning in Modern Businesses",
        "Applications of supervised machine learning include:   Email Spam Detection Here we train the model using historical data that consists of emails categorized as spam or not spam. This labeled information is fed as input to the model.   Healthcare Diagnosis By providing images regarding a disease, a model can be trained to detect if a person is suffering from the disease or not.   Sentiment Analysis This refers to the process of using algorithms to mine documents and determine whether they’re positive, neutral, or negative in sentiment.   Fraud Detection Training the model to identify suspicious patterns, we can detect instances of possible fraud."
    ],
    [
        "What Is Semi-supervised Machine Learning",
        "Supervised learning uses data that is completely labeled, whereas unsupervised learning uses no training data. In the case of semi-supervised learning, the training data contains a small amount of labeled data and a large amount of unlabeled data."
    ],
    [
        "What Are Unsupervised Machine Learning Techniques",
        "There are two techniques used in unsupervised learning: clustering and association. Clustering   Clustering problems involve data to be divided into subsets. These subsets, also called clusters, contain data that are similar to each other. Different clusters reveal different details about the objects, unlike classification or regression. Association   In an association problem, we identify patterns of associations between different variables or items.   For example, an eCommerce website can suggest other items for you to buy, based on the prior purchases that you have made, spending habits, items in your wishlist, other customers’ purchase habits, and so on."
    ],
    [
        "What Is  naive’ in the Naive Bayes Classifier",
        "The classifier is called  naive’ because it makes assumptions that may or may not turn out to be correct. The algorithm assumes that the presence of one feature of a class is not related to the presence of any other feature (absolute independence of features), given the class variable. For instance, a fruit may be considered to be a cherry if it is red in color and round in shape, regardless of other features. This assumption may or may not be right (as an apple also matches the description)."
    ],
    [
        "Explain Latent Dirichlet Allocation (LDA)",
        "Latent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying documents by subject matter. LDA is a generative model that represents documents as a mixture of topics that each have their own probability distribution of possible words. The \"Dirichlet\" distribution is simply a distribution of distributions. In LDA, documents are distributions of topics that are distributions of words."
    ],
    [
        "Explain Principle Component Analysis (PCA)",
        "PCA is a method for transforming features in a dataset by combining them into uncorrelated linear combinations. These new features, or principal components, sequentially maximize the variance represented (i.e. the first principal component has the most variance, the second principal component has the second most, and so on). As a result, PCA is useful for dimensionality reduction because you can set an arbitrary variance cutoff."
    ],
    [
        "What’s the F1 score",
        "How would you use it? The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives don’t matter much."
    ],
    [
        "When should you use classification over regression",
        "Classification produces discrete values and dataset to strict categories, while regression gives you continuous results that allow you to better distinguish differences between individual points. You would use classification over regression if you wanted your results to reflect the belongingness of data points in your dataset to certain explicit categories (ex: If you wanted to know whether a name was male or female rather than just how correlated they were with male and female names.)"
    ],
    [
        "How do you ensure you’re not overfitting with a model",
        "This is a simple restatement of a fundamental problem in machine learning: the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations. There are three main methods to avoid overfitting: 1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data. 2- Use cross-validation techniques such as k-folds cross-validation. 3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting."
    ],
    [
        "How Will You Know Which Machine Learning Algorithm to Choose for Your Classification Problem",
        "While there is no fixed rule to choose an algorithm for a classification problem, you can follow these guidelines:   If accuracy is a concern, test different algorithms and cross-validate them   If the training dataset is small, use models that have low variance and high bias   If the training dataset is large, use models that have high variance and little bias"
    ],
    [
        "How Do You Design an Email Spam Filter",
        "Building a spam filter involves the following process:   The email spam filter will be fed with thousands of emails   Each of these emails already has a label:  spam’ or  not spam.’   The supervised machine learning algorithm will then determine which type of emails are being marked as spam based on spam words like the lottery, free offer, no money, full refund, etc.   The next time an email is about to hit your inbox, the spam filter will use statistical analysis and algorithms like Decision Trees and SVM to determine how likely the email is spam   If the likelihood is high, it will label it as spam, and the email won’t hit y our inbox   Based on the accuracy of each model, we will use the algorithm with the highest accuracy after testing all the models"
    ],
    [
        "What evaluation approaches would you work to gauge the effectiveness of a machine learning model",
        "You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: here is a fairly comprehensive list. You could use measures such as the F1 score, the accuracy, and the confusion matrix. What’s important here is to demonstrate that you understand the nuances of how a model is measured and how to choose the right performance measures for the right situations."
    ],
    [
        "How would you implement a recommendation system for our company’s users",
        "A lot of machine learning interview questions of this type will involve the implementation of machine learning models to a company’s problems. You’ll have to research the company and its industry in-depth, especially the revenue drivers the company has, and the types of users the company takes on in the context of the industry it’s in."
    ],
    [
        "Explain bagging",
        "Bagging, or Bootstrap Aggregating, is an ensemble method in which the dataset is first divided into multiple subsets through resampling. Then, each subset is used to train a model, and the final predictions are made through voting or averaging the component models. Bagging is performed in parallel."
    ],
    [
        "What is the ROC Curve and what is AUC (a.k.a. AUROC)",
        "The ROC (receiver operating characteristic) the performance plot for binary classifiers of True Positive Rate (y-axis) vs. False Positive Rate (x- axis). AUC is the area under the ROC curve, and it's a common performance metric for evaluating binary classification models. It's equivalent to the expected probability that a uniformly drawn random positive is ranked before a uniformly drawn random negative."
    ],
    [
        "Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of-sample evaluation metric",
        "AUROC is robust to class imbalance, unlike raw accuracy. For example, if you want to detect a type of cancer that's prevalent in only 1% of the population, you can build a model that achieves 99% accuracy by simply classifying everyone has cancer-free."
    ],
    [
        "What are the advantages and disadvantages of neural networks",
        "Advantages : Neural networks (specifically deep NNs) have led to performance breakthroughs for unstructured datasets such as images, audio, and video. Their incredible flexibility allows them to learn patterns that no other ML algorithm can learn. Disadvantages : However, they require a large amount of training data to converge. It's also difficult to pick the right architecture, and the internal \"hidden\" layers are incomprehensible."
    ],
    [
        "Define Precision and Recall",
        "Precision   Precision is the ratio of several events you can correctly recall to the total number of events you recall (mix of correct and wrong recalls).   Precision = (True Positive) / (True Positive + False Positive) Recall   A recall is the ratio of a number of events you can recall the number of total event s.   Recall = (True Positive) / (True Positive + False Negative)"
    ],
    [
        "What Is Decision Tree Classification",
        "A decision tree builds classification (or regression) models as a tree structure, with datasets broken up into ever-smaller subsets while developing the decision tree, literally in a tree-like way with branches and nodes. Decision trees can handle both categorical and numerical data."
    ],
    [
        "What Is Pruning in Decision Trees, and How Is It Done",
        "Pruning is a technique in machine learning that reduces the size of decision trees. It reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting. Pruning can occur in:   Top-down fashion. It will traverse nodes and trim subtrees starting at the root   Bottom-up fashion. It will begin at the leaf nodes There is a popular pruning algorithm called reduced error pruning, in which:   Starting at the leaves, each node is replaced with its most popular class   If the prediction accuracy is not affected, the change is kept   There is an advantage of simplicity and speed"
    ],
    [
        "What Is a Recommendation System",
        "Anyone who has used Spotify or shopped at Amazon will recognize a recommendation system: It’s an information filtering system that predicts what a user might want to hear or see based on choice patterns provided by the user."
    ],
    [
        "What Is Kernel SVM",
        "Kernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods are a class of algorithms for pattern analysis, and the most common one is the kernel SVM."
    ],
    [
        "What Are Some Methods of Reducing Dimensionality",
        "You can reduce dimensionality by combining features with feature engineering, removing collinear features, or using algorithmic dimensionality reduction. Now that you have gone through these machine learning interview questions, you must have got an idea of your strengths and weaknesses in this domain."
    ],
    [
        "What Are the Three Stages of Building a Model in Machine Learning",
        "The three stages of building a machine learning model are:   Model BuildingChoose a suitable algorithm for the model and train it according to the requirement   Model Testing Check the accuracy of the model through the test data   Applying the Mode Make the required changes after testing and use the final model for real-time projects. Here, it’s important to remember that once in a while, the model needs to be checked to make sure it’s working correctly. It should be modified to make sure that it is up-to-date."
    ],
    [
        "How is KNN different from k-means clustering",
        "K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this really means is that in order for K-Nearest Neighbors to work, you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points."
    ],
    [
        "Mention the difference between Data Mining and Machine learning",
        "Machine learning relates to the study, design, and development of the algorithms that give computers the capability to learn without being explicitly programmed. While data mining can be defined as the process in which the unstructured data tries to extract knowledge or unknown interesting patterns. During this processing machine, learning algorithms are used."
    ],
    [
        "What are the different Algorithm techniques in Machine Learning",
        "The different types of techniques in Machine Learning are   Supervised Learning   Unsupervised Learning   Semi-supervised Learning   Reinforcement Learning   Transduction   Learning to Learn"
    ],
    [
        "You are given a data set. The data set has missing values that spread along 1 standard deviation from the median. What percentage of data would remain unaffected",
        "Why? This question has enough hints for you to start thinking! Since the data is spread across the median, let’s assume it’s a normal distribution. We know, in a normal distribution, ~68% of the data lies in 1 standard deviation from mean (or mode, median), which leaves ~32% of the data unaffected. Therefore, ~32% of the data would remain unaffected by missing values."
    ],
    [
        "What are PCA, KPCA, and ICA used for",
        "PCA (Principal Components Analysis), KPCA ( Kernel-based Principal Component Analysis) and ICA ( Independent Component Analysis) are important feature extraction techniques used for dimensionality reduction."
    ],
    [
        "What are support vector machines",
        "Support vector machines are supervised learning algorithms used for classification and regression analysis."
    ],
    [
        "What is batch statistical learning",
        "Statistical learning techniques allow learning a function or predictor from a set of observed data that can make predictions about unseen or future data. These techniques provide guarantees on the performance of the learned predictor on the future unseen data based on a statistical assumption on the data generating process."
    ],
    [
        "What is the bias-variance decomposition of classification error in the ensemble method",
        "The expected error of a learning algorithm can be decomposed into bias and variance. A bias term measures how closely the average classifier produced by the learning algorithm matches the target function. The variance term measures how much the learning algorithm’s prediction fluctuates for different training sets."
    ],
    [
        "When is Ridge regression favorable over Lasso regression",
        "You can quote ISLR’s authors Hastie, Tibshirani who asserted that, in the presence of few variables with medium / large sized effect, use lasso regression. In presence of many variables with small/medium-sized effects, use ridge regression. Conceptually, we can say, lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In the presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective."
    ],
    [
        "You’ve built a random forest model with 10000 trees. You got delighted after getting training error as 0.00. But, the validation error is 34.23. What is going on",
        "Haven’t you trained your model perfectly? The model has overfitted. Training error 0.00 means the classifier has mimicked the training data patterns to an extent, that they are not available in the unseen data. Hence, when this classifier was run on an unseen sample, it couldn’t find those patterns and returned predictions with higher error. In a random forest, it happens when we use a larger number of trees than necessary. Hence, to avoid this situation, we should tune the number of trees using cross-validation."
    ],
    [
        "What is a convex hull",
        "In the case of linearly separable data, the convex hull represents the outer boundaries of the two groups of data points. Once the convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create the greatest separation between two groups."
    ],
    [
        "What do you understand by Type I vs Type II error",
        "Type I error is committed when the null hypothesis is true and we reject it, also known as a  False Positive’. Type II error is committed when the null hypothesis is false and we acceptit, also known as  False Negative’. In the context of the confusion matrix, we can say Type I error occurs when we classify a value as positive (1) when it is actually negative (0). Type II error occurs when we classify a valueas negative (0) when it is actually positive(1)."
    ],
    [
        "In k-means or kNN, we use euclidean distance to calculate the distance between nearest neighbors. Why not manhattan distance",
        "We don’t use manhattan distance because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, the euclidean metric can be used in any space to calculate distance. Since the data points can be present in any dimension, euclidean distance is a more viable option. Example: Think of a chessboard, the movement made by a bishop or a rook is calculated by manhattan distance because of their respective vertical & horizontal movements."
    ],
    [
        "Do you suggest that treating a categorical variable as a continuous variable would result in a better predictive model",
        "For better predictions, the categorical variable can be considered as a continuous variable only when the variable is ordinal in nature."
    ],
    [
        "OLS is to linear regression",
        "The maximum likelihood is logistic regression. Explain the statement. OLS and Maximum likelihood are the methods used by the respective regression methods to approximate the unknown parameter (coefficient) value. In simple words, Ordinary least square(OLS) is a method used in linear regression which approximates the parameters resulting in minimum distance between actual and predicted values. Maximum Likelihood helps in choosing the values of parameters which maximizes the likelihood that the parameters are most likely to produce observed data."
    ],
    [
        "When does regularization becomes necessary in Machine Learning",
        "Regularization becomes necessary when the model begins to overfit/underfit. This technique introduces a cost term for bringing in more features with the objective function. Hence, it triesto push the coefficients for many variables to zero and hence reduce the cost term. This helps to reduce model complexity so that the model can become better at predicting (generalizing)."
    ],
    [
        "What is Linear Regression",
        "Linear Regression is a supervised Machine Learning algorithm. It is used to find the linear relationship between the dependent and the independent variables for predictive analysis."
    ],
    [
        "What is the Variance Inflation Factor",
        "Variance Inflation Factor (VIF) is the estimate of the volume of multicollinearity in a collection of many regression variables. VIF = Variance of the model / Variance of the model with a single independent variable We have to calculate this ratio for every independent variable. If VIF is high, then it shows the high collinearity of the independent variables."
    ],
    [
        "We know that one hot encoding increases the dimensionality of a dataset, but label encoding doesn’t. How",
        "When we use one-hot encoding , there is an increase in the dimensionality of a dataset. The reason for the increase in dimensionality is that, for every class in the categorical variables, it forms a different variable."
    ],
    [
        "What is a Decision Tree",
        "A decision tree is used to explain the sequence of actions that must be performed to get the desired output. It is a hierarchical diagram that shows the actions."
    ],
    [
        "What is the Binarizing of data",
        "How to Binarize? In most of the Machine Learning Interviews, apart from theoretical questions, interviewers focus on the implementation part. So, this ML Interview Questions focused on the implementation of the theoretical concepts. Converting data into binary values on the basis of threshold values is known as the binarizing of data. The values that are less than the threshold are set to 0 and the values that are greater than the threshold are set to 1. This process is useful when we have to perform feature engineering, and we can also use it for adding unique features."
    ],
    [
        "What is cross-validation",
        "Cross-validation is essentially a technique used to assess how well a model performs on a new independent dataset. The simplest example of cross-validation is when you split your data into two groups: training data and testing data, where you use the training data to build the model and the testing data to test the model."
    ],
    [
        "When would you use random forests Vs SVM and why",
        "There are a couple of reasons why a random forest is a better choice of the model than a support vector machine:   Random forests allow you to determine the feature importance. SVM’s can’t do this.   Random forests are much quicker and simpler to build than an SVM.   For multi-class classification problems, SVMs require a one-vs-rest method, which is less scalable and more memory intensive."
    ],
    [
        "What are the drawbacks of a linear model",
        "There are a couple of drawbacks of a linear model:   A linear model holds some strong assumptions that may not be true in the application. It assumes a linear relationship, multivariate normality, no or little multicollinearity, no auto-correlation, and homoscedasticity   A linear model can’t be used for discrete or binary outcomes.   You can’t vary the model flexibility of a linear model."
    ],
    [
        "Do you think 50 small decision trees are better than a large one",
        "Why? Another way of asking this question is “Is a random forest a better model than a decision tree?” And the answer is yes because a random forest is an ensemble method that takes many weak decision trees to make a strong learner. Random forests are more accurate, more robust, and less prone to overfitting."
    ],
    [
        "What is a kernel",
        "Explain the kernel trick A kernel is a way of computing the dot product of two vectors𝐱x and𝐲y in some (possibly very high dimensional) feature space, which is why kernel functions are sometimes called “generalized dot product” The kernel trick is a method of using a linear classifier to solve a non-linear problem by transforming linearly inseparable data to linearly separable ones in a higher dimension."
    ],
    [
        "State the differences between causality and correlation",
        "Causality applies to situations where one action, say X, causes an outcome, say Y, whereas Correlation is just relating one action (X) to another action(Y) but X does not necessarily cause Y."
    ],
    [
        "What is the exploding gradient problem while using the backpropagation technique",
        "When large error gradients accumulate and result in large changes in the neural network weights during training, it is called the exploding gradient problem. The values of weights can become so large as to overflow and result in NaN values. This makes the model unstable and the learning of the model to stall just like the vanishing gradient problem."
    ],
    [
        "What do you mean by Associative Rule Mining (ARM)",
        "Associative Rule Mining is one of the techniques to discover patterns in data like features (dimensions) which occur together and features (dimensions) which are correlated."
    ],
    [
        "What is Marginalisation",
        "Explain the process. Marginalizationarginalisation is summing the probability of a random variable X given the joint probability distribution of X with other variables. It is an application of the law of total probability."
    ],
    [
        "Why is the rotation of components so important in Principle Component Analysis(PCA)",
        "Rotation in PCA is very important as it maximizes the separation within the variance obtained by all the components because of which interpretation of components would become easier. If the components are not rotated, then we need extended components to describe the variance of the components."
    ],
    [
        "What is the difference between regularization and normalisation",
        "Normalisation adjusts the data; regularisation adjusts the prediction function. If your data is on very different scales (especially low to high), you would want to normalise the data. Alter each column to have compatible basic statistics. This can be helpful to make sure there is no loss of accuracy. One of the goals of model training is to identify the signal and ignore the noise if the model is given free rein to minimize error, there is a possibility of suffering from overfitting. Regularization imposes some control on this by providing simpler fitting functions over complex ones."
    ],
    [
        "When does the linear regression line stop rotating or finds an optimal spot where it is fitted on data",
        "A place where the highest RSquared value is found, is the place where the line comes to rest. RSquared represents the amount of variance captured by the virtual linear regression line with respect to the total variance captured by the dataset."
    ],
    [
        "How does the SVM algorithm deal with self-learning",
        "SVM has a learning rate and expansion rate which takes care of this. The learning rate compensates or penalises the hyperplanes for making all the wrong moves and expansion rate deals with finding the maximum separation area between classes."
    ],
    [
        "How do you handle outliers in the data",
        "Outlier is an observation in the data set that is far away from other observations in the data set. We can discover outliers using tools and functions like box plot, scatter plot, Z-Score, IQRscore etc. and then handle them based on the visualization we have got. To handle outliers, we can cap at some threshold, use transformations to reduce skewness of the data and remove outliers if they are anomalies or errors."
    ],
    [
        "Name and define techniques used to find similarities in the recommendation system",
        "Pearson correlation and Cosine correlation are techniques used to find similarities in recommendation systems."
    ],
    [
        "Why would you Prune your tree",
        "In the context of data science or AIML, pruning refers to the process of reducing redundant branches of a decision tree. Decision Trees are prone to overfitting, pruning the tree helps to reduce the size and minimizes the chances of overfitting. Pruning involves turning branches of a decision tree into leaf nodes and removing the leaf nodes from the original branch. It serves as a tool to perform the tradeoff."
    ],
    [
        "Mention some of the EDA Techniques",
        "Exploratory Data Analysis (EDA) helps analysts to understand the data better and forms the foundation of better models. Visualization   Univariate visualization   Bivariate visualization   Multivariate visualization Missing Value Treatmen t – Replace missing values with Either Mean/Median Outlier Detection– Use Boxplot to identify the distribution of Outliers, then Apply IQR to set the boundary for IQR"
    ],
    [
        "What is data augmentation",
        "Can you give some examples? Data augmentation is a technique for synthesizing new data by modifying existing data in such a way that the target is not changed, or it is changed in a known way. CV is one of the fields where data augmentation is very useful. There are many modifications that we can do to images:   Resize   Horizontal or vertical flip   Rotate   Add noise   Deform   Modify colors Each problem needs a customized data augmentation pipeline. For example, on OCR, doing flips will change the text and won’t be beneficial; however, resizes and small rotations may help."
    ],
    [
        "What is Inductive Logic Programming in Machine Learning (ILP)",
        "Inductive Logic Programming (ILP) is a subfield of machine learning which uses logic programming representing background knowledge and examples."
    ],
    [
        "What is the difference between inductive machine learning and deductive machine learning",
        "The difference between inductive machine learning and deductive machine learning are as follows: machine-learning where the model learns by examples from a set of observed instances to draw a generalized conclusion whereas in deductive learning the model first draws the conclusion and then the conclusion is drawn."
    ],
    [
        "Difference between machine learning and deep learning Machine learning is a branch of computer science and a method to implement artificial intelligence",
        "This technique provides the ability to automatically learn and improve from experiences without being explicitly programmed. Deep learning can be said as a subset of machine learning. It is mainly based on the artificial neural network where data is taken as an input and the technique makes intuitive decisions using the artificial neural network."
    ],
    [
        "What Are The Steps Involved In Machine Learning Project",
        "As you plan for doing a machine learning project. There are several important steps you must follow to achieve a good working model and they are data collection, data preparation, choosing a machine learning model, training the model, model evaluation, parameter tuning and lastly prediction."
    ],
    [
        "Differences between Artificial Intelligence and Machine Learning",
        "Artificial intelligence is a broader prospect than machine learning. Artificial intelligencemimics the cognitive functions of the human brain. The purpose of AI is to carry out a task in an intelligent manner based on algorithms. On the other hand, machine learning is a subclass of artificial intelligence. To develop an autonomous machine in such a way so that it can learn without being explicitly programmed is the goal of machine learning."
    ],
    [
        "Steps Needed to Choose the Appropriate Machine Learning Algorithm for your Classification problem. Firstly, you need to have a clear picture of your data, your constraints, and your problems before heading towards different machine learning algorithms. Secondly, you have to understand which type and kind of data you have because it plays a primary role in deciding which algorithm you have to use. Following this step is the data categorization step, which is a two-step process – categorization by input and categorization by output. The next step is to understand your constraints; that is, what is your data storage capacity",
        "How fast the prediction has to be? etc. Finally, find the available machine learning algorithms and implement them wisely. Along with that, also try to optimize the hyperparameters which can be done in three ways – grid search, random search, and Bayesian optimization."
    ],
    [
        "Explain Backpropagation in Machine Learning",
        "A very important question for your machine learning interview.Backpropagation is the algorithm for computing artificial neural networks (ANN). It is used by the gradient descent optimization that exploits the chain rule. By calculating the gradient of the loss function, the weight of the neurons is adjusted to a certain value. To train a multi-layered neural network is the prime motivation of backpropagation so that it can learn the appropriate internal demonstrations. This will help them learn to map any input to its respective output arbitrarily."
    ],
    [
        "What is the Convex Function",
        "This question is very often asked in machine learning interviews. A convex function is a continuous function, and the value of the midpoint at every interval in its given domain is less than the numerical mean of the values at the two ends of the interval."
    ],
    [
        "What’s the Relationship between True Positive Rate and Recall",
        "The True positive rate in machine learning is the percentage of the positives that have been properly acknowledged, and recall is just the count of the results that have been correctly identified and are relevant. Therefore, they are the same things, just having different names. It is also known as sensitivity."
    ],
    [
        "List some Tools for Parallelizing Machine Learning Algorithms",
        "Although this question may seem very easy, make sure not to skip this one because it is also very closely related to artificial intelligence and thereby, AI interview questions. Almost all machine learning algorithms are easy to serialize. Some of the basic tools for parallelizing are Matlab, Weka, R, Octave, or the Python-based sci-kit learn."
    ],
    [
        "What do you mean by Genetic Programming",
        "Genetic Programming (GP) is almost similar to an Evolutionary Algorithm, a subset of machine learning. Genetic programming software systems implement an algorithm that uses random mutation, a fitness function, crossover, and multiple generations of evolution to resolve a user-defined task. The genetic programming model is based on testing and choosing the best option among a set of results."
    ],
    [
        "What do you know about Bayesian Networks",
        "Bayesian Networks also referred to as 'belief networks' or 'casual networks', are used to represent the graphical model for probability relationship among a set of variables. For example, a Bayesian network can be used to represent the probabilistic relationships between diseases and symptoms. As per the symptoms, the network can also compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference or learning in Bayesian networks. Bayesian networks which relate the variables (e.g., speech signals or protein sequences) are called dynamic Bayesian networks."
    ],
    [
        "Which are the two components of the Bayesian logic program",
        "A Bayesian logic program consists of two components:   Logical It contains a set of Bayesian Clauses, which capture the qualitative structure of the domain.   Quantitative It is used to encode quantitative information about the domain."
    ],
    [
        "How is machine learning used in day-to-day life",
        "Most of the people are already using machine learning in their everyday life. Assume that you are engaging with the internet, you are actually expressing your preferences, likes, dislikes through your searches. All these things are picked up by cookies coming on your computer, from this, the behavior of a user is evaluated. It helps to increase the progress of a user through the internet and provide similar suggestions. The navigation system can also be considered as one of the examples where we are using machine learning to calculate a distance between two places using optimization techniques. Surely, people are going to more engage with machine learning in the near future"
    ],
    [
        "Define Sampling. Why do we need it",
        "Answer: Sampling is a process of choosing a subset from a target population that would serve as its representative. We use the data from the sample to understand the pattern in the community as a whole. Sampling is necessary because often, we can not gather or process the complete data within a reasonable time."
    ],
    [
        "What does the term decision boundary mean",
        "Answer: A decision boundary or a decision surface is a hypersurface which divides the underlying feature space into two subspaces, one for each class. If the decision boundary is a hyperplane, then the classes are linearly separable."
    ],
    [
        "Define entropy",
        "Answer: Entropy is the measure of uncertainty associated with random variable Y. It is the expected number of bits required to communicate the value of the variable."
    ],
    [
        "Indicate the top intents of machine learning",
        "Answer: The top intents of machine learning are stated below,   The system gets information from the already established computations to give well-founded decisions and outputs.   It locates certain patterns in the data and then makes certain predictions on it to provide answers on matters."
    ],
    [
        "Highlight the differences between the Generative model and the Discriminative model",
        "The aim of the Generative model is to generate new samples from the same distribution and new data instances, Whereas, the Discriminative model highlights the differences between different kinds of data instances. It tries to learn directly from the data and then classifies the data."
    ],
    [
        "Identify the most important aptitudes of a machine learning engineer",
        "Machine learning allows the computer to learn itself without being decidedly programmed. It helps the system to learn from experience and then improve from its mistakes. The intelligence system, which is based on machine learning, can learn from recorded data and past incidents. In-depth knowledge of statistics, probability, data modelling, programming language, as well as CS, Application of ML Libraries and algorithms, and software design is required to become a successful machine learning engineer."
    ],
    [
        "What is feature engineering",
        "How do you apply it in the process of modelling? Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data."
    ],
    [
        "How can learning curves help create a better model",
        "Learning curves give the indication of the presence of overfitting or underfitting. In a learning curve, the training error and cross-validating error are plotted against the number of training data points. References 1 springboard.com 2 simplilearn.com 3 geeksforgeeks.org 4 elitedatascience.com 5 analyticsvidhya.com 6 guru99.com 7 intellipaat.com 8 towardsdatascience.com 9 mygreatlearning.com 10 mindmajix.com 11 toptal.com 12 glassdoor.co.in 13 udacity.com 14 educba.com 15 analyticsindiamag.com 16 ubuntupit.com 17 javatpoint.com 18 quora.com 19 hackr.io 20 kaggle.com"
    ],
    [
        "What is Naive Bayes algorithm, When we can use this algorithm in NLP?",
        "Naive Bayes algorithm is a collection of classifiers which works on the principles of the Bayes’ theorem. This series of NLP model forms a family of algorithms that can be used for a wide range of classification tasks including sentiment prediction, filtering of spam, classifying documents and more.Naive Bayes algorithm converges faster and requires less training data. Compared to other discriminative models like logistic regression, Naive Bayes model it takes lesser time to train. This algorithm is perfect for use while working with multiple classes and text classification where the data is dynamic and changes frequently."
    ],
    [
        "Explain Dependency Parsing in NLP?",
        "Dependency Parsing, also known as Syntactic parsing in NLP is a process of assigning syntactic structure to a sentence and identifying its dependency parses. This process is crucial to understand the correlations between the “head” words in the syntactic structure. The process of dependency parsing can be a little complex considering how any sentence can have more than one dependency parses. Multiple parse trees are known as ambiguities. Dependency parsing needs to resolve these ambiguities in order to effectively assign a syntactic structure to a sentence.Dependency parsing can be used in the semantic analysis of a sentence apart from the syntactic structuring."
    ],
    [
        "What is text Summarization?",
        "Text summarization is the process of shortening a long piece of text with its meaning and effect intact. Text summarization intends to create a summary of any given piece of text and outlines the main points of the document. This technique has improved in recent times and is capable of summarizing volumes of text successfully.Text summarization has proved to a blessing since machines can summarise large volumes of text in no time which would otherwise be really time-consuming. There are two types of text summarization: Extraction-based summarization Abstraction-based summarization"
    ],
    [
        "What is NLTK?How is it different from Spacy?",
        "NLTK or Natural Language Toolkit is a series of libraries and programs that are used for symbolic and statistical natural language processing. This toolkit contains some of the most powerful libraries that can work on different ML techniques to break down and understand human language. NLTK is used for Lemmatization, Punctuation, Character count, Tokenization, and Stemming. The difference between NLTK and Spacey are as follows: While NLTK has a collection of programs to choose from, Spacey contains only the best-suited algorithm for a problem in its toolkit NLTK supports a wider range of languages compared to Spacey (Spacey supports only 7 languages) While Spacey has an object-oriented library, NLTK has a string processing library Spacey can support word vectors while NLTK cannot"
    ],
    [
        "What is information extraction?",
        "Information extraction in the context of Natural Language Processing refers to the technique of extracting structured information automatically from unstructured sources to ascribe meaning to it. This can include extracting information regarding attributes of entities, relationship between different entities and more. The various models of information extraction includes:Tagger ModuleRelation Extraction ModuleFact Extraction ModuleEntity Extraction ModuleSentiment Analysis ModuleNetwork Graph ModuleDocument Classification & Language Modeling Module"
    ],
    [
        "What is Bag of Words?",
        "Bag of Words is a commonly used model that depends on word frequencies or occurrences to train a classifier. This model creates an occurrence matrix for documents or sentences irrespective of its grammatical structure or word order."
    ],
    [
        "What is Pragmatic Ambiguity in NLP?",
        "Pragmatic ambiguity refers to those words which have more than one meaning and their use in any sentence can depend entirely on the context. Pragmatic ambiguity can result in multiple interpretations of the same sentence. More often than not, we come across sentences which have words with multiple meanings, making the sentence open to interpretation. This multiple interpretation causes ambiguity and is known as Pragmatic ambiguity in NLP."
    ],
    [
        "What is Masked Language Model?",
        "Masked language models help learners to understand deep representations in downstream tasks by taking an output from the corrupt input. This model is often used to predict the words to be used in a sentence."
    ],
    [
        "What is the difference between NLP and CI(Conversational Interface)?",
        "The difference between NLP and CI is as follows:Natural Language Processing (NLP)Conversational Interface (CI)NLP attempts to help machines understand and learn how language concepts work.CI focuses only on providing users with an interface to interact with.NLP uses AI technology to identify, understand, and interpret the requests of users through language.CI uses voice, chat, videos, images, and more such conversational aid to create the user interface."
    ],
    [
        "What are the best NLP Tools?",
        "Some of the best NLP tools from open sources are:SpaCyTextBlobTextacyNatural language Toolkit (NLTK)RetextNLP.jsStanford NLPCogcompNLP"
    ],
    [
        "What is POS tagging?",
        "Parts of speech tagging better known as POS tagging refer to the process of identifying specific words in a document and grouping them as part of speech, based on its context. POS tagging is also known as grammatical tagging since it involves understanding grammatical structures and identifying the respective component.POS tagging is a complicated process since the same word can be different parts of speech depending on the context. The same general process used for word mapping is quite ineffective for POS tagging because of the same reason."
    ],
    [
        "What is NES?",
        "Name entity recognition is more commonly known as NER is the process of identifying specific entities in a text document that are more informative and have a unique context. These often denote places, people, organizations, and more. Even though it seems like these entities are proper nouns, the NER process is far from identifying just the nouns. In fact, NER involves entity chunking or extraction wherein entities are segmented to categorize them under different predefined classes. This step further helps in extracting information. NLP Interview Questions for Experienced"
    ],
    [
        "Which of the following techniques can be used for keyword normalization in NLP, the process of converting a keyword into its base form? a. Lemmatizationb. Soundexc. Cosine Similarityd. N-grams ",
        "a)Lemmatization. helps to get to the base form of a word, e.g. are playing -> play, eating -> eat, etc. Other options are meant for different purposes."
    ],
    [
        "Which of the following techniques can be used to compute the distance between two-word vectors in NLP? a. Lemmatizationb. b. Euclidean distancec. c. Cosine Similarityd. d. N-grams",
        "b) and c)Distance between two-word vectors can be computed using Cosine similarity and Euclidean Distance.  Cosine Similarity establishes a cosine angle between the vector of two words. A cosine angle close to each other between two-word vectors indicates the words are similar and vice versa.E.g. cosine angle between two words “Football” and “Cricket” will be closer to 1 as compared to the angle between the words “Football” and “New Delhi”.Python code to implement CosineSimlarity function would look like this:def cosine_similarity(x,y): return np.dot(x,y)/( np.sqrt(np.dot(x,x)) * np.sqrt(np.dot(y,y)) )q1 = wikipedia.page(‘Strawberry’)q2 = wikipedia.page(‘Pineapple’)q3 = wikipedia.page(‘Google’)q4 = wikipedia.page(‘Microsoft’)cv = CountVectorizer()X = np.array(cv.fit_transform([q1.content, q2.content, q3.content, q4.content]).todense())print (“Strawberry Pineapple Cosine Distance”, cosine_similarity(X[0],X[1]))print (“Strawberry Google Cosine Distance”, cosine_similarity(X[0],X[2]))print (“Pineapple Google Cosine Distance”, cosine_similarity(X[1],X[2]))print (“Google Microsoft Cosine Distance”, cosine_similarity(X[2],X[3]))print (“Pineapple Microsoft Cosine Distance”, cosine_similarity(X[1],X[3]))Strawberry Pineapple Cosine Distance 0.8899200413701714Strawberry Google Cosine Distance 0.7730935582847817Pineapple Google Cosine Distance 0.789610214147025Google Microsoft Cosine Distance 0.8110888282851575Usually Document similarity is measured by how close semantically the content (or words) in the document are to each other. When they are close, the similarity index is close to 1, otherwise near 0.The Euclidean distance between two points is the length of the shortest path connecting them. Usually computed using Pythagoras theorem for a triangle."
    ],
    [
        "What are the possible features of a text corpus in NLP?",
        "a. Count of the word in a documentb. Vector notation of the wordc. Part of Speech Tagd. Basic Dependency Grammare. All of the aboveAnswer: e)All of the above can be used as features of the text corpus."
    ],
    [
        "You created a document term matrix on the input data of 20K documents for a Machine learning model.",
        "Which of the following can be used to reduce the dimensions of data?Keyword NormalizationLatent Semantic IndexingLatent Dirichlet Allocationa. only 1 b. 2, 3c. 1, 3d. 1, 2, 3Answer: d)"
    ],
    [
        "Which of the text parsing techniques can be used for noun phrase detection, verb phrase detection, subject detection, and object detection in NLP. a. Part of speech tagging b. Skip Gram and N-Gram extractionc. Continuous Bag of Words d. Dependency Parsing and Constituency Parsing",
        "d"
    ],
    [
        "Dissimilarity between words expressed using cosine similarity will have values significantly higher than 0? a. True b. False",
        "a"
    ],
    [
        "Which one of the following is keyword Normalization techniques in NLP a.Stemming b. Part of Speechc. c.Named entity recognitiond. d.Lemmatization",
        "a) and d)Part of Speech (POS) and Named Entity Recognition(NER) is not keyword Normalization techniques. Named Entity helps you extract Organization, Time, Date, City, etc., type of entities from the given sentence, whereas Part of Speech helps you extract Noun, Verb, Pronoun, adjective, etc., from the given sentence tokens."
    ],
    [
        "Which of the below are NLP use cases? a. Detecting objects from an image b. Facial Recognition c.Speech Biometric d.Text Summarization",
        "d)a) And b) are Computer Vision use cases, and c) is the Speech use case.Only d) Text Summarization is an NLP use case."
    ],
    [
        "In NLP, The process of converting a sentence or paragraph into tokens is referred to as Stemminga? True b. False",
        "b)The statement describes the process of tokenization and not stemming, hence it is False."
    ],
    [
        "In NLP, Tokens are converted into numbers before giving to any Neural Networka. a.True b.False",
        "a)In NLP, all words are converted into a number before feeding to a Neural Network."
    ],
    [
        "TF-IDF helps you to establish? a. most frequently occurring word in document b. the most important word in the document",
        "b)TF-IDF helps to establish how important a particular word is in the context of the document corpus. TF-IDF takes into account the number of times the word appears in the document and is offset by the number of documents that appear in the corpus.TF is the frequency of terms divided by the total number of terms in the document.IDF is obtained by dividing the total number of documents by the number of documents containing the term and then taking the logarithm of that quotient.Tf.idf is then the multiplication of two values TF and IDF.Suppose that we have term count tables of a corpus consisting of only two documents, as listed here:TermDocument 1 FrequencyDocument 2 FrequencyThis11is11a2 Sample1 another  2example 3The calculation of tf–idf for the term “this” is performed as follows:for \"this\"-----------tf(\"this\", d1) = 1/5 = 0.2tf(\"this\", d2) = 1/7 = 0.14idf(\"this\", D) = log (2/2) =0hence tf-idftfidf(\"this\", d1, D) = 0.2* 0 = 0tfidf(\"this\", d2, D) = 0.14* 0 = 0for \"example\"------------tf(\"example\", d1) = 0/5 = 0tf(\"example\", d2) = 3/7 = 0.43idf(\"example\", D) = log(2/1) = 0.301tfidf(\"example\", d1, D) = tf(\"example\", d1) * idf(\"example\", D) = 0 * 0.301 = 0tfidf(\"example\", d2, D) = tf(\"example\", d2) * idf(\"example\", D) = 0.43 * 0.301 = 0.129In its raw frequency form, TF is just the frequency of the “this” for each document. In each document, the word “this” appears once; but as document 2 has more words, its relative frequency is smaller.An IDF is constant per corpus, and accounts for the ratio of documents that include the word “this”. In this case, we have a corpus of two documents and all of them include the word “this”. So TF–IDF is zero for the word “this”, which implies that the word is not very informative as it appears in all documents.The word “example” is more interesting – it occurs three times, but only in the second document. To understand more about NLP, check out these NLP projects."
    ],
    [
        "In NLP, The process of identifying people, an organization from a given sentence, paragraph is called a.Stemming b. Lemmatization c. Stop word removald. Named entity recognition",
        "d"
    ],
    [
        "Which one of the following is not a pre-processing technique in NLP a.Stemming and Lemmatization b. converting to lowercase c. removing punctuations d. removal of stop words e. Sentiment analysis",
        "e)Sentiment Analysis is not a pre-processing technique. It is done after pre-processing and is an NLP use case. All other listed ones are used as part of statement pre-processing."
    ],
    [
        "In text mining, converting text into tokens and then converting them into an integer or floating-point vectors can be done usinga. a.CountVectorizer b.  TF-IDF c. Bag of Words d. NERs",
        "a)CountVectorizer helps do the above, while others are not applicable.text =[\"Rahul is an avid writer, he enjoys studying understanding and presenting. He loves to play\"]vectorizer = CountVectorizer()vectorizer.fit(text)vector = vectorizer.transform(text)print(vector.toarray())Output [[1 1 1 1 2 1 1 1 1 1 1 1 1 1]]The second section of the interview questions covers advanced NLP techniques such as Word2Vec, GloVe word embeddings, and advanced models such as GPT, Elmo, BERT, XLNET-based questions, and explanations."
    ],
    [
        "In NLP, Words represented as vectors are called Neural Word Embeddingsa. a.True b. False",
        "a)Word2Vec, GloVe based models build word embedding vectors that are multidimensional."
    ],
    [
        "In NLP, Context modeling is supported with which one of the following word embeddings a.Word2Vec b) GloVe c) BERT d) All of the above",
        "c)Only BERT (Bidirectional Encoder Representations from Transformer) supports context modelling where the previous and next sentence context is taken into consideration. In Word2Vec, GloVe only word embeddings are considered and previous and next sentence context is not considered."
    ],
    [
        "In NLP, Bidirectional context is supported by which of the following embedding a.Word2Vec b. BERT c. GloVed. All the above",
        "b)Only BERT provides a bidirectional context. The BERT model uses the previous and the next sentence to arrive at the context.Word2Vec and GloVe are word embeddings, they do not provide any context."
    ],
    [
        "Which one of the following Word embeddings can be custom trained for a specific subject in NLP a.Word2Vec b. BERT c. GloVe d. All the above",
        "b)BERT allows Transform Learning on the existing pre-trained models and hence can be custom trained for the given specific subject, unlike Word2Vec and GloVe where existing word embeddings can be used, no transfer learning on text is possible."
    ],
    [
        "Word embeddings capture multiple dimensions of data and are represented as vectors a.True b.False",
        "a)"
    ],
    [
        "In NLP, Word embedding vectors help establish distance between two tokens a.True b. False",
        "a)One can use Cosine similarity to establish the distance between two vectors represented through Word Embeddings"
    ],
    [
        "Language Biases are introduced due to historical data used during training of word embeddings, which one amongst the below is not an example of biasa. New Delhi is to India, Beijing is to China b. Man is to Computer, Woman is to Homemaker",
        "a)Statement b) is a bias as it buckets Woman into Homemaker, whereas statement a) is not a biased statement."
    ],
    [
        "Which of the following will be a better choice to address NLP use cases such as semantic similarity, reading comprehension, and common sense reasoninga.",
        "ELMob. Open AI’s GPTc. ULMFitAnswer: b)Open AI’s GPT is able to learn complex patterns in data by using the Transformer models Attention mechanism and hence is more suited for complex use cases such as semantic similarity, reading comprehensions, and common sense reasoning."
    ],
    [
        "Transformer architecture was first introduced with? a. GloVe b. BERT c. Open AI’s GPT d. ULMFit",
        "c)ULMFit has an LSTM based Language modeling architecture. This got replaced into Transformer architecture with Open AI’s GPT."
    ],
    [
        "Which of the following architecture can be trained faster and needs less amount of training data a.LSTM-based Language Modelling b. Transformer architecture",
        "b)Transformer architectures were supported from GPT onwards and were faster to train and needed less amount of data for training too."
    ],
    [
        "Same word can have multiple word embeddings possible with ____________? a. GloVeb. Word2Vecc. ELMod. nltk",
        "c)EMLo word embeddings support the same word with multiple embeddings, this helps in using the same word in a different context and thus captures the context than just the meaning of the word unlike in GloVe and Word2Vec. Nltk is not a word embedding."
    ],
    [
        "For a given token, its input representation is the sum of embedding from the token, segment and position embedding a.ELMo b. GPT c. BERT d. ULMFit",
        "c)BERT uses token, segment and position embedding."
    ],
    [
        "Trains two independent LSTM language model left to right and right to left and shallowly concatenates them. a. GPTb. BERTc. ULMFitd. ELMo",
        "d)ELMo tries to train two independent LSTM language models (left to right and right to left) and concatenates the results to produce word embedding."
    ],
    [
        "Uses unidirectional language model for producing word embedding. a. BERTb. GPTc. ELMod. Word2Vec",
        "b)GPT is a bidirectional model and word embedding is produced by training on information flow from left to right. ELMo is bidirectional but shallow. Word2Vec provides simple word embedding."
    ],
    [
        "In this architecture, the relationship between all words in a sentence is modelled irrespective of their position. Which architecture is this?a. OpenAI GPTb. ELMoc. BERTd. ULMFit",
        "c)BERT Transformer architecture models the relationship between each word and all other words in the sentence to generate attention scores. These attention scores are later used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation."
    ],
    [
        "List 10 use cases to be solved using NLP techniques?",
        "Sentiment AnalysisLanguage Translation (English to German, Chinese to English, etc..)Document SummarizationQuestion AnsweringSentence CompletionAttribute extraction (Key information extraction from the documents)Chatbot interactionsTopic classificationIntent extractionGrammar or Sentence correctionImage captioningDocument RankingNatural Language inference"
    ],
    [
        "Transformer model pays attention to the most important word in Sentence. a. True b. False",
        "a) Attention mechanisms in the Transformer model are used to model the relationship between all words and also provide weights to the most important word."
    ],
    [
        "Which NLP model gives the best accuracy amongst the following? a. BERT b. XLNET c. GPT-2 d. ELMo",
        "b) XLNETXLNET has given best accuracy amongst all the models. It has outperformed BERT on 20 tasks and achieves state of art results on 18 tasks including sentiment analysis, question answering, natural language inference, etc."
    ],
    [
        "Permutation Language models is a feature of a.BERT b. EMMo c. GPT d. XLNET",
        "d) XLNET provides permutation-based language modelling and is a key difference from BERT. In permutation language modeling, tokens are predicted in a random manner and not sequential. The order of prediction is not necessarily left to right and can be right to left. The original order of words is not changed but a prediction can be random. The conceptual difference between BERT and XLNET can be seen from the following diagram."
    ],
    [
        "Transformer XL uses relative positional embedding a.Trueb. False",
        "a)Instead of embedding having to represent the absolute position of a word, Transformer XL uses an embedding to encode the relative distance between the words. This embedding is used to compute the attention score between any 2 words that could be separated by n words before or after.There, you have it – all the probable questions for your NLP interview. Now go, give it your best shot. Natural Language Processing FAQs"
    ],
    [
        "Why do we need NLP?",
        "One of the main reasons why NLP is necessary is because it helps computers communicate with humans in natural language. It also scales other language-related tasks. Because of NLP, it is possible for computers to hear speech, interpret this speech, measure it and also determine which parts of the speech are important."
    ],
    [
        "What must a natural language program decide?",
        "A natural language program must decide what to say and when to say something."
    ],
    [
        "Where can NLP be useful?",
        "NLP can be useful in communicating with humans in their own language. It helps improve the efficiency of the machine translation and is useful in emotional analysis too. It can be helpful in sentiment analysis using python too. It also helps in structuring highly unstructured data. It can be helpful in creating chatbots, Text Summarization and virtual assistants."
    ],
    [
        "How to prepare for an NLP Interview?",
        "The best way to prepare for an NLP Interview is to be clear about the basic concepts. Go through blogs that will help you cover all the key aspects and remember the important topics. Learn specifically for the interviews and be confident while answering all the questions."
    ],
    [
        "What are the main challenges of NLP?",
        "Breaking sentences into tokens, Parts of speech tagging, Understanding the context, Linking components of a created vocabulary, and Extracting semantic meaning are currently some of the main challenges of NLP."
    ],
    [
        "Which NLP model gives best accuracy?",
        "Naive Bayes Algorithm has the highest accuracy when it comes to NLP models. It gives up to 73% correct predictions."
    ],
    [
        "What are the major tasks of NLP?",
        "Translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation are few of the major tasks of NLP. Under unstructured data, there can be a lot of untapped information that can help an organization grow."
    ],
    [
        "What are stop words in NLP?",
        "Common words that occur in sentences that add weight to the sentence are known as stop words. These stop words act as a bridge and ensure that sentences are grammatically correct. In simple terms, words that are filtered out before processing natural language data is known as a stop word and it is a common pre-processing method."
    ],
    [
        "What is stemming in NLP?",
        "The process of obtaining the root word from the given word is known as stemming. All tokens can be cut down to obtain the root word or the stem with the help of efficient and well-generalized rules. It is a rule-based process and is well-known for its simplicity."
    ],
    [
        "Why is NLP so hard?",
        "There are several factors that make the process of Natural Language Processing difficult. There are hundreds of natural languages all over the world, words can be ambiguous in their meaning, each natural language has a different script and syntax, the meaning of words can change depending on the context, and so the process of NLP can be difficult. If you choose to upskill and continue learning, the process will become easier over time."
    ],
    [
        "What does a NLP pipeline consist of *?",
        "The overall architecture of an NLP pipeline consists of several layers: a user interface; one or several NLP models, depending on the use case; a Natural Language Understanding layer to describe the meaning of words and sentences; a preprocessing layer; microservices for linking the components together and of course."
    ],
    [
        "How many steps of NLP is there?",
        "The five phases of NLP involve lexical (structure) analysis, parsing, semantic analysis, discourse integration, and pragmatic analysis.→ Explore this Curated Program for You ← Great Learning Editorial TeamThe Great Learning Editorial Staff includes a dynamic team of subject matter experts, instructors, and education professionals who combine their deep industry knowledge with innovative teaching methods. Their mission is to provide learners with the skills and insights needed to excel in their careers, whether through upskilling, reskilling, or transitioning into new fields.Recommended for you Top 6 Artificial Intelligence Documentaries in 2025 | AI DocumentariesUpdated on Jan 6, 202511465 What are Expert Systems in Artificial Intelligence?Updated on Feb 6, 202591633 How to Start a Career in Artificial Intelligence and Machine Learning in 2025?Updated on Jan 6, 20252032 What are the Most In-demand Skills in Artificial Intelligence in 2025?Updated on Jan 6, 20253020 10 Best Artificial Intelligence BooksUpdated on Feb 6, 202594882 TOP NLP Projects in 2025Updated on Jan 6, 202518067 Recommended AI CoursesMIT No Code AI and Machine Learning ProgramLearn Artificial Intelligence & Machine Learning from University of Texas. Get a completion certificate and grow your professional career.4.70 ★(4,175 Ratings) : 12 WeeksView ProgramAI and ML Program from UT AustinEnroll in the PG Program in AI and Machine Learning from University of Texas McCombs. Earn PG Certificate and and unlock new opportunities4.73 ★(1,402 Ratings) : 7 monthsView Program Free CoursesFree Artificial Intelligence Course With CertificateFree Prompt Engineering Course With CertificatePython for Machine Learning Free CourseData Science Foundations Free CourseDeep Learning with Python Free CourseIntroduction to Cyber Security Free CourseFree Digital Marketing CourseJava Programming Free CourseView More →Study AbroadStudy Abroad ProgramsStudy in USAStudy in Germany Popular CoursesPGP In Data Science and Business AnalyticsPGP In Artificial Intelligence And Machine LearningPGP In ManagementPGP In Cloud ComputingSoftware Engineering CoursePGP In Digital MarketingView More →Blog CategoriesData Science ArticlesArtificial Intelligence ArticlesCloud Computing ArticlesCyber Security ArticlesIT/Software GuidesCareer Development ArticlesResearch and Studies Premium CoursesMaster Generative AIMaster Python programmingMaster Data Analytics in SQLMaster Data Analytics in ExcelMaster Data Analytics in SQL & ExcelMaster Data Science & Machine Learning in Python About UsContact UsPrivacy PolicyTerms of UseGreat Learning Careers © 2013 - 2025 Great Learning Education Services Private Limited (Formerly known as Great Lakes E-Learning Services Private Limited).All rights reservedGet our android appGet our ios app Browse TopicsMenu ToggleAI and Machine Learning Data Science and Business Analytics Cloud Computing Cybersecurity IT/Software Development Career Development Digital Marketing Business Management UI/UX Design Study Abroad Research and StudiesFree CoursesMenu ToggleArtificial Intelligence Free Courses Machine Learning Free Courses Data Science Free Courses Cyber Security Free Courses Cloud Computing Free Courses IT & Software Free Courses Interview Preparation Free Courses Digital Marketing Free Courses Management Free CoursesWrite for Us Browse TopicsMenu ToggleAI and Machine Learning Data Science and Business Analytics Cloud Computing Cybersecurity IT/Software Development Career Development Digital Marketing Business Management UI/UX Design Study Abroad Research and StudiesFree CoursesMenu ToggleArtificial Intelligence Free Courses Machine Learning Free Courses Data Science Free Courses Cyber Security Free Courses Cloud Computing Free Courses IT & Software Free Courses Interview Preparation Free Courses Digital Marketing Free Courses Management Free CoursesWrite for Us Scroll to Top"
    ],
    [
        "What’s the trade-off between bias and variance",
        "More reading: Bias-Variance Tradeoff (Wikipedia)Bias is error due to erroneous or overly simplistic assumptions in thelearning algorithm you’re using. This can lead to themodel underfitting your data, making it hard for it to have high predictiveaccuracy and for you to generalize your knowledge from the training setto the test set.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 3/23Variance is error due to too much complexity in the learning algorithmyou’re using. This leads to the algorithm being highly sensitive to highdegrees of variation in your training data, which can lead your modelto overfit the data. You’ll be carrying too much noise from your trainingdata for your model to be very useful for your test data.The bias-variance decomposition essentially decomposes the learningerror from any algorithm by adding the bias, the variance and a bit ofirreducible error due to noise in the underlying dataset. Essentially, ifyou make the model more complex and add more variables, you’ll losebias but gain some variance — in order to get the optimally reducedamount of error, you’ll have to tradeoff bias and variance. You don’twant either high bias or high variance in your model."
    ],
    [
        "What is the difference between supervised and unsupervisedmachine learning",
        "More reading: What is the difference between supervised andunsupervised machine learning? (Quora)Supervised learning requires training labeled data. For example, inorder to do classification (a supervised learning task), you’ll need to firstlabel the data you’ll use to train the model to classify data into yourlabeled groups. Unsupervised learning, in contrast, does not requirelabeling data explicitly."
    ],
    [
        "How is KNN different from k-means clustering",
        "More reading: How is the k-nearest neighbor algorithm different from k-means clustering? (Quora)7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 4/23K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an unsupervised clustering algorithm. While themechanisms may seem similar at first, what this really means is that inorder for K-Nearest Neighbors to work, you need labeled data you wantto classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and athreshold: the algorithm will take unlabeled points and gradually learnhow to cluster them into groups by computing the mean of the distancebetween different points.The critical difference here is that KNN needs labeled points and is thussupervised learning, while k-means doesn’t — and is thus unsupervisedlearning."
    ],
    [
        "Explain how a ROC curve works",
        "More reading: Receiver operating characteristic (Wikipedia)The ROC curve is a graphical representation of the contrast betweentrue positive rates and the false positive rate at various thresholds. It’soften used as a proxy for the trade-off between the sensitivity of themodel (true positives) vs the fall-out or the probability it will trigger afalse alarm (false positives).7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 5/23"
    ],
    [
        "Define precision and recall",
        "More reading: Precision and recall (Wikipedia)Recall is also known as the true positive rate: the amount of positivesyour model claims compared to the actual number of positives there arethroughout the data. Precision is also known as the positive predictivevalue, and it is a measure of the amount of accurate positives yourmodel claims compared to the number of positives it actually claims. Itcan be easier to think of recall and precision in the context of a casewhere you’ve predicted that there were 10 apples and 5 oranges in acase of 10 apples. You’d have perfect recall (there are actually 10apples, and you predicted there would be 10) but 66.7% precision7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 6/23because out of the 15 events you predicted, only 10 (the apples) arecorrect."
    ],
    [
        "What is Bayes’ Theorem",
        "How is it useful in a machinelearning context?More reading: An Intuitive (and Short) Explanation of Bayes’ Theorem(BetterExplained)Bayes’ Theorem gives you the posterior probability of an event givenwhat is known as prior knowledge.Mathematically, it’s expressed as the true positive rate of a conditionsample divided by the sum of the false positive rate of the populationand the true positive rate of a condition. Say you had a 60% chance ofactually having the flu after a flu test, but out of people who had the flu,the test will be false 50% of the time, and the overall population onlyhas a 5% chance of having the flu. Would you actually have a 60%chance of having the flu after having a positive test?Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (TruePositive Rate of a Condition Sample) / (.6*0.05)(True Positive Rate of aCondition Sample) + (.5*0.95) (False Positive Rate of a Population) =0.0594 or 5.94% chance of getting a flu.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 7/23Bayes’ Theorem is the basis behind a branch of machine learning thatmost notably includes the Naive Bayes classifier. That’s somethingimportant to consider when you’re faced with machine learninginterview questions."
    ],
    [
        "Why is “Naive” Bayes naive",
        "More reading: Why is “naive Bayes” naive? (Quora)Despite its practical applications, especially in text mining, Naive Bayesis considered “Naive” because it makes an assumption that is virtuallyimpossible to see in real-life data: the conditional probability iscalculated as the pure product of the individual probabilities ofcomponents. This implies the absolute independence of features — acondition probably never met in real life.As a Quora commenter put it whimsically, a Naive Bayes classifier thatfigured out that you liked pickles and ice cream would probably naivelyrecommend you a pickle ice cream.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 8/23"
    ],
    [
        "Explain the difference between L1 and L2 regularization.More reading: What is the difference between L1 and L2 regularization",
        "(Quora)L2 regularization tends to spread error among all the terms, while L1 ismore binary/sparse, with many variables either being assigned a 1 or 0in weighting. L1 corresponds to setting a Laplacean prior on the terms,while L2 corresponds to a Gaussian prior."
    ],
    [
        "What’s your favorite algorithm, and can you explain it to me inless than a minute",
        "This type of question tests your understanding of how to communicatecomplex and technical nuances with poise and the ability to summarize7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 9/23quickly and efficiently. Make sure you have a choice and make sure youcan explain different algorithms so simply and effectively that a five-year-old could grasp the basics!"
    ],
    [
        "What’s the difference between Type I and Type II error",
        "More reading: Type I and type II errors (Wikipedia)Don’t think that this is a trick question! Many machine learning interviewquestions will be an attempt to lob basic questions at you just to makesure you’re on top of your game and you’ve prepared all of your bases.Type I error is a false positive, while Type II error is a false negative.Briefly stated, Type I error means claiming something has happenedwhen it hasn’t, while Type II error means that you claim nothing ishappening when in fact something is.A clever way to think about this is to think of Type I error as telling aman he is pregnant, while Type II error means you tell a pregnantwoman she isn’t carrying a baby."
    ],
    [
        "What’s a Fourier transform",
        "More reading: Fourier transform (Wikipedia)A Fourier transform is a generic method to decompose genericfunctions into a superposition of symmetric functions. Or as this moreintuitive tutorial puts it, given a smoothie, it’s how we find the recipe.The Fourier transform finds the set of cycle speeds, amplitudes andphases to match any time signal. A Fourier transform converts a signalfrom time to frequency domain — it’s a very common way to extractfeatures from audio signals or other time series such as sensor data.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 10/23"
    ],
    [
        "What’s the difference between probability and likelihood",
        "More reading: What is the difference between “likelihood” and“probability”? (Cross Validated)"
    ],
    [
        "What is deep learning, and how does it contrast with othermachine learning algorithms",
        "More reading: Deep learning (Wikipedia)Deep learning is a subset of machine learning that is concerned withneural networks: how to use backpropagation and certain principlesfrom neuroscience to more accurately model large sets of unlabelled orsemi-structured data. In that sense, deep learning represents anunsupervised learning algorithm that learns representations of datathrough the use of neural nets."
    ],
    [
        "What’s the difference between a generative anddiscriminative model",
        "7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 11/23More reading: What is the difference between a Generative andDiscriminative Algorithm? (Stack Overflow)A generative model will learn categories of data while a discriminativemodel will simply learn the distinction between different categories ofdata. Discriminative models will generally outperform generativemodels on classification tasks."
    ],
    [
        "What cross-validation technique would you use on a timeseries dataset",
        "More reading: Using k-fold cross-validation for time-series modelselection (CrossValidated)Instead of using standard k-folds cross-validation, you have to payattention to the fact that a time series is not randomly distributed data— it is inherently ordered by chronological order. If a pattern emerges inlater time periods for example, your model may still pick up on it even ifthat effect doesn’t hold in earlier years!You’ll want to do something like forward chaining where you’ll be ableto model on past data then look at forward-facing data.fold 1 : training [1], test [2]fold 2 : training [1 2], test [3]fold 3 : training [1 2 3], test [4]fold 4 : training [1 2 3 4], test [5]fold 5 : training [1 2 3 4 5], test [6]"
    ],
    [
        "How is a decision tree pruned",
        "More reading: Pruning (decision trees)Pruning is what happens in decision trees when branches that haveweak predictive power are removed in order to reduce the complexity of7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 12/23the model and increase the predictive accuracy of a decision treemodel. Pruning can happen bottom-up and top-down, with approachessuch as reduced error pruning and cost complexity pruning.Reduced error pruning is perhaps the simplest version: replace eachnode. If it doesn’t decrease predictive accuracy, keep it pruned. Whilesimple, this heuristic actually comes pretty close to an approach thatwould optimize for maximum accuracy."
    ],
    [
        "Which is more important to you– model accuracy, or modelperformance",
        "More reading: Accuracy paradox (Wikipedia)This question tests your grasp of the nuances of machine learningmodel performance! Machine learning interview questions often looktowards the details. There are models with higher accuracy that canperform worse in predictive power — how does that make sense?Well, it has everything to do with how model accuracy is only a subsetof model performance, and at that, a sometimes misleading one. Forexample, if you wanted to detect fraud in a massive dataset with asample of millions, a more accurate model would most likely predict nofraud at all if only a vast minority of cases were fraud. However, thiswould be useless for a predictive model — a model designed to findfraud that asserted there was no fraud at all! Questions like this helpyou demonstrate that you understand model accuracy isn’t the be-alland end-all of model performance."
    ],
    [
        "What’s the F1 score",
        "How would you use it?More reading: F1 score (Wikipedia)7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 13/23The F1 score is a measure of a model’s performance. It is a weightedaverage of the precision and recall of a model, with results tending to 1being the best, and those tending to 0 being the worst. You would use itin classification tests where true negatives don’t matter much."
    ],
    [
        "How would you handle an imbalanced dataset",
        "More reading: 8 Tactics to Combat Imbalanced Classes in YourMachine Learning Dataset (Machine Learning Mastery)An imbalanced dataset is when you have, for example, a classificationtest and 90% of the data is in one class. That leads to problems: anaccuracy of 90% can be skewed if you have no predictive power on theother category of data! Here are a few tactics to get over the hump:1- Collect more data to even the imbalances in the dataset.2- Resample the dataset to correct for imbalances.3- Try a different algorithm altogether on your dataset.What’s important here is that you have a keen sense for what damagean unbalanced dataset can cause, and how to balance that."
    ],
    [
        "When should you use classification over regression",
        "More reading: Regression vs Classification (Math StackExchange)Classification produces discrete values and dataset to strict categories,while regression gives you continuous results that allow you to betterdistinguish differences between individual points. You would useclassification over regression if you wanted your results to reflect the7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 14/23belongingness of data points in your dataset to certain explicitcategories (ex: If you wanted to know whether a name was male orfemale rather than just how correlated they were with male and femalenames.)"
    ],
    [
        "Name an example where ensemble techniques might beuseful",
        "More reading: Ensemble learning (Wikipedia)Ensemble techniques use a combination of learning algorithms tooptimize better predictive performance. They typically reduce overfittingin models and make the model more robust (unlikely to be influencedby small changes in the training data). You could list some examples of ensemble methods, from bagging toboosting to a “bucket of models” method and demonstrate how theycould increase predictive power."
    ],
    [
        "How do you ensure you’re not overfitting with a model",
        "More reading: How can I avoid overfitting? (Quora)This is a simple restatement of a fundamental problem in machinelearning: the possibility of overfitting training data and carrying the noiseof that data through to the test set, thereby providing inaccurategeneralizations.There are three main methods to avoid overfitting:1- Keep the model simpler: reduce variance by taking into accountfewer variables and parameters, thereby removing some of the noise inthe training data.2- Use cross-validation techniques such as k-folds cross-validation.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 15/233- Use regularization techniques such as LASSO that penalize certainmodel parameters if they’re likely to cause overfitting."
    ],
    [
        "What evaluation approaches would you work to gauge theeffectiveness of a machine learning model",
        "More reading: How to Evaluate Machine Learning Algorithms (MachineLearning Mastery)You would first split the dataset into training and test sets, or perhapsuse cross-validation techniques to further segment the dataset intocomposite sets of training and test sets within the data. You should thenimplement a choice selection of performance metrics: here is afairly comprehensive list. You could use measures such as the F1score, the accuracy, and the confusion matrix. What’s important here isto demonstrate that you understand the nuances of how a model ismeasured and how to choose the right performance measures for theright situations."
    ],
    [
        "How would you evaluate a logistic regression model",
        "More reading: Evaluating a logistic regression (CrossValidated)A subsection of the question above. You have to demonstrate anunderstanding of what the typical goals of a logistic regression are(classification, prediction etc.) and bring up a few examples and usecases."
    ],
    [
        "What’s the “kernel trick” and how is it useful",
        "More reading: Kernel method (Wikipedia)The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 16/23within that dimension: instead, kernel functions compute the innerproducts between the images of all pairs of data in a feature space.This allows them the very useful attribute of calculating the coordinatesof higher dimensions while being computationally cheaper than theexplicit calculation of said coordinates. Many algorithms can beexpressed in terms of inner products. Using the kernel trick enables useffectively run algorithms in a high-dimensional space with lower-dimensional data.Machine Learning Interview Questions: ProgrammingThese machine learning interview questions test your knowledge ofprogramming principles you need to implement machine learningprinciples in practice. Machine learning interview questions tend to betechnical questions that test your logic and programming skills: thissection focuses more on the latter."
    ],
    [
        "How do you handle missing or corrupted data in a dataset",
        "More reading: Handling missing data (O’Reilly)You could find missing/corrupted data in a dataset and either dropthose rows or columns, or decide to replace them with another value.In Pandas, there are two very useful methods: isnull() and dropna() thatwill help you find columns of data with missing or corrupted data anddrop those values. If you want to fill the invalid values with aplaceholder value (for example, 0), you could use the fillna() method."
    ],
    [
        "Do you have experience with Spark or big data tools formachine learning",
        "7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 17/23More reading: 50 Top Open Source Tools for Big Data (Datamation)You’ll want to get familiar with the meaning of big data for differentcompanies and the different tools they’ll want. Spark is the big data toolmost in demand now, able to handle immense datasets with speed. Behonest if you don’t have experience with the tools demanded, but alsotake a look at job descriptions and see what tools pop up: you’ll want toinvest in familiarizing yourself with them."
    ],
    [
        "Pick an algorithm",
        "Write the psuedo-code for a parallelimplementation.More reading: Writing pseudocode for parallel programming (StackOverflow)This kind of question demonstrates your ability to think in parallelismand how you could handle concurrency in programmingimplementations dealing with big data. Take a look at pseudocodeframeworks such as Peril-L and visualization tools such as WebSequence Diagrams to help you demonstrate your ability to write codethat reflects parallelism."
    ],
    [
        "What are some differences between a linked list and anarray",
        "More reading: Array versus linked list (Stack Overflow)An array is an ordered collection of objects. A linked list is a series ofobjects with pointers that direct how to process them sequentially. Anarray assumes that every element has the same size, unlike the linkedlist. A linked list can more easily grow organically: an array has to bepre-defined or re-defined for organic growth. Shuffling a linked list7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 18/23involves changing which points direct where — meanwhile, shuffling anarray is more complex and takes more memory."
    ],
    [
        "Describe a hash table",
        "More reading: Hash table (Wikipedia)A hash table is a data structure that produces an associative array. Akey is mapped to certain values through the use of a hash function.They are often used for tasks such as database indexing."
    ],
    [
        "Which data visualization libraries do you use",
        "What are yourthoughts on the best data visualization tools?More reading: 31 Free Data Visualization Tools (Springboard)What’s important here is to define your views on how to properlyvisualize data and your personal preferences when it comes to tools.Popular tools include R’s ggplot, Python’s seaborn and matplotlib, andtools such as Plot.ly and Tableau.Machine Learning Interview Questions:Company/Industry Specific7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 19/23These machine learning interview questions deal with how toimplement your general machine learning knowledge to a specificcompany’s requirements. You’ll be asked to create case studies andextend your knowledge of the company and industry you’re applying forwith your machine learning skills."
    ],
    [
        "How would you implement a recommendation system for ourcompany’s users",
        "More reading: How to Implement A Recommendation System? (StackOverflow)A lot of machine learning interview questions of this type will involveimplementation of machine learning models to a company’s problems.You’ll have to research the company and its industry in-depth,especially the revenue drivers the company has, and the types of usersthe company takes on in the context of the industry it’s in."
    ],
    [
        "How can we use your machine learning skills to generaterevenue",
        "More reading: Startup Metrics for Startups (500 Startups)This is a tricky question. The ideal answer would demonstrateknowledge of what drives the business and how your skills could relate.For example, if you were interviewing for music-streaming startupSpotify, you could remark that your skills at developing a betterrecommendation model would increase user retention, which wouldthen increase revenue in the long run.The startup metrics Slideshare linked above will help you understandexactly what performance indicators are important for startups and techcompanies as they think about revenue and growth.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 20/23"
    ],
    [
        "What do you think of our current data process",
        "More reading: The Data Science Process Email Course – SpringboardThis kind of question requires you to listen carefully and impartfeedback in a manner that is constructive and insightful. Yourinterviewer is trying to gauge if you’d be a valuable member of theirteam and whether you grasp the nuances of why certain things are setthe way they are in the company’s data process based on company- orindustry-specific conditions. They’re trying to see if you can be anintellectual peer. Act accordingly.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 21/23Machine Learning Interview Questions: General MachineLearning InterestThis series of machine learning interview questions attempts to gaugeyour passion and interest in machine learning. The right answers willserve as a testament for your commitment to being a lifelong learner inmachine learning."
    ],
    [
        "What are the last machine learning papers you’ve read",
        "More reading: What are some of the best research papers/books formachine learning?Keeping up with the latest scientific literature on machine learning is amust if you want to demonstrate interest in a machine learning position.This overview of deep learning in Nature by the scions of deep learningthemselves (from Hinton to Bengio to LeCun) can be a good referencepaper and an overview of what’s happening in deep learning — and thekind of paper you might want to cite."
    ],
    [
        "Do you have research experience in machine learning",
        "Related to the last point, most organizations hiring for machine learningpositions will look for your formal experience in the field. Researchpapers, co-authored or supervised by leaders in the field, can make thedifference between you being hired and not. Make sure you have asummary of your research experience and papers ready — and anexplanation for your background and lack of formal researchexperience if you don’t."
    ],
    [
        "What are your favorite use cases of machine learningmodels",
        "7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 22/23More reading: What are the typical use cases for different machinelearning algorithms? (Quora)The Quora thread above contains some examples, such as decisiontrees that categorize people into different tiers of intelligence based onIQ scores. Make sure that you have a few examples in mind anddescribe what resonated with you. It’s important that you demonstratean interest in how machine learning is implemented."
    ],
    [
        "How would you approach the “Netflix Prize” competition",
        "More reading: Netflix Prize (Wikipedia)The Netflix Prize was a famed competition where Netflix offered$1,000,000 for a better collaborative filtering algorithm. The team thatwon called BellKor had a 10% improvement and used an ensemble ofdifferent methods to win. Some familiarity with the case and its solutionwill help demonstrate you’ve paid attention to machine learning for awhile."
    ],
    [
        "Where do you usually source datasets",
        "More reading: 19 Free Public Data Sets For Your First Data ScienceProject (Springboard)Machine learning interview questions like these try to get at the heart ofyour machine learning interest. Somebody who is truly passionateabout machine learning will have gone off and done side projects ontheir own, and have a good idea of what great datasets are out there. Ifyou’re missing any, check out Quandl for economic and financial data,and Kaggle’s Datasets collection for another great list.7/30/2017 41 Key Machine Learning Interview Questions with Answershttps://www.springboard.com/blog/machine-learning-interview-questions/ 23/23"
    ],
    [
        "How do you think Google is training data for self-drivingcars",
        "More reading: Waymo TechMachine learning interview questions like this one really test yourknowledge of different machine learning methods, and yourinventiveness if you don’t know the answer. Google is currentlyusing recaptcha to source labelled data on storefronts and traffic signs.They are also building on training data collected by Sebastian Thrun atGoogleX — some of which was obtained by his grad students drivingbuggies on desert dunes!"
    ],
    [
        "How would you simulate the approach AlphaGo took to beatLee Sidol at Go",
        "More reading: Mastering the game of Go with deep neural networksand tree search (Nature)AlphaGo beating Lee Sidol, the best human player at Go, in a best-of-five series was a truly seminal event in the history of machine learningand deep learning. The Nature paper above describes how this wasaccomplished with “Monte-Carlo tree search with deep neural networksthat have been trained by supervised learning, from human expertgames, and by reinforcement learning from games of self-play.”Cover image credit: https://www.flickr.com/photos/iwannt/8596885627"
    ],
    [
        "Differentiate between AI, Machine Learning and Deep Learning",
        "Artificial Intelligence is a technique which enables machines to mimic human behavior.Machine Learning is a subset of AI technique which uses statistical methods to enable machines to improve with experience.Deep learning is a subset of ML which make the computation of multi-layer neural network feasible. It uses Neural networks to simulate human-like decision making."
    ],
    [
        "Do you think Deep Learning is Better than Machine Learning",
        "If so, why?Though traditional ML algorithms solve a lot of our cases, they are not useful while working with high dimensional data, that is where we have a large number of inputs and outputs. For example, inthe case of handwriting recognition, we have a large amount of input where we will have a different type of inputs associated with different type of handwriting.The second major challenge is to tell the computer what are the features it should look for that will play an important role in predicting the outcome as well as to achieve better accuracy whiledoing so."
    ],
    [
        "What is Perceptron",
        "And How does it Work?If we focus on the structure of a biological neuron, it has dendrites which are used to receive inputs. These inputs are summed in the cell body and using the Axon it is passed on to the nextbiological neuron as shown below. Dendrite: Receives signals from other neuronsCell Body: Sums all the inputsAxon: It is used to transmit signals to the other cellsSimilarly, a perceptron receives multiple inputs, applies various transformations and functions and provides an output. A Perceptron is a linear model used for binary classi+cation. It models aneuron which has a set of inputs, each of which is given a specific weight. The neuron computes some function on these weighted inputs and gives the output. Subscribe "
    ],
    [
        "What is the role of weights and bias",
        "For a perceptron, there can be one more input called bias. While the weights determine the slope of the classifier line, bias allows us to shift the line towards left or right. Normally bias is treatedas another weighted input with the input value x"
    ],
    [
        "What are the activation functions",
        "Activation function translates the inputs into outputs. Activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. Thepurpose of the activation function is to introduce non-linearity into the output of a neuron.There can be many Activation functions like:Linear or IdentityUnit or Binary StepSigmoid or LogisticTanhReLUSoftmax"
    ],
    [
        "Explain Learning of a Perceptron",
        "1. Initializing the weights and threshold.2. Provide the input and calculate the output.3. Update the weights.4. Repeat Steps 2 and 3Wj (t+1) – Updated WeightWj (t) – Old Weightd – Desired Outputy – Actual Outputx – Input"
    ],
    [
        "What is the significance of a Cost/Loss function",
        "A cost function is a measure of the accuracy of the neural network with respect to a given training sample and expected output. It provides the performance of a neural network as a whole. Indeep learning, the goal is to minimize the cost function. For that, we use the concept of gradient descent."
    ],
    [
        "What is gradient descent",
        "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.Stochastic Gradient Descent: Uses only a single training example to calculate the gradient and update parameters.Batch Gradient Descent: Calculate the gradients for the whole dataset and perform just one update at each iteration.Mini-batch Gradient Descent: Mini-batch gradient is a variation of stochastic gradient descent where instead of single training example, mini-batch of samples is used. It’s one of the mostpopular optimization algorithms."
    ],
    [
        "What are the benefits of mini-batch gradient descent",
        "This is more efficient compared to stochastic gradient descent.The generalization by finding the flat minima.Mini-batches allows help to approximate the gradient of the entire training set which helps us to avoid local minima. 0."
    ],
    [
        "What are the steps for using a gradient descent algorithm",
        "Initialize random weight and bias.Pass an input through the network and get values from the output layer.Calculate the error between the actual value and the predicted value.Go to each neuron which contributes to the error and then change its respective values to reduce the error.Reiterate until you find the best weights of the network."
    ],
    [
        "Create a Gradient Descent in python",
        ""
    ],
    [
        "What are the shortcomings of a single layer perceptron",
        "Well, there are two major problems:Single-Layer Perceptrons cannot classify non-linearly separable data points.Complex problems, that involve a lot of parameters cannot be solved by Single-Layer Perceptrons"
    ],
    [
        "What is a Multi-Layer-PerceptronA multilayer perceptron (MLP) is a deep, arti+cial neural network",
        "It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makesa decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP."
    ],
    [
        "What are the different parts of a multi-layer perceptron",
        "Input Nodes: The Input nodes provide information from the outside world to the network and are together referred to as the “Input Layer”. No computation is performed in any of the Inputnodes – they just pass on the information to the hidden nodes.Hidden Nodes: The Hidden nodes perform computations and transfer information from the input nodes to the output nodes. A collection of hidden nodes forms a “Hidden Layer”. While a networkwill only have a single input layer and a single output layer, it can have zero or multiple Hidden Layers.Output Nodes: The Output nodes are collectively referred to as the “Output Layer” and are responsible for computations and transferring information from the network to the outside world."
    ],
    [
        "What Is Data Normalization And Why Do We Need It",
        "Data normalization is very important preprocessing step, used to rescale values to +t in a speci+c range to assure better convergence during backpropagation. In general, it boils down tosubtracting the mean of each data point and dividing by its standard deviation.These were some basic Deep Learning Interview Questions. Now, let’s move on to some advanced ones.Advance Interview Questions"
    ],
    [
        "Which is Better Deep Networks or Shallow ones",
        "and Why?Both the Networks, be it shallow or Deep are capable of approximating any function. But what matters is how precise that network is in terms of getting the results. A shallow network works withonly a few features, as it can’t extract more. But a deep network goes deep by computing efficiently and working on more features/parameters."
    ],
    [
        "Why is Weight Initialization important in Neural Networks",
        "Weight initialization is one of the very important steps. A bad weight initialization can prevent a network from learning but good weight initialization helps in giving a quicker convergence and abetter overall error.12345678910111213params = [weights_hidden, weights_output, bias_hidden, bias_output] def sgd(cost, params, lr=0.05): grads = T.grad(cost=cost, wrt=params)updates = [] for p, g in zip(params, grads):updates.append([p, p - g * lr]) return updates updates = sgd(cost, params)Biases can be generally initialized to zero. The rule for setting the weights is to be close to zero without being too small."
    ],
    [
        "What’s the difference between a feed-forward and a backpropagation neural network",
        "A Feed-Forward Neural Network is a type of Neural Network architecture where the connections are “fed forward”, i.e. do not form cycles. The term “Feed-Forward” is also used when you inputsomething at the input layer and it travels from input to hidden and from hidden to the output layer.Backpropagation is a training algorithm consisting of 2 steps:Feed-Forward the values.Calculate the error and propagate it back to the earlier layers.So to be precise, forward-propagation is part of the backpropagation algorithm but comes before back-propagating."
    ],
    [
        "What are the Hperparameteres",
        "Name a few used in any Neural Network.Hyperparameters are the variables which determine the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate).Hyperparameters are set before training.Number of Hidden LayersNetwork Weight InitializationActivation FunctionLearning RateMomentumNumber of EpochsBatch Size"
    ],
    [
        "Explain the different Hyperparameters related to Network and Training",
        "Network HyperparametersThe number of Hidden Layers: Many hidden units within a layer with regularization techniques can increase accuracy. Smaller number of units may cause underfitting.Network Weight Initialization: Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. Mostly uniform distribution is used.Activation function: Activation functions are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.Training HyperparametersLearning Rate: The learning rate de+nes how quickly a network updates its parameters. Low learning rate slows down the learning process but converges smoothly. Larger learning rate speeds upthe learning but may not converge.Momentum: Momentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.The number of epochs: Number of epochs is the number of times the whole training data is shown to the network while training. Increase the number of epochs until the validation accuracystarts decreasing even when training accuracy is increasing(overfitting).Batch size: Mini batch size is the number of sub-samples given to the network after which parameter update happens. A good default for batch size might be 32. Also try 32, 64, 128, 256, and soon."
    ],
    [
        "What is Dropout",
        "Dropout is a regularization technique to avoid over+tting thus increasing the generalizing power. Generally, we should use a small dropout value of 20%-50% of neurons with 20% providing a goodstarting point. A probability too low has minimal effect and a value too high results in under-learning by the network.Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations."
    ],
    [
        "In training a neural network, you notice that the loss does not decrease in the few starting epochs. What could be the reason",
        "The reasons for this could be:The learning is rate is lowRegularization parameter is highStuck at local minima"
    ],
    [
        "What are Tensors",
        "Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, that allows you to represent data having higher dimensions. In general, DeepLearning you deal with high dimensional data sets where dimensions refer to different features present in the data set."
    ],
    [
        "List a few advantages of TensorFlow",
        "It has platform flexibilityIt is easily trainable on CPU as well as GPU for distributed computing.TensorFlow has auto differentiation capabilitiesIt has advanced support for threads, asynchronous computation, and queue es.It is a customizable and open source."
    ],
    [
        "What is Computational Graph",
        "A computational graph is a series of TensorFlow operations arranged as nodes in the graph. Each node takes zero or more tensors as input and produces a tensor as output.Basically, one can think of a Computational Graph as an alternative way of conceptualizing mathematical calculations that takes place in a TensorFlow program. The operations assigned to differentnodes of a Computational Graph can be performed in parallel, thus, providing better performance in terms of computations."
    ],
    [
        "What is a CNN",
        "Convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. Unlike neural networks, where the input is a vector, herethe input is a multi-channeled image. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing."
    ],
    [
        "Explain the different Layers of CNN",
        "There are four layered concepts we should understand in Convolutional Neural Networks:Convolution: The convolution layer comprises of a set of independent +lters. All these +lters are initialized randomly and become our parameters which will be learned by the networksubsequently.ReLu: This layer is used with the convolutional layer.Pooling: Its function is to progressively reduce the spatial size of the representation to reduce the number of parameters and computation in the network. Pooling layer operates on each featuremap independently.Full Connectedness: Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computedwith a matrix multiplication followed by a bias offset."
    ],
    [
        "What is an RNN",
        "Recurrent Networks are a type of arti+cial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting, the spoken word, numerical times series data.Recurrent Neural Networks use backpropagation algorithm for training Because of their internal memory, RNN’s are able to remember important things about the input they received, whichenables them to be very precise in predicting what’s coming next."
    ],
    [
        "What are some issues faced while training an RNN",
        "Recurrent Neural Networks use backpropagation algorithm for training, but it is applied for every timestamp. It is commonly known as Back-propagation Through Time (BTT).There are some issues with Back-propagation such as:Vanishing GradientExploding Gradient"
    ],
    [
        "What is Vanishing Gradient",
        "And how is this harmful?When we do Back-propagation, the gradients tend to get smaller and smaller as we keep on moving backward in the Network. This means that the neurons in the Earlier layers learn very slowly ascompared to the neurons in the later layers in the Hierarchy.Earlier layers in the Network are important because they are responsible to learn and detecting the simple patterns and are actually the building blocks of our Network.Obviously, if they give improper and inaccurate results, then how can we expect the next layers and the complete Network to perform nicely and produce accurate results. The Training processtakes too long and the Prediction Accuracy of the Model will decrease."
    ],
    [
        "What is Exploding Gradient Descent",
        "Exploding gradients are a problem when large error gradients accumulate and result in very large updates to neural network model weights during training.Gradient Descent process works best when these updates are small and controlled. When the magnitudes of the gradients accumulate, an unstable network is likely to occur, which can cause poorprediction of results or even a model that reports nothing useful what so ever."
    ],
    [
        "Explain the importance of LSTM",
        "Long short-term memory(LSTM) is an arti+cial recurrent neural network architecture used in the +eld of deep learning. Unlike standard feedforward neural networks, LSTM has feedbackconnections that make it a “general purpose computer”. It can not only process single data points, but also entire sequences of data.They are a special kind of Recurrent Neural Networks which are capable of learning long-term dependencies."
    ],
    [
        "What are capsules in Capsule Neural Network",
        "Capsules are a vector specifying the features of the object and its likelihood. These features can be any of the instantiation parameters like position, size, orientation, deformation, velocity, hue,texture and much more.A capsule can also specify its attributes like angle and size so that it can represent the same generic information. Now, just like a neural network has layers of neurons, a capsule network can havelayers of capsules.Now, let’s continue this Deep Learning Interview Questions and move to the section of autoencoders and RBMs."
    ],
    [
        "Explain Autoencoders and it’s uses",
        "An autoencoder neural network is an Unsupervised Machine learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. Autoencoders are used to reducethe size of our inputs into a smaller representation. If anyone needs the original data, they can reconstruct it from the compressed data."
    ],
    [
        "In terms of Dimensionality Reduction, How does Autoencoder differ from PCAs",
        "An autoencoder can learn non-linear transformations with a non-linear activation function and multiple layers.It doesn’t have to learn dense layers. It can use convolutional layers to learn which is better for video, image and series data.It is more efficient to learn several layers with an autoencoder rather than learn one huge transformation with PCA.An autoencoder provides a representation of each layer as the output.It can make use of pre-trained layers from another model to apply transfer learning to enhance the encoder/decoder."
    ],
    [
        "Give some real-life examples where autoencoders can be applied",
        "Image Coloring: Autoencoders are used for converting any black and white picture into a colored image. Depending on what is in the picture, it is possible to tell what the color should be.Feature variation: It extracts only the required features of an image and generates the output by removing any noise or unnecessary interruption.Dimensionality Reduction: The reconstructed image is the same as our input but with reduced dimensions. It helps in providing a similar image with a reduced pixel value.Denoising Image: The input seen by the autoencoder is not the raw input but a stochastically corrupted version. A denoising autoencoder is thus trained to reconstruct the original input from thenoisy version."
    ],
    [
        "what are the different layers of Autoencoders",
        "An Autoencoder consist of three layers:EncoderCodeDecoder"
    ],
    [
        "Explain the architecture of an Autoencoder",
        "Encoder: This part of the network compresses the input into a latent space representation. The encoder layer encodes the input image as a compressed representation in a reduced dimension.The compressed image is the distorted version of the original image.Code: This part of the network represents the compressed input which is fed to the decoder.Decoder: This layer decodes the encoded image back to the original dimension. The decoded image is a lossy reconstruction of the original image and it is reconstructed from the latent spacerepresentation."
    ],
    [
        "What is a Bottleneck in autoencoder and why is it used",
        "The layer between the encoder and decoder, ie. the code is also known as Bottleneck. This is a well-designed approach to decide which aspects of observed data are relevant information and whataspects can be discarded.It does this by balancing two criteria:Compactness of representation, measured as the compressibility.It retains some behaviourally relevant variables from the input."
    ],
    [
        "Is there any variation of Autoencoders",
        "Convolution AutoencodersSparse AutoencodersDeep AutoencodersContractive Autoencoders"
    ],
    [
        "What are Deep Autoencoders",
        "The extension of the simple Autoencoder is the Deep Autoencoder. The +rst layer of the Deep Autoencoder is used for +rst-order features in the raw input. The second layer is used for second-order features corresponding to patterns in the appearance of first-order features. Deeper layers of the Deep Autoencoder tend to learn even higher-order features.A deep autoencoder is composed of two, symmetrical deep-belief networks:First four or five shallow layers representing the encoding half of the net.The second set of four or five layers that make up the decoding half."
    ],
    [
        "What is a Restricted Boltzmann Machine",
        "Restricted Boltzmann Machine is an undirected graphical model that plays a major role in Deep Learning Framework in recent times.It is an algorithm which is useful for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling."
    ],
    [
        "How Does RBM differ from Autoencoders",
        "Autoencoder is a simple 3-layer neural network where output units are directly connected back to input units. Typically, the number of hidden units is much less than the number of visible ones.The task of training is to minimize an error or reconstruction, i.e. find the most efficient compact representation for input data.RBM shares a similar idea, but it uses stochastic units with particular distribution instead of deterministic distribution. The task of training is to +nd out how these two sets of variables are actually"
    ],
    [
        "What do you understand by Natural Language Processing?",
        "Natural Language Processing is a field of computer science that deals with communication between computer systems and humans. It is a technique used in Artificial Intelligence and Machine Learning. It is used to create automated software that helps understand human-spoken languages to extract useful information from the data. Techniques in NLP allow computer systems to process and interpret data in the form of natural languages."
    ],
    [
        "List any two real-life applications of Natural Language Processing.",
        "Two real-life applications of Natural Language Processing are as follows:Google Translate: Google Translate is one of the famous applications of Natural Language Processing. It helps convert written or spoken sentences into any language. Also, we can find the correct pronunciation and meaning of a word by using Google Translate. It uses advanced techniques of Natural Language Processing to achieve success in translating sentences into various languages.Chatbots: To provide a better customer support service, companies have started using chatbots for 24/7 service. AI Chatbots help resolve the basic queries of customers. If a chatbot is not able to resolve any query, then it forwards it to the support team, while still engaging the customer. It helps make customers feel that the customer support team is quickly attending to them. With the help of chatbots, companies have become capable of building cordial relations with customers. It is only possible with the help of Natural Language Processing."
    ],
    [
        "What are stop words?",
        "Stop words are said to be useless data for a search engine. Words such as articles, prepositions, etc. are considered stop words. There are stop words such as was, were, is, am, the, a, an, how, why, and many more. In Natural Language Processing, we eliminate the stop words to understand and analyze the meaning of a sentence. The removal of stop words is one of the most important tasks for search engines. Engineers design the algorithms of search engines in such a way that they ignore the use of stop words. This helps show the relevant search result for a query.Step Up as a Visionary in Data Science Simplify Data Science with Our CourseExplore Program"
    ],
    [
        "What is NLTK?",
        "NLTK is a Python library, which stands for Natural Language Toolkit. We use NLTK to process data in human-spoken languages. NLTK allows us to apply techniques such as parsing, tokenization, lemmatization, stemming, and more to understand natural languages. It helps in categorizing text, parsing linguistic structure, analyzing documents, etc.A few of the libraries of the NLTK package that we often use in NLP are:SequentialBackoffTaggerDefaultTaggerUnigramTaggertreebankwordnetFreqDistpatternsRegexpTaggerbackoff_taggerUnigramTagger, BigramTagger, and TrigramTagger"
    ],
    [
        "What is Syntactic Analysis?",
        "Syntactic analysis is a technique of analyzing sentences to extract meaning from them. Using syntactic analysis, a machine can analyze and understand the order of words arranged in a sentence. NLP employs grammar rules of a language that helps in the syntactic analysis of the combination and order of words in documents.The techniques used for syntactic analysis are as follows:Parsing: It helps in deciding the structure of a sentence or text in a document. It helps analyze the words in the text based on the grammar of the language.Word segmentation: The segmentation of words segregates the text into small significant units.Morphological segmentation: The purpose of morphological segmentation is to break words into their base form.Stemming: It is the process of removing the suffix from a word to obtain its root word.Lemmatization: It helps combine words using suffixes, without altering the meaning of the word."
    ],
    [
        "What is Semantic Analysis?",
        "Semantic analysis helps make a machine understand the meaning of a text. It uses various algorithms for the interpretation of words in sentences. It also helps understand the structure of a sentence.Techniques used for semantic analysis are as given below:Named entity recognition: This is the process of information retrieval that helps identify entities such as the name of a person, organization, place, time, emotion, etc.Word sense disambiguation: It helps identify the sense of a word used in different sentences.Natural language generation: It is a process used by the software to convert structured data into human-spoken languages. By using NLG, organizations can automate content for custom reports."
    ],
    [
        "List the components of Natural Language Processing.",
        "The major components of NLP are as follows:Entity extraction: Entity extraction refers to the retrieval of information such as place, person, organization, etc. by the segmentation of a sentence. It helps in the recognition of an entity in a text.Syntactic analysis: Syntactic analysis helps draw the specific meaning of a text.Pragmatic analysis: To find useful information from a text, we implement pragmatic analysis techniques.Morphological and lexical analysis: It helps in explaining the structure of words by analyzing them through parsing."
    ],
    [
        "What is Latent Semantic Indexing (LSI)?",
        "Latent semantic indexing is a mathematical technique used to improve the accuracy of the information retrieval process. The design of LSI algorithms allows machines to detect the hidden (latent) correlation between semantics (words). To enhance information understanding, machines generate various concepts that associate with the words of a sentence.The technique used for information understanding is called singular value decomposition. It is generally used to handle static and unstructured data. The matrix obtained for singular value decomposition contains rows for words and columns for documents. This method is best suited to identify components and group them according to their types.The main principle behind LSI is that words carry a similar meaning when used in a similar context. Computational LSI models are slow in comparison to other models. However, they are good at contextual awareness which helps improve the analysis and understanding of a text or a document."
    ],
    [
        "What are Regular Expressions?",
        "A regular expression is used to match and tag words. It consists of a series of characters for matching strings.Suppose, if A and B are regular expressions, then the following are true for them:If {ɛ} is a regular language, then ɛ is a regular expression for it.If A and B are regular expressions, then A + B is also a regular expression within the language {A, B}.If A and B are regular expressions, then the concatenation of A and B (A.B) is a regular expression.If A is a regular expression, then A* (A occurring multiple times) is also a regular expression."
    ],
    [
        "What is Regular Grammar?",
        "Regular grammar is used to represent a regular language.Regular grammar comprises rules in the form of A -> a, A -> aB, and many more. The rules help detect and analyze strings by automated computation.Regular grammar consists of four tuples:‘N’ is used to represent the non-terminal set.‘∑’ represents the set of terminals.‘P’ stands for the set of productions.‘S € N’ denotes the start of non-terminal.Regular grammar is of 2 types:(a) Left Linear Grammar(LLG)(b) Right Linear Grammar(RLG)Watch this video on Natural Language Processing Interview Questions for Beginners:"
    ],
    [
        "What is Parsing in the context of NLP?",
        "Parsing in NLP refers to the understanding of a sentence and its grammatical structure by a machine. Parsing allows the machine to understand the meaning of a word in a sentence and the grouping of words, phrases, nouns, subjects, and objects in a sentence. Parsing helps analyze the text or the document to extract useful insights from it. To understand parsing, refer to the below diagram:In this, ‘Jonas ate an orange’ is parsed to understand the structure of the sentence.Get 100% Hike!Master Most in Demand Skills Now! By providing your contact details, you agree to our Terms of Use & Privacy PolicyIntermediate NLP Interview Questions"
    ],
    [
        "What is TF-IDF?",
        "TFIDF or Term Frequency-Inverse Document Frequency indicates the importance of a word in a set. It helps in information retrieval with numerical statistics. For a specific document, TF-IDF shows a frequency that helps identify the keywords in a document. The major use of TF-IDF in NLP is the extraction of useful information from crucial documents by statistical data. It is ideally used to classify and summarize the text in documents and filter out stop words.TF helps calculate the ratio of the frequency of a term in a document and the total number of terms. Whereas, IDF denotes the importance of the term in a document.The formula for calculating TF-IDF:TF(W) = (Frequency of W in a document)/(The total number of terms in the document)IDF(W) = log_e(The total number of documents/The number of documents having the term W)When TF*IDF is high, the frequency of the term is less and vice versa.Google uses TF-IDF to decide the index of search results according to the relevancy of pages. The design of the TF-IDF algorithm helps optimize the search results in Google. It helps quality content rank up in search results."
    ],
    [
        "Define the terminology in NLP.",
        "This is one of the most often asked NLP interview questions.The interpretation of Natural Language Processing depends on various factors, and they are:Weights and VectorsUse of TF-IDF for information retrievalLength (TF-IDF and doc)Google Word VectorsWord VectorsStructure of the TextPOS taggingHead of the sentenceNamed Entity Recognition (NER)Sentiment AnalysisKnowledge of the characteristics of sentimentKnowledge about entities and the common dictionary available for sentiment analysisClassification of TextSupervised learning algorithmTraining setValidation setTest setFeatures of the textLDAMachine ReadingRemoval of possible entitiesJoining with other entitiesDBpediaFRED (lib) Pikes"
    ],
    [
        "Explain Dependency Parsing in NLP.",
        "Dependency parsing helps assign a syntactic structure to a sentence. Therefore, it is also called syntactic parsing. Dependency parsing is one of the critical tasks in NLP. It allows the analysis of a sentence using parsing algorithms. Also, by using the parse tree in dependency parsing, we can check the grammar and analyze the semantic structure of a sentence.For implementing dependency parsing, we use the spaCy package. It implements token properties to operate the dependency parse tree.The below diagram shows the dependency parse tree:"
    ],
    [
        "What is the difference between NLP and NLU?",
        "The below table shows the difference between NLP and NLU:"
    ],
    [
        "What is the difference between NLP and CI?",
        "The below table shows the difference between NLP and CI:"
    ],
    [
        "What is Pragmatic Analysis?",
        "Pragmatic analysis is an important task in NLP for interpreting knowledge that is lying outside a given document. The aim of implementing pragmatic analysis is to focus on exploring a different aspect of the document or text in a language. This requires a comprehensive knowledge of the real world. The pragmatic analysis allows software applications for the critical interpretation of the real-world data to know the actual meaning of sentences and words.Example:Consider this sentence: ‘Do you know what time it is?’This sentence can either be asked for knowing the time or for yelling at someone to make them note the time. This depends on the context in which we use the sentence."
    ],
    [
        "What is Pragmatic Ambiguity?",
        "Pragmatic ambiguity refers to the multiple descriptions of a word or a sentence. An ambiguity arises when the meaning of the sentence is not clear. The words of the sentence may have different meanings. Therefore, in practical situations, it becomes a challenging task for a machine to understand the meaning of a sentence. This leads to pragmatic ambiguity.Example:Check out the below sentence.‘Are you feeling hungry?’The given sentence could be either a question or a formal way of offering food."
    ],
    [
        "What are unigrams, bigrams, trigrams, and n-grams in NLP?",
        "When we parse a sentence one word at a time, then it is called a unigram. The sentence parsed two words at a time is a bigram.When the sentence is parsed three words at a time, then it is a trigram. Similarly, n-gram refers to the parsing of n words at a time.Example: To understand unigrams, bigrams, and trigrams, you can refer to the below diagram:Therefore, parsing allows machines to understand the individual meaning of a word in a sentence. Also, this type of parsing helps predict the next word and correct spelling errors."
    ],
    [
        "What are the steps involved in solving an NLP problem?",
        "Below are the steps involved in solving an NLP problem:Gather the text from the available dataset or by web scrapingApply stemming and lemmatization for text cleaningApply feature engineering techniquesEmbed using word2vecTrain the built model using neural networks or other Machine Learning techniquesEvaluate the model’s performanceMake appropriate changes in the modelDeploy the model"
    ],
    [
        "What is Feature Extraction in NLP?",
        "Features or characteristics of a word help in text or document analysis. They also help in sentiment analysis of a text. Feature extraction is one of the techniques that are used by recommendation systems. Reviews such as ‘excellent,’ ‘good,’ or ‘great’ for a movie are positive reviews, recognized by a recommender system. The recommender system also tries to identify the features of the text that help in describing the context of a word or a sentence. Then, it makes a group or category of the words that have some common characteristics. Now, whenever a new word arrives, the system categorizes it as per the labels of such groups."
    ],
    [
        "What are precision and recall?",
        "The metrics used to test an NLP model are precision, recall, and F"
    ],
    [
        "Also, we use accuracy for evaluating the model’s performance.",
        "The ratio of prediction and the desired output yields the accuracy of the model.Precision is the ratio of true positive instances and the total number of positively predicted instances.Recall is the ratio of true positive instances and the total actual positive instances."
    ],
    [
        "What is F1 score in NLP?",
        "F1 score evaluates the weighted average of recall and precision. It considers both false negative and false positive instances while evaluating the model. F1 score is more accountable than accuracy for an NLP model when there is an uneven distribution of class. Let us look at the formula for calculating F1 score:Advanced NLP Interview Questions"
    ],
    [
        "How to tokenize a sentence using the nltk package?",
        "Tokenization is a process used in NLP to split a sentence into tokens. Sentence tokenization refers to splitting a text or paragraph into sentences.For tokenizing, we will import sent_tokenize from the nltk package: from nltk.tokenize import sent_tokenize<><br>We will use the below paragraph for sentence tokenization:Para = “Hi Guys. Welcome to Intellipaat. This is a blog on the NLP interview questions and answers.” sent_tokenize(Para)Output: [ 'Hi Guys.' ,<br>'Welcome to Intellipaat. ',<br>'This is a blog on the NLP interview questions and answers. ' ]<br>Tokenizing a word refers to splitting a sentence into words.Now, to tokenize a word, we will import word_tokenize from the nltk package. from nltk.tokenize import word_tokenizePara = “Hi Guys. Welcome to Intellipaat. This is a blog on the NLP interview questions and answers.” word_tokenize(Para)Output: [ 'Hi' , 'Guys' , ' . ' , 'Welcome' , 'to' , 'Intellipaat' , ' . ' , 'This' , 'is' , 'a', 'blog' , 'on' , 'the' , 'NLP' , 'interview' , 'questions' , 'and' , 'answers' , ' . ' ]Excel in Every Aspect of Artificial Intelligence Get Certified in Artificial IntelligenceExplore Program"
    ],
    [
        "Explain how we can do parsing.",
        "Parsing is the method to identify and understand the syntactic structure of a text. It is done by analyzing the individual elements of the text. The machine parses the text one word at a time, then two at a time, further three, and so on.When the machine parses the text one word at a time, then it is a unigram.When the text is parsed two words at a time, it is a bigram.The set of words is a trigram when the machine parses three words at a time.Look at the below diagram to understand unigram, bigram, and trigram.Now, let’s implement parsing with the help of the nltk package. import nltk<br>text = ”Top 30 NLP interview questions and answers”We will now tokenize the text using word_tokenize. text_token= word_tokenize(text)Now, we will use the function for extracting unigrams, bigrams, and trigrams. list(nltk.unigrams(text))Output: [ \"Top 30 NLP interview questions and answer\"]  list(nltk.bigrams(text))Output: [\"Top 30\", \"30 NLP\", \"NLP interview\", \"interview questions\", \"questions and\", \"and answer\"]  list(nltk.trigrams(text))Output: [\"Top 30 NLP\", \"NLP interview questions\", \"questions and answers\"]For extracting n-grams, we can use the function nltk.ngrams and give the argument n for the number of parsers. list(nltk.ngrams(text,n))"
    ],
    [
        "Explain Stemming with the help of an example.",
        "In Natural Language Processing, stemming is the method to extract the root word by removing suffixes and prefixes from a word.For example, we can reduce ‘stemming’ to ‘stem’ by removing ‘m’ and ‘ing.’We use various algorithms for implementing stemming, and one of them is PorterStemmer.First, we will import PorterStemmer from the nltk package. from nltk.stem import PorterStemmerCreating an object for PorterStemmer pst=PorterStemmer()<br>pst.stem(“running”), pst.stem(“cookies”), pst.stem(“flying”)Output: (‘run’, ‘cooki', ‘fly’ )"
    ],
    [
        "Explain Lemmatization with the help of an example.",
        "We use stemming and lemmatization to extract root words. However, stemming may not give the actual word, whereas lemmatization generates a meaningful word.In lemmatization, rather than just removing the suffix and the prefix, the process tries to find out the root word with its proper meaning.Example: ‘Bricks’ becomes ‘brick,’ ‘corpora’ becomes ‘corpus,’ etc.Let’s implement lemmatization with the help of some nltk packages.First, we will import the required packages. from nltk.stem import wordnet<br>from nltk.stem import WordnetLemmatizerCreating an object for WordnetLemmatizer() lemma= WordnetLemmatizer()<br>list = [“Dogs”, “Corpora”, “Studies”]<br>for n in list:<br>print(n + “:” + lemma.lemmatize(n))Output: Dogs: Dog<br>Corpora: Corpus<br>Studies: Study"
    ],
    [
        "What is Parts-of-speech Tagging?",
        "The parts-of-speech (POS) tagging is used to assign tags to words such as nouns, adjectives, verbs, and more. The software uses the POS tagging to first read the text and then differentiate the words by tagging. The software uses algorithms for the parts-of-speech tagging. POS tagging is one of the most essential tools in Natural Language Processing. It helps in making the machine understand the meaning of a sentence.We will look at the implementation of the POS tagging using stop words.Let’s import the required nltk packages. import nltk<br>from nltk.corpus import stopwords<br>from nltk.tokenize import word_tokenize, sent_tokenize<br>stop_words = set(stopwords.words('english'))<br>txt = \"Sourav, Pratyush, and Abhinav are good friends.\"Tokenizing using sent_tokenize tokenized_text = sent_tokenize(txt)To find punctuation and words in a string, we will use word_tokenizer and then remove the stop words. for n in tokenized_text:<br>wordsList = nltk.word_tokenize(i)<br>wordsList = [w for w in wordsList if not w instop_words]Now, we will use the POS tagger. tagged_words = nltk.pos_tag(wordsList)<br>print(tagged_words)Output: [('Sourav', 'NNP'), ('Pratyush', 'NNP'), ('Abhinav', 'NNP'), ('good', 'JJ'), ('friends', 'NNS')]"
    ],
    [
        "Explain Named Entity Recognition by implementing it.",
        "Named Entity Recognition (NER) is an information retrieval process. NER helps classify named entities such as monetary figures, location, things, people, time, and more. It allows the software to analyze and understand the meaning of the text. NER is mostly used in NLP, Artificial Intelligence, and Machine Learning. One of the real-life applications of NER is chatbots used for customer support.Let’s implement NER using the spaCy package.Importing the spaCy package: import spacy<br>nlp = spacy.load('en_core_web_sm')<br>Text = \"The head office of Google is in California\"<br>document = nlp(text)for ent in document.ents:<br>print(ent.text, ent.start_char, ent.end_char, ent.label_)Output: Office 9 15 Place<br>Google 19 25 ORG<br>California 32 41 GPENote: Office 9 15 Place means word starts at 9th position when tokenized and ends at 15, this is inclusive of spaces."
    ],
    [
        "How to check word similarity using the spaCy packageTo find out the similarity among words, we use word similarity.",
        "We evaluate the similarity with the help of a number that lies between 0 and"
    ],
    [
        "We use the spacy library to implement the technique of word similarity.",
        "import spacy<br>nlp = spacy.load('en_core_web_md')<br>print(\"Enter the words\")<br>input_words = input()<br>tokens = nlp(input_words)<br>for i in tokens:<br>print(i.text, i.has_vector, i.vector_norm, i.is_oov)<br>token_1, token_2 = tokens[0], tokens[1]<br>print(\"Similarity between words:\", token_1.similarity(token_2))Output: hot  True 5.6898586 False<br>cold True6.5396233 False<br>Similarity: 0.597265This means that the similarity between the words ‘hot’ and ‘cold’ is just 59 percent.We hope these Data Science training interview questions will help you prepare for your upcoming interviews. If you are looking to learn Data Science course online in a systematic manner with expert guidance and support then you can check our data science online course.Our Data Science Courses Duration and FeesProgram NameStart DateFeesData Science Course in BangaloreCohort Starts on: 19th Apr 2025₹69,027Data Science Course in HyderabadCohort Starts on: 26th Apr 2025₹69,027About the AuthorAkash PushkarPrincipal Data ScientistMeet Akash, a Principal Data Scientist with expertise in advanced analytics, machine learning, and AI-driven solutions. With a master’s degree from IIT Kanpur, Aakash combines technical knowledge with industry insights to deliver impactful, scalable models for complex business challenges.Recommended VideosData Science Full Course 2025Data Science Full CourseData Science CourseData Analytics Full Course - Beginner to AdvancedDevOps Course Free Full CourseSalesforce Full Course For BeginnersCloud Computing Course Free | Learn Cloud ComputingFull Stack Web Development Course×Recommended ProgramsData Science Course5 (92536)PGP in Data Science and Machine Learning Course5 (6230)Data Analytics Course5 (80663)Data Science Bootcamp5 (4598)Advanced Certification in Data Science and Artificial Intelligence Course5 (89994)Data Engineering Course5 (2818)Advanced Certification in Data Analytics for Business5 (3123)M.Tech in Artificial Intelligence and Machine Learning5 (25240)Recommended ArticlesWhat is Data Science?Updated on: Apr 9, 2025Data Science Tutorial for BeginnersUpdated on: Feb 17, 2025Top 110+ Data Science Interview Questions and Answers [2025]Updated on: Apr 7, 2025How to Become a Data ScientistUpdated on: Apr 2, 2025How to Build a Career in Data Science?Updated on: Jan 6, 2025Top Data Science Projects with Dataset [2025]Updated on: Mar 5, 2025×Course PreviewExpert-Led No.1Recommended Videos By providing your contact details, you agree to our Terms of Use & Privacy Policyfacebooktwitter linkedin youtube insta telegram facebook twitter linkedin youtube insta telegramGet Our App Now!Get Our App Now!CoursesData Scientist CourseMachine Learning CoursePython CourseDevops TrainingBusiness Analyst CertificationCyber Security CoursesBusiness Analytics TrainingInvestment Banking CourseSQL CourseAWS DevOps CourseFull Stack Developer CourseProduct Management CourseCoursesAWS Solutions ArchitectUI UX Design CourseSalesforce TrainingSelenium TrainingArtificial Intelligence CourseEthical Hacking CourseAzure Administrator CertificationCyber Security CourseDigital Marketing CourseElectric Vehicle CourseAzure DevOps CourseWeb Development CoursesTutorialsPython TutorialAWS TutorialDevops TutorialJava TutorialNode Js TutorialCyber Security Tutorial Salesforce TutorialAzure TutorialEthical Hacking TutorialData Science TutorialCloud Computing CoursesPython Data Science CourseInterview QuestionsPython Interview QuestionsAWS Interview QuestionsData Science Interview QuestionsDevops Interview QuestionsSalesforce Interview QuestionsJava Interview QuestionsSQL Interview QuestionsReact Interview QuestionsNode Js Interview QuestionsDigital Marketing Interview Questions Browse By DomainsData ScienceBig Data Analytics CoursesBusiness Intelligence CoursesSalesforce CoursesCloud Computing CoursesDigital Marketing CoursesAI & Machine Learning CoursesProgramming CoursesDatabase CoursesProject Management CoursesCyber Security and Ethical Hacking CoursesWeb Development CoursesSoftware Testing CoursesAutomation CoursesJob Oriented CoursesDegree CoursesTop TutorialsMachine Learning TutorialPower BI TutorialSQL TutorialArtificial Intelligence TutorialDigital Marketing TutorialData Analytics TutorialUI/UX TutorialTop ArticlesCloud ComputingData ScienceMachine LearningWhat is AWSDigital MarketingCyber SecuritySalesforceArtificial IntelligenceTop Interview QuestionsSelenium Interview QuestionsAzure Interview QuestionsMachine Learning Interview QuestionsCyber Security Interview Questions Business Analyst Interview Questions and Answers C Interview QuestionsData Analyst Interview QuestionsSoftware Engineering Interview Questions© Copyright 2011 - 2025 Intellipaat Software Solutions Pvt. Ltd.Media Contact Us Tutorials Interview Questions Address: 6th Floor, Primeco Towers, Arekere Gate Junction, Bannerghatta Main Road, Bengaluru, Karnataka 560076, India.Disclaimer: The certification names are the trademarks of their respective owners.INTPL_2025-04-17"
    ],
    [
        "Please Explain Machine Learning, Artificial Intelligence, And Deep Learning?",
        "Machine learning is defined as a subset of Artificial Intelligence, and it contains the techniqueswhich enable computers to sort things out from the data and deliver Artificial Intelligenceapplications. Artificial Intelligence (AI) is a branch of computer science that is mainly focused onbuilding smart machines that can perform certain tasks that mainly require human intelligence. Itis the venture to replicate or simulate human intelligence in machines.Deep learning can be defined as a class of machine learning algorithms in Artificial Intelligencethat mainly uses multiple layers to cumulatively extract higher-level features from the given rawinput."
    ],
    [
        "How Difficult Is Machine Learning?",
        "Machine Learning is huge and comprises a lot of things. Therefore, it will take more than sixmonths to learn Machine Learning if you spend at least 6-7 hours per day. If you have goodhands-on mathematical and analytical skills, then six months will be sufficient for you."
    ],
    [
        "Can You Explain Kernel Trick In An SVM Algorithm?",
        "A Kernel Trick is a method where the Non-Linear data is projected onto a bigger dimensionspace in order to make it easy to classify the data where it can be linearly divided by a plane."
    ],
    [
        "Can You List Some Of The Popular Cross-Validation Techniques?",
        "1. Holdout Method: This kind of technique works by removing the part of the training dataset and sending the same to the model that was trained on the remaining data set to getthe required predictions."
    ],
    [
        "K-Fold Cross-Validation: Here, the data is divided into k subsets so that every time, oneamong the k subsets can be used as a validation set, and the other k-1 subsets are usedas the training set3. Stratified K-Fold Cross-Validation: It works on imbalanced data.4. Leave-P-Out Cross-Validation: Here, we leave p data points out of the training data outof the n data points, then we use the n-p samples to train the model and p points for thevalidation set. 5. Differences Between The Bagging And Boosting Algorithms?",
        "Bagging Boosting It is a method that merges the same type ofpredictions. It is a method that merges the different typesof predictions. It decreases the variance, not the bias It decreases the bias, not the variance. Each and every model receives equal weight Models are weighed based on performance."
    ],
    [
        "What Are Kernels In SVM?",
        "Can You List Some Popular Kernels Used In SVM? The kernel is basically used to set mathematical functions that are used in the Support VectorMachine by providing the window to manipulate the data. Kernel Function is used to transformthe training set of data so that a non-linear decision surface will be transformed to a linearequation in a bigger number of dimension spaces.Some of the popular kernels used in SVM are:"
    ],
    [
        "Polynomial kernel2. Gaussian kernel3. Gaussian radial basis function (RBF)4. Laplace RBF kernel5. Hyperbolic tangent kernel6. Sigmoid kernel7. Bessel function of the first kind Kernel8. ANOVA radial basis kernel 7. Can You Explain The OOB Error?",
        "An out-of-bag error called OBB error, also known as an out-of-bag estimate, is a technique tomeasure the prediction error of random forests, boosted decision trees. Bagging mainly usessubsampling with replacement to create the training samples for the model to learn from them."
    ],
    [
        "Can You Differentiate Between K-Means And KNN Algorithms?",
        "K-Means KNN algorithms It is unsupervised machine learning. It is supervised machine learning. It is a clustering machine learning algorithm. It is a classification or regression machinelearning algorithm. Its performance is slow. It performs much better. It is an eager learner. It is a lazy learner."
    ],
    [
        "Explain The Term Variance Inflation Factor Mean?",
        "Variance inflation factor known as VIF is a measure of the amount of multicollinearity in thegiven set of multiple regression variables. The ratio here is calculated for each of theindependent variables. A high VIF means that the associated independent variable is mostlycollinear with the other variables in the model."
    ],
    [
        "Explain SVM (Support Vector Machines) In Machine Learning?",
        "Support Vector Machine, known as SVM, is one of the most commonly used SupervisedLearning algorithms that is mainly used for Classification as well as Regression problems. It isprimarily used for Classification problems in Machine Learning. The main aim of the SVMalgorithm is to create the best decision boundary, which segregates n-dimensional space intoclasses so that one can easily put the new obtained data point in the correct category in thefuture."
    ],
    [
        "Differentiate Between Supervised And Unsupervised Machine Learning?",
        "Supervised Model Unsupervised Model Here, the algorithm learns on a labeleddataset, Here, it provides unlabeled data. Here, the models need to find the mappingfunction that is used to map the input variable(X) with the output variable (Y). The main aim of unsupervised learning is tofind the structure and patterns from the giveninput data."
    ],
    [
        "Explain The Terms Precision And Recall?",
        "Precision, also known as a positive predictive value, is defined as the fraction of relevantinstances among the retrieved instances.Precision = TP/TP+FPWhere TP is true positiveFP id False PositiveRecall, also known as sensitivity, is defined as the fraction of relevant instances that wereRetrieved.Recall = TP/TP+FP.Where TP is true positiveFP is False positive."
    ],
    [
        "Differentiate Between L1 And L2 Regularization?",
        "L1 Regularization L2 Regularization A regression model that makes use of the L1regularization process is called LassoRegression. A regression model that makes use of the L1regularization process is called RidgeRegression. Lasso Regression adds the absolute value ofthe magnitude of coefficient as a penalty termto the loss function. Ridge regression adds the squaredmagnitude of coefficient as a penalty term tothe loss function. It tries to estimate the median of the data. It tries to estimate the mean of the data."
    ],
    [
        "Explain Fourier Transform?",
        "The Fourier transform is a way to split something up into a bunch of sine waves. In terms ofmathematics, The Fourier Transform is a process that can transform a signal into its respectiveconstituent components and frequencies. Fourier transform is used not only in signal, radio,acoustic, etc."
    ],
    [
        "What Is The F1 Score?",
        "How To Use It? The F1-score combines both the precision and recall of a classifier into one single metric bytaking the harmonic mean. It is used to compare the performances of two classifiers. Forexample, classifier X has a higher recall, and classifier Y has higher precision. Now theF1-scores calculated for both the classifiers will be used to predict which one produces thebetter results.The F1 score can be calculated as2(P*R)/(P+R)Where P is the precision.R is the Recall of the classification model.Machine Learning Interview Questions And Answers"
    ],
    [
        "Differentiate Between Type I And Type II Error?",
        "Type I Error Type II Error It is equivalent to a False positive. It is equivalent to a False negative It refers to non-acceptance of hypothesis It refers to the acceptance of the hypothesis There can be a rejection even with anauthorized match. There can be an acceptance even with anunauthorized match."
    ],
    [
        "Can You Explain How A ROC Curve Works?",
        "The ROC curve is represented graphically by plotting the true positive rate (TPR) against theFPR (False Positive rates). Where"
    ],
    [
        "The true positive rate can be defined as the proportion of observations that are predictedto be positive out of all the given positive observations (TP/(TP + FN)) 2. The false-positive rate is defined as the proportion of observations that are predictedwrongly to be positive out of all the given negative observations.(FP/(TN + FP)) 18. Differentiate Between Deep Learning And Machine Learning?",
        "Deep Learning Machine Learning It is a subset of Machine Learning It is a superset of Deep Learning. It solves complex issues. It is used to learn new things. It is an evolution to Machine Learning. It is an evolution of AI. Here, algorithms are largely self-depicted onthe data analysis Algorithms are detected by the data analysts."
    ],
    [
        "Can You Name The Different Machine Learning Algorithms?",
        "Different machine learning algorithms are listed below:"
    ],
    [
        "Decision trees, 2. Naive Bayes, 3. Random forest 4. Support vector machine 5. K-nearest neighbor, 6. K-means clustering, 7. Gaussian mixture model, 8. Hidden Markov model etc. 20. What Is AI?",
        "AI (Artificial intelligence) refers to the simulation of human intelligence in machines that areprogrammed to reflect like humans and imitate their actions.Examples: Face Detection and Recognition, Google Maps, andRide-Hailing Applications, E-Payments."
    ],
    [
        "How To Select Important Variables While Working On A Data Set?",
        "1. You have to remove the correlated variables before selecting important variables."
    ],
    [
        "Make use of linear regression and select the variables based on their p values. 3. Use Forward Selection, Stepwise Selection, and Backward Selection. 4. Use Random Forest, Xgboost, and plot variable importance chart 5. Use the Lasso Regression 6. You have to select top n features by measuring the information gain for the available setof features. 22. Differentiate Between Causality And Correlation?",
        "The Causality explicitly applies to the cases where action A causes the outcome of action B.Correlation can simply be defined as a relationship. Where the actions of A can relate to theactions of B, but here it is not necessary for one event to cause the other event to happen."
    ],
    [
        "What Is Overfitting?",
        "Overfitting is a type of modeling error that results in the failure to predict or guess the futureobservations effectively or fit additional data in the model that already exists."
    ],
    [
        "Explain The Terms Standard Deviation And Variance?",
        "A standard deviation is defined as the number that specifies how spread out the values are. Alow standard deviation represents that most of the numbers are close to the mean value. Thehigher standard deviation means that the values are spread out over, the wider range.Variance in Machine Learning is a type of error that occurs due to the model’s sensitivity tosmall fluctuations in the given training set."
    ],
    [
        "Explain Multilayer Perceptron And Boltzmann Machine?",
        "A Multilayer Perceptron (MLP) is defined as a class of artificial neural networks that cangenerate a set of outputs from the set of given inputs. An MLP consists of several layers of inputnodes that are connected as a directed graph between input and output layers.The main purpose of the Boltzmann Machine is to optimize the solution to a given problem. It ismainly used to optimize the weights and quantity related to that specified problem.Machine Learning Interview Questions And Answers"
    ],
    [
        "Explain The Term Bias?",
        "Data bias in machine learning is defined as a type of error where certain elements of a givendataset are weighted more heavily than others. A biased dataset will not accurately representthe model’s use case, and it results in low accuracy levels and analytical errors."
    ],
    [
        "Name The Types Of Machine Learning?",
        "The types of machine learning are listed below:"
    ],
    [
        "Supervised Learning 2. Unsupervised Learning 3. Reinforcement Learning 28. Differentiate Between Classification And Regression?",
        "Classification Regression It is about predicting a label It is about predicting a quantity Here, the data is labeled in one or multipleclasses. Here, you need to predict the quantitycontinuously. It may predict a continuous value. It may predict a discrete value. It can be evaluated using accuracy. It can be evaluated using root mean squarederror."
    ],
    [
        "What Is A Confusion Matrix?",
        "In the field of machine learning, a confusion matrix also called an error matrix, is defined as aspecific table layout that allows the user to visualize the performance of an algorithm, mainly asupervised learning one."
    ],
    [
        "When Your Dataset Is Suffering From High Variance, How Would You Handle It?",
        "For datasets with high variance, we can make use of the bagging algorithm. The baggingalgorithm splits the data into different subgroups with sampling replicated from random data.Once the data is split, using a training algorithm, the random data can be used to create rules.Then we make use of the polling technique to gather all the predicted outcomes of the model."
    ],
    [
        "Differentiate Between Inductive And Deductive Learning?",
        "Inductive Learning Deductive Learning It aims at developing a theory. It aims at testing an existing theory. It moves from the specific observations to thebroad generalizations If there is no theory, you cannot conductdeductive research. It consists of threestages,ObservationObserve apatternDevelop a theory It consists of four stages:Start with anexisting theoryFormulate a hypothesis basedon existing theoryCollect data to test thehypothesisAnalyze the results"
    ],
    [
        "Explain The Handling Of Corrupted Values In The Given Dataset?",
        "The below are the ways to handle missing data?"
    ],
    [
        "Remove the rows with missing values. 2. Build another predictive model so that you can predict the missing values. 3. Use a model in such a way that it can incorporate missing data. 4. You need to replace the missing data with the aggregated values. 5. You can predict the missing values. 6. create an unknown category 33. Which Among These Is More Important Model Accuracy Or Model Performance?",
        "Model accuracy is considered as the important characteristic of a Machine Language /AI model.Whenever we discuss the performance of the model, we first clarify whether it is the modelscoring performance or Model training performance.Model performance is improved by using distributed computing and parallelizing over the givenscored assets, but we need to carefully build the accuracy during the model training process."
    ],
    [
        "What Is A Time Series?",
        "The time series in Machine learning is defined as a set of random variables that are orderedwith respect to time. Time series are studied to interpret a phenomenon, identify thecomponents of a trend, cyclicity, and predict its future values."
    ],
    [
        "Differentiate Between Entropy And Information Gain?",
        "The Information Gain is defined as the amount of information gained about a signal or randomvariable from observing another random variable.Entropy can be defined as the average rate at which information is produced by the stochasticsource of data, Or it can be defined as a measure of the uncertainty that is associated with arandom variable."
    ],
    [
        "Differentiate Between Stochastic Gradient Descent (SGD) And Gradient Descent (GD)?",
        "Batch Gradient Descent is involved in calculations over the full training set of each step, whichresults in a very slow process on very large training data. Hence, it becomes very expensive todo Batch GD. However, It is great for relatively smooth error manifolds. Also, it scales well withthe number of features.Stochastic Gradient Descent tries to solve the primary problem in Batch Gradient descent that isthe usage of entire training data to calculate the gradients as each step. SGD is stochastic innature means it picks up some “random” instances of training data at each and every step, andthen it computes the gradient making it faster as there are very little data to manipulate at oneshot,Batch Gradient Descent Stochastic Gradient Descent It computes the gradient using the entireTraining sample. It computes gradient using a single Trainingsample. It can’t be suggested for huge trainingsamples. It can be suggested for large trainingsamples. It is deterministic in nature. It is sophisticated in nature."
    ],
    [
        "Differentiate Between Gini Impurity And Entropy In A Decision Tree?",
        "Gini Entropy It has values inside the interval [0, 0.5] It has values inside the interval [0, 1] It is more complex. It is not complex. Its measurement is the probability of arandom sample that is being classifiedcorrectly. It is a measurement to calculate the lack ofinformation,"
    ],
    [
        "Mention Some Of The Advantages And Disadvantages Of Decision Trees?",
        "Advantages of the decision tree:"
    ],
    [
        "Decision trees require less effort for data preparation during the pre-processing whencompared with other algorithms. 2. A decision tree doesn’t require the normalization of data. 3. It does not require scaling of data. 4. Missing values in the data do not affect the process of building a decision tree. 5. A Decision tree model is very easy to explain to technical teams and stakeholders. 39. Can You Explain The Ensemble Learning Technique In Machine Learning?",
        "Ensemble methods are the techniques used to create multiple models and combine them toproduce enhanced results. Ensemble methods usually produce more precise solutions than asingle model would.In Ensemble Learning, we divide the training data set into multiple subsets, where each subsetis then used to build a separate model. Once the models are trained, they are then combined topredict an outcome in such a way that there is a reduction in the variance of the output.Machine Learning Interview Questions And Answers"
    ],
    [
        "Explain The Terms Collinearity And Multicollinearity?",
        "Multicollinearity occurs when multiple independent variables are highly correlated with eachother in a regression model, which means that an independent variable can be predicted fromanother independent variable inside a regression model.Collinearity mainly occurs when two predictor variables in a multiple regression have somecorrelation."
    ],
    [
        "Differentiate Between Random Forest And Gradient Boosting Machines?",
        "Like random forests, gradient boosting is also a set of decision trees. The two primarydifferences are:"
    ],
    [
        "How trees are built: Each tree in the random forest is built independently, whereasgradient boosting builds only one tree at a time. 2. Combining results: random forests combine results at the end of the process byaveraging. Whereas gradient boosting combines results along the path. 42. Explain The Terms Eigenvectors And Eigenvalues?",
        "Eigenvectors are unit vectors, meaning their length or magnitude is equal to 1."
    ],
    [
        "They arereferred to as right vectors, which means a column vector.Eigenvalues are coefficients that are applied to eigenvectors that, in turn, give the vectors theirlength or magnitude. 43. Can You Explain Associative Rule Mining (ARM)?",
        "Association rule mining (ARM) aims to find out the association rules that will satisfy thepredefined minimum support and confidence from a database. AMO is mainly used to reducethe number of association rules with the new fitness functions that can incorporate frequentrules."
    ],
    [
        "What Is A/B Testing?",
        "A/B testing is defined as a basic randomized control experiment. It is used to compare twoversions of a variable to find out which one among them performs better in a controlledenvironment.A/B Testing can be best used to compare two models to check which one is thebest-recommended product to a customer."
    ],
    [
        "Explain Marginalisation And Its Process?",
        "Marginalization is a method that requires the summing of the possible values of one variable todetermine the marginal contribution of another variable.P(X=x) = ∑YP(X=x,Y)"
    ],
    [
        "What Is Cluster Sampling?",
        "Cluster sampling is defined as a type of sampling method. With cluster sampling, theresearchers usually divide the population into separate groups or sets, known as clusters. Then,a random sample of clusters is picked from the population. Then the researcher conducts theiranalysis on the data from the collected sampled clusters."
    ],
    [
        "Explain The Term“Curse Of Dimensionality”?",
        "The curse of dimensionality basically refers to the increase in the error with the increase in thenumber of features. It can be referred to the fact that algorithms are vigorous to design in highdimensions, and they often have a running time exponential in the dimensions."
    ],
    [
        "Can You Name A Few Libraries In Python Used For Data Analysis And ScientificComputations?",
        "1. NumPy"
    ],
    [
        "SciPy 3. Pandas 4. SciKit 5. Matplotlib 6. Seaborn 7. Bokeh 49. What Are Outliers?",
        "Mention The Methods To Deal With Outliers? An outlier can be defined as an object that deviates significantly from other objects. They can becaused by execution errors.The three main methods to deal with outliers are as follows:"
    ],
    [
        "Univariate method 2. Multivariate method 3. Minkowski error 50. List Some Popular Distribution Curves Along With Scenarios Where You Will Use Them InAn Algorithm?",
        "The most popular distribution curves are:Uniform distribution can be defined as a probability distribution that has a constant probability.Example: Rolling a single dice since it has multiple outcomes.The binomial distribution is defined as a probability with two possible outcomes only. Example: acoin toss. The result will either be heads or tails.Normal distribution specifies how the values of a variable are distributed. Example: The heightof students in a classroom.Poisson distribution helps to predict the probability of specific events that are happening whenyou know how often that event has occurred.The exponential distribution is mainly concerned with the amount of time until the specific eventoccurs. Example: how long a car battery could last, in months."
    ],
    [
        "Can You List The Assumptions For Data To Be Met Before Starting With Linear Regression?",
        "The assumptions to be met are:"
    ],
    [
        "Linear relationship 2. Multivariate normality 3. No or little multicollinearity 4. No auto-correlation 5. Homoscedasticity 52. Explain The Term Variance Inflation Factor Mean?",
        "Variance inflation factor that is VIF is defined as a measure of the amount of multicollinearity in agiven set of multiple regression variables.Mathematically, the Variance inflation factor for a regression model variable is equal to the ratioof the final model variance to the variance of a model that comprises that single independentvariable.This ratio is calculated for each of the independent variables. A high VIF represents that theassociated independent variable is hugely collinear with the other variables in the model."
    ],
    [
        "Can You Tell Us When The Linear Regression Line Stops Rotating Or Finds An OptimalSpot Where It Is Fitted On Data?",
        "The place where the highest RSquared value is found is where the line comes to rest.RSquared usually represents the amount of variance that is captured by the virtual linearregression line w.r.t the total variance captured by the dataset."
    ],
    [
        "Can You Tell Us Which Machine Learning Algorithm Is Known As The Lazy Learner AndWhy It Is Called So?",
        "KNN Machine Learning algorithm is called a lazy learner. K-NN is defined as a lazy learnerbecause it will not learn any machine-learned values or variables from the given training data,but dynamically it calculates the distance every time it wants to classify. Hence it memorizes thetraining dataset instead."
    ],
    [
        "Can You Tell Us What Could Be The Problem When The Beta Value For A Specific VariableVaries Too Much In Each Subset When Regression Is Run On Various Subsets Of The Dataset?",
        "The variations in the beta values in every subset suggest that the dataset is heterogeneous. Toovercome this problem, we use a different model for each of the clustered subsets of the givendataset, or we use a non-parametric model like decision trees."
    ],
    [
        "How To Choose A Classifier Based On A Training Set Data Size?",
        "If the training set is small in size, high bias or low variance models, for example, Naive Bayestends to perform better as they are less likely to overfit.If the training set is large in size, low bias or high variance models, for example, LogisticRegression, tend to perform better as they can reflect more complicated relationships."
    ],
    [
        "Differentiate Between Training Set And Test Set In A Machine Learning Model?",
        "Training set Test set 70% of the total data is taken as the trainingdataset. The remaining 30% is taken as a testingdataset. It is implemented to build up a model. It is used to validate the model built. It is a labeled data used to train the model. We usually test without labeled data and thenverify the results with labels."
    ],
    [
        "Explain A False Positive And False Negative And How Are They Significant?",
        "A false positive is a concept where you receive a positive result for a given test when youshould have actually received a negative result. It’s also called a “false alarm” or “false positiveerror.” It is basically used in the medical field, but it can also apply to software testing.Examples of False positive:"
    ],
    [
        "A pregnancy test is positive, where in fact, you are not pregnant. 2. A cancer screening test is positive, but you do not have the disease. 3. Prenatal tests are positive for Down’s Syndrome when your fetus does not have anydisorder. 4. Virus software on your system incorrectly identifies a harmless program as the maliciousone.A false negative is defined where a negative test result is wrong. In simple words, you get anegative test result, where you should have got a positive test result.For example, consider taking a pregnancy test, and you test as negative (not pregnant). But infact, you are pregnant.The false negative pregnancy test results due to taking the test too early, using the diluted urine,or checking the results very soon. Just about every medical test has the risk of a false negative. 59. Explain The Term Semi-Supervised Machine Learning?",
        "Semi-supervised learning is defined as an approach to machine learning that combines a lessamount of labeled data with a huge amount of unlabeled data during the training process. It fallsbetween unsupervised learning and supervised learning."
    ],
    [
        "Can You Tell Us The Applications Of Supervised Machine Learning In Modern Businesses?",
        "1. Healthcare Diagnosis"
    ],
    [
        "Fraud detection 3. Email spam detection 4. Sentimental analysis 61. Can You Differentiate Between Inductive Machine Learning And Deductive MachineLearning?",
        "Inductive Machine Learning Deductive Machine Learning A ⋀ B ⊢ A → B (Induction) A ⋀ (A –>B) ⊢ B(Deduction) It observes and learns from the set ofinstances, and then it draws the conclusion. It derives the conclusion first, and then itworks on it based on the previous decision. It is a Statistical machine learning like KNN orSVM, Machine learning algorithm to deductivereasoning using the decision tree."
    ],
    [
        "What Is Random Forest In Machine Learning?",
        "The random forest can be defined as a supervised learning algorithm that is used forclassifications and regression. Similarly, the random forest algorithm creates decision trees onthe data samples, and then it gets the prediction from each of the samples and finally selectsthe best one by means of voting."
    ],
    [
        "Explain The Trade-Off Between Bias And Variance?",
        "Bias can be defined as the assumptions made by the model to make the target function easy toapproximate.Variance is defined as the amount that the estimate of the target function will change given thedifferent training data.The trade-off is defined as the tension between the error introduced by bias and variance."
    ],
    [
        "Explain Pruning In Decision Trees, And How Is It Done?",
        "Pruning is a data compression process in machine learning and search algorithms that canreduce the size of the decision trees by removing certain sections of the tree that are non-criticaland unnecessary to classify instances. A tree that is too huge risks overfitting the training dataand is poorly generalizing to the new samples.Pruning can take place as follows."
    ],
    [
        "Top-down fashion (It will travel the nodes and trim subtrees starting at the root) 2. Bottom-up fashion (It will start at the leaf nodes)We have reduced the error algorithm for the pruning of decision trees. 65. How Reduced Error Algorithms Work For Pruning In Decision Trees?",
        "The reduced error algorithm works as follows:"
    ],
    [
        "It considers each node for pruning. 2. Pruning = removing the subtree at that node, then make it a leaf and assign the majorcommon class at that node. 3. A node is removed from the tree if the resulting tree performs worse than the original. 4. Nodes are removed iteratively by choosing the node in such a way that whose removalmostly increases the accuracy of the decision tree on the graph. 5. Pruning continues to perform until further pruning is harmful. 6. It uses training, test sets, and validations. It is an effective approach if a vast amount ofdata is available. 66. Explain The Term Decision Tree Classification?",
        "A decision tree builds classification models as a tree structure, with datasets broken up intosmaller subsets while developing the decision tree; basically, it is a tree-like way with branchesand nodes defined. Decision trees handle both categorical and numerical data."
    ],
    [
        "Explain Logistic Regression?",
        "Logistic regression analysis is a technique used to examine the association of independentvariables with one dichotomous dependent variable. This is in contrast to the linear regressionanalysis, where the dependent variable is a continuous variable.Every time the output of logistic regression is 0 or 1 with a threshold value of 0."
    ],
    [
        "Any valueabove 0.5 is taken as 1, and any point below 0.5 is taken as 0. 68. Name Some Methods Of Reducing Dimensionality?",
        "Some of the methods of reducing dimensionality are given below:"
    ],
    [
        "By combining features with feature engineering 2. Removing collinear features 3. using algorithmic dimensionality reduction. 69. What Is A Recommendation System?",
        "Recommendation systems mainly collect the customer data and auto analyze this data togenerate the customized recommendations for the customers. These systems mainly rely onimplicit data like browsing history and recent purchases and explicit data like ratings provided bythe customer."
    ],
    [
        "Explain The K Nearest Neighbor Algorithm?",
        "K-Nearest Neighbour is the simplest Machine Learning algorithm that is based on theSupervised Learning technique. It assumes the similarity between the new case or data and theavailable cases, and it puts the new case into a category that is similar to that of the availablecategories.For example, we have an image of a creature that looks similar to that of a cat and a dog, butwe want to know whether it is a cat or a dog. For this identification, we can make use of theKNN algorithm, as it works on a similarity basis. The KNN model will find the similarities of thenew data set to that of the cats and dogs images, and that is based on the similar features; it willput it in either a cat or a dog category."
    ],
    [
        "Considering A Given Long List Of Machine Learning Algorithms, Given A Data Set, How DoThe Spam Filters Of The Email Will Be Fed With Hundreds Of Emails You Decide Which One ToUse?",
        "Choosing an algorithm depends on the below-mentioned questions:"
    ],
    [
        "How much data you have, and is that continuous or categorical?",
        "2. Is the problem related to classification, clustering, association, or regression?"
    ],
    [
        "Is it a Predefined variable (labeled), unlabeled, or a mix of both?",
        "4. What is the primary purpose?Based on the above questions, one has to choose the right algorithm that suits theirrequirement."
    ],
    [
        "Can You Tell Us How To Design An Email Spam Filter?",
        "1. The spam filter of the email will be fed with hundreds of emails."
    ],
    [
        "Each of these emails has a label: ‘spam’ or ‘not spam.’ 3. The supervised machine learning algorithm will then identify which type of emails arebeing marked as spam based on spam keywords like the lottery, no money, full refund,etc. 4. The next time an email hits the inbox, the spam filter will use statistical analysis andalgorithms like Decision Trees and SVM to identify how likely the email is spam. 5. If the probability is high, then it will be labeled as spam, and the email will not hit yourinbox. 6. Based on the accuracy of each of the models, we use the algorithm with the highestreliability after testing all the given models. 73. How Can You Avoid Overfitting?",
        "Overfitting is avoided by following the steps:"
    ],
    [
        "Cross-validation: The idea here is to use the initial training data to generate varioussmall train test spills. Where these test spills are used to tune the model. 2. Train with more data: Training with a lot of data can help the algorithms to detect thesignals better. 3. Remove feature: You can manually remove some of the features. 4. Early stopping: It refers to stopping the training process before the learner passes thespecified point. 5. Regularization: It refers to a broad range of techniques for artificially forcing the model tobe simple. 6. Ensembling: These are machine learning algorithms that combine predictions frommultiple separate models. 74. Explain The Term Selection Bias In Machine Learning?",
        "Selection bias takes place if a data set’s examples are chosen in such a way that it is notreflective of their real-world distribution. Selection bias can take many various forms."
    ],
    [
        "Coverage bias: Data here is not selected in a representative manner. Example: A model is trained in such a way to predict the future sales of a new product based onthe phone surveys conducted with the sample of customers who bought the product.Consumers who instead opted for buying a competing product were not surveyed, and as aresult, this set of people were not represented in the training data. 2. Non-response bias: Data here ends up being unrepresentative due to the participationgaps in the collection of data processes.Example: A model is trained in such a way to predict the future sales of a new product basedon the phone surveys conducted with a sample of customers who bought the product and with asample of customers who bought the competing product. Customers who bought the competingproduct were 80% more expected to refuse to complete the survey, and their data wereunderrepresented in the sample. 3. Sampling bias: Here, proper randomization is not used during the data collectionprocess.Example: A model that is trained to predict the future sales of a new product based on thephone surveys conducted with a sample of customers who bought the product and with asample of customers who bought a competing product. Instead of randomly targetingcustomers, the surveyor chose the first 200 consumers that responded to their email, who mighthave been more eager about the product than the average purchasers. 75. Explain The Types Of Supervised Learning?",
        "Supervised learning is of two types, namely,"
    ],
    [
        "Regression: It is a kind of Supervised Learning that learns from the given LabelledDatasets, and then it is able to predict the continuous-valued output for the new data thatis given to the algorithm. It is used in cases where an output requirement is a numberlike money or height etc. Some popular Supervised Learning algorithms are LinearRegression, Logistic Regression. 2. Classification: It is a kind of learning where the algorithm needs to be mapped to the newdata that is obtained from any one of the two classes that we have in the dataset. Theclasses have to be mapped to either 1 or 0, which in real-life translates to the ‘Yes’ or‘No.’ The output will have to be either one of the classes, and it should not be a numberas it was in the case of Regression. Some of the most well-known algorithms areDecision trees, Naive Bayes Classifier, Support vector Algorithms. 76. What Vanishing Gradient Descent?",
        "In Machine Learning, we encounter the Vanishing Gradient Problem while training the NeuralNetworks with gradient-based methods like Back Propagation. This problem makes it hard totune and learn the parameters of the earlier layers in the given network.The vanishing gradients problem can be taken as one example of the unstable behavior that wemay encounter when training the deep neural network. It describes a situation where the deep multilayer feed-forward network or the recurrent neuralnetwork is not able to propagate the useful gradient information from the given output end of themodel back to the layers close to the input end of the model."
    ],
    [
        "Can You Name The Proposed Methods To Overcome The Vanishing Gradient Problem?",
        "The methods proposed to overcome the vanishing gradient problems are:"
    ],
    [
        "Multi-level hierarchy 2. The long short – term memory 3. Faster hardware 4. Residual neural networks (ResNets) 5. ReLU 78. Differentiate Between Data Mining And Machine Learning?",
        "Data Mining Machine Learning It extracts useful information from a largeamount of data. It introduces algorithms from data as well asfrom past experience. It is used to understand the flow of data. It teaches the computers to learn andunderstand from the data flow. It has huge databases with unstructureddata. It has existing data as well as algorithms. It requires human interference in it. No need for the human effort required afterdesign Models are developed using data miningtechnique machine-learning algorithm can be used inthe decision tree, neural networks, and someother parts of artificial intelligence It is more of research using methods likemachine learning. It is self-learned and trains the system to dointelligent tasks."
    ],
    [
        "Name The Different Algorithm Techniques In Machine Learning?",
        "The different algorithm techniques in machines learning are listed below:"
    ],
    [
        "Unsupervised Learning 2. Semi-supervised Learning 3. Transduction 4. Reinforcement Learning 5. Learning to Learn 6. Supervised Learning 80. Explain The Function Of ‘Unsupervised Learning?",
        "1. It has to find clusters of the data."
    ],
    [
        "Find the low-dimensional representations of the data 3. To find interesting directions in data 4. To calculate interesting coordinates and correlations. 5. Find novel observations or database cleaning. 81. Explain The Term Classifier In Machine Learning?",
        "A classifier in machine learning is defined as an algorithm that automatically categorizes thedata into one or more of a group of “classes.” One of the common examples is an emailclassifier that can scan the emails to filter them by the given class labels: Spam or Not Spam.We have five types of classification algorithms, namely,"
    ],
    [
        "Decision Tree 2. Naive Bayes Classifier 3. K-Nearest Neighbors 4. Support Vector Machines 5. Artificial Neural Networks 82. What Are Genetic Algorithms ?",
        "Genetic algorithms are defined as stochastic search algorithms which can act on a population ofpossible solutions. Genetic algorithms are mainly used in artificial intelligence to search a spaceof potential solutions to find one who can solve the problem."
    ],
    [
        "Can You Name The Area Where Pattern Recognition Can Be Used?",
        "1. Speech Recognition"
    ],
    [
        "Statistics 3. Informal Retrieval 4. Bioinformatics 5. Data Mining 6. Computer Vision 84. Explain The Term Perceptron In Machine Learning?",
        "A Perceptron is defined as an algorithm for supervised learning of binary classifiers. Thisalgorithm enables the neurons to learn and processes the elements in the given training set oneat a time. There are two types of Perceptrons, namely."
    ],
    [
        "Single-layer 2. Multilayer. 85. What Is Isotonic Regression?",
        "Isotonic regression is used iteratively to fit ideal distances to protect the relative dissimilarityorder. Isotonic regression is also used in the probabilistic classification to balance the predictedprobabilities of the supervised machine learning models."
    ],
    [
        "What Are Bayesian Networks?",
        "A Bayesian network can be defined as a probabilistic graphical model that presents a set ofvariables and their conditional dependencies through a DAG (directed acyclic graph).For example, a Bayesian network would represent the probabilistic relationships between thediseases and their symptoms. Given the specific symptoms, the network can be used tocompute the possibilities of the presence of different diseases."
    ],
    [
        "Can You Explain The Two Components Of The Bayesian Logic Program?",
        "The bayesian logic program mainly comprises two components."
    ],
    [
        "The first component is the logical one: it comprises a set of Bayesian Clauses thatcaptures the qualitative structure of the domain. 2. The second component is quantitative: it encodes the quantitative information about thedomain. 88. What Is An Incremental Learning Algorithm In An Ensemble?",
        "The incremental learning method is defined as the ability of an algorithm to learn from new datathat is available after the classifier has already been generated from the already availabledataset."
    ],
    [
        "Name The Components Of Relational Evaluation Techniques?",
        "The components of the relational evaluation technique are listed below:"
    ],
    [
        "Data Acquisition 2. Ground Truth Acquisition 3. Cross-Validation Technique 4. Query Type 5. Scoring Metric 6. Significance Test 90. Can You Explain The Bias-Variance Decomposition Of Classification Error In The EnsembleMethod?",
        "The expected error of the learning algorithm can be divided into bias and variance. A bias termis a measure of how closely the average classifier produced by the learning algorithm matcheswith the target function. The variance term is a measure of how much the learning algorithm’sprediction fluctuates for various training sets."
    ],
    [
        "Name The Different Methods For Sequential Supervised Learning?",
        "The different methods for sequential supervised learning are given below:"
    ],
    [
        "Recurrent sliding windows 2. Hidden Markow models 3. Maximum entropy Markow models 4. Conditional random fields 5. Graph transformer networks 6. Sliding-window methods 92. What Is Batch Statistical Learning?",
        "A training dataset is divided into one or more batches. When all the training samples are used inthe creation of one batch, then that learning algorithm is known as batch gradient descent.When the given batch is the size of one sample, then the learning algorithm is called stochasticgradient descent."
    ],
    [
        "Can You Name The Areas In Robotics And Information Processing Where SequentialPrediction Problem Arises?",
        "The areas in robotics and information processing where sequential prediction problem arisesare given below"
    ],
    [
        "Structured prediction 2. Model-based reinforcement learning 3. Imitation Learning 94. Name The Different Categories You Can Categorize The Sequence Learning Process?",
        "The different categories where you can categorize the sequence learning process are listedbelow:"
    ],
    [
        "Sequence generation 2. Sequence recognition 3. Sequential decision 4. Sequence prediction 95. What Is Sequence Prediction?",
        "Sequence prediction aims to predict elements of the sequence on the basis of the precedingelements.A prediction model is trained with the set of training sequences. On training, the model is usedto perform sequence predictions. A prediction comprises predicting the next items of asequence. This task has a number of applications like web page prefetching, weatherforecasting, consumer product recommendation, and stock market prediction.Examples of sequence prediction problems include:"
    ],
    [
        "Weather Forecasting. Given a sequence of observations about the particular weatherover a period of time, it predicts the expected tomorrow’s weather. 2. Stock Market Prediction. Given a sequence of movements of the security over a periodof time, it predicts the next movement of the security. 3. Product Recommendation. Given a sequence of the last purchases of a customer, itpredicts the next purchase of a customer. 96. Explain PAC Learning?",
        "Probably approximately correct, i.e., PAC learning is defined as a theoretical framework used foranalyzing the generalization error of the learning algorithm in terms of its error on a giventraining set and some measures of the complexity. The main goal here is to typically show thatan algorithm can achieve low generalization error with high probability."
    ],
    [
        "What Are PCA, KPCA, And ICA, And What Are They Used For?",
        "Principal Components Analysis(PCA): It linearly transforms the original inputs into the newuncorrelated features.Kernel-based Principal Component Analysis(KCPA): It is a nonlinear PCA developed by usingthe kernel method.Independent Component Analysis(ICA): In ICA, the original inputs are linearly transformed intocertain features that are mutually statistically independent."
    ],
    [
        "Explain The Three Stages Of Building A Model In Machine Learning?",
        "The three stages are:"
    ],
    [
        "Model Building 2. Model Testing 3. Applying the model 99. Explain The Term Hypothesis In ML?",
        "Machine Learning, especially supervised learning, can be specified as the desire to use theavailable data to learn a function that best maps the inputs to outputs.Technically, this problem is called function approximation, where we are approximating anunknown target function that we assume as it exists that can best map the given inputs tooutputs on all possible considerations from the problem domain.An example of the model that approximates the target function and performs the mappings ofinputs to the outputs is known as the hypothesis in machine learning.The choice of algorithm and the configuration of the algorithm define the space of possiblehypotheses that the model may constitute."
    ],
    [
        "Explain The Terms Eepoch, Eentropy, Bbias, And Vvariance In Machine Learning?",
        "Epoch is a term widely used in machine learning that indicates the number of passes of thewhole training dataset that the machine learning algorithm has completed. If the batch size isthe entire training dataset, then the number of epochs is defined as the number of iterations.Entropy in Machine learning can be defined as the measure of disorder or uncertainty. The maingoal of machine learning models and Data Scientists, in general, is to decrease uncertainty.Data bias is a type of error in which certain elements of a dataset are more heavily weightedthan others.Variance is defined as the amount that the estimate of the target function will change if adifferent training data set was used. The target function is usually estimated from the trainingdata by the machine learning algorithm."
    ],
    [
        "What is Deep Learning",
        "If you are going for a deep learning interview, you know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example. Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog)."
    ],
    [
        "What is a Neural Network",
        "Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler. The most common Neural Networks consist of three network layers:"
    ],
    [
        "An output layer Each layer contains neurons called “nodes,” performing various operations",
        "Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc."
    ],
    [
        "What Is a Multi-layer Perceptron(MLP)",
        "As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same structure as a single-layer perceptron with one or more hidden layers. A single-layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes. Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called “backpropagation.” In backpropagation, the neural network calculates the error with the help of the cost function. It propagates this error backward from where it came from (adjusts the weights to train the model more accurately)."
    ],
    [
        "What Is Data Normalization, and Why Do We Need It",
        "The process of standardizing and reforming data is called “Data Normalization.” It’s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence."
    ],
    [
        "What is the Boltzmann Machine",
        "One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected."
    ],
    [
        "What Is the Role of Activation Functions in a Neural Network",
        "At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions."
    ],
    [
        "What Is the Cost Function",
        "Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s performance is. It’s used to compute the error of the output layer during backpropagation. We push that error backward through the neural network and use that during the different training functions."
    ],
    [
        "What Is Gradient Descent",
        "Gradient Descent is an optimal algorithm to minimize the cost function or to minimize an error. The aim is to find the local-global minima of a function. This determines the direction the model should take to reduce the error."
    ],
    [
        "What Do You Understand by Backpropagation",
        "This is one of the most frequently asked deep learning interview questions. Backpropagation is a neural network technique to improve the performance of the network and minimize the cost function. It backpropagates the error and updates the weights to reduce the error."
    ],
    [
        "What Is the Difference Between a Feedforward Neural Network and Recurrent Neural Network",
        "In this deep learning interview question, the interviewee expects you to give a detailed answer. A Feedforward Neural Network signals travel in one direction from input to output. There are no feedback loops; the network considers only the current input. It cannot memorize previous inputs (e.g., CNN). A Recurrent Neural Network’s signals travel in both directions, creating a looped network. It considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory."
    ],
    [
        "What Are the Applications of a Recurrent Neural Network (RNN)",
        "The RNN can be used for sentiment analysis, text mining, and image captioning. Recurrent Neural Networks can also address time series problems such as predicting the prices of stocks in a month or quarter."
    ],
    [
        "What Are the Softmax and ReLU Functions",
        "Softmax is an activation function that generates the output between zero and one. It divides each output, such that the total sum of the outputs is equal to one. Softmax is often used for output layers.ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an output of X if X is positive and zeros otherwise. ReLU is often used for hidden layers."
    ],
    [
        "What Are Hyperparameters",
        "This is another frequently asked deep learning interview question. With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.)."
    ],
    [
        "What Will Happen If the Learning Rate Is Set Too Low or Too High",
        "When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point. If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (the model can give a good output) or even diverge (data is too chaotic for the network to train)."
    ],
    [
        "What Is Dropout and Batch Normalization",
        "Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping"
    ],
    [
        "percent of the nodes)",
        "It doubles the number of iterations needed to converge the network.Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and a standard deviation of one. The next step on this top Deep Learning interview questions and answers blog will be to discuss intermediate questions."
    ],
    [
        "What Is the Difference Between Batch Gradient Descent and Stochastic Gradient Descent",
        "Batch Gradient Descent Stochastic Gradient Descent The batch gradient computes the gradient using the entire dataset. It takes time to converge because the volume of data is huge, and weights update slowly. The stochastic gradient computes the gradient using a single sample. It converges much faster than the batch gradient because it updates weight more frequently."
    ],
    [
        "What is Overfitting and Underfitting, and How to Combat Them",
        "Overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. It is more likely to occur with nonlinear models that have more flexibility when learning a target function. An example would be if a model is looking at cars and trucks but only recognizes trucks that have a specific box shape. It might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. The model performs well on training data, but not in the real world. Underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. This usually happens when there is less and incorrect data to train a model. Underfitting has both poor performance and accuracy. To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model."
    ],
    [
        "How Are Weights Initialized in a Network",
        "There are two methods here: we can either initialize the weights to zero or assign them randomly. Initializing all weights to"
    ],
    [
        "This makes your model similar to a linear model",
        "All the neurons and every layer perform the same operation, giving the same output and making the deep net useless. Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to"
    ],
    [
        "It gives better accuracy to the model since every neuron performs different computations",
        "This is the most commonly used method."
    ],
    [
        "What Are the Different Layers on CNN",
        "There are four layers in CNN:"
    ],
    [
        "Convolutional Layer - the layer that performs a convolutional operation, creating several smaller picture windows to go over the data",
        ""
    ],
    [
        "ReLU Layer - it brings non-linearity to the network and converts all the negative pixels to zero",
        "The output is a rectified feature map."
    ],
    [
        "Pooling Layer - pooling is a down-sampling operation that reduces the dimensionality of the feature map",
        ""
    ],
    [
        "Fully Connected Layer - this layer recognizes and classifies the objects in the image",
        ""
    ],
    [
        "What is Pooling on CNN, and How Does It Work",
        "Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix."
    ],
    [
        "How Does an LSTM Network Work",
        "Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. There are three steps in an LSTM network:   Step"
    ],
    [
        "The network decides what to forget and what to remember",
        "Step"
    ],
    [
        "It selectively updates cell state values",
        "Step"
    ],
    [
        "The network decides what part of the current state makes it to the output",
        ""
    ],
    [
        "What Are Vanishing and Exploding Gradients",
        "While training an RNN, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a “Vanishing Gradient.” When the slope tends to grow exponentially instead of decaying, it’s referred to as an “Exploding Gradient.” Gradient problems lead to long training times, poor performance, and low accuracy."
    ],
    [
        "What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning",
        "Epoch - Represents one iteration over the entire dataset (everything put into the training model).   Batch - Refers to when we cannot pass the entire dataset into the neural network at once, so we divide the dataset into several batches.   Iteration - if we have 10,"
    ],
    [
        "divided by 50)",
        ""
    ],
    [
        "Why is TensorFlow the Most Preferred Library in Deep Learning",
        "TensorFlow provides both C++ and Python APIs, making it easier to work on, and has a faster compilation time compared to other Deep Learning libraries like Keras and Torch. TensorFlow supports both CPU and GPU computing devices."
    ],
    [
        "What Do You Mean by Tensor in TensorFlow",
        "This is another most frequently asked deep learning interview question. A tensor is a mathematical object represented as an array of higher dimensions. These arrays of data with different dimensions and ranks fed as input to the neural network are called “Tensors.”"
    ],
    [
        "What Are the Programming Elements in TensorFlow",
        "Constants - Constants are parameters whose value does not change. To define a constant we use the tf.constant() command. For example: a = tf.constant(2.0,tf.float32) b = tf.constant(3.0) Print(a, b) Variables - Variables allow us to add new trainable parameters to the graph. To define a variable, we use the tf.Variable() command and initialize them before running the graph in a session. An example: W = tf.Variable([.3].dtype=tf.float32) b = tf.Variable([-.3].dtype=tf.float32) Placeholders - these allow us to feed data to a TensorFlow model from outside a model. It permits a value to be assigned later. To define a placeholder, we use the tf.placeholder() command. An example: a = tf.placeholder (tf.float32)b = a*"
    ],
    [
        "with tf",
        "Session() as sess: result = sess.run(b,feed_dict={a:3.0}) print result Sessions - a session is run to evaluate the nodes. This is called the “TensorFlow runtime.” For example: a = tf.constant(2.0) b = tf.constant(4.0) c = a+b # Launch Session Sess = tf.Session() # Evaluate the tensor c print(sess.run(c))"
    ],
    [
        "Explain a Computational Graph",
        "Everything in TensorFlow is based on creating a computational graph. It has a network of nodes where each node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a “Dataflow Graph.”"
    ],
    [
        "Explain Generative Adversarial Network",
        "Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner’s check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic. The forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately. Let us understand this example with the help of the image shown above. There is a noise vector coming into the forger who is generating fake wine. Here the forger acts as a Generator. The shop owner acts as a Discriminator. The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake. So, there are two primary components of a Generative Adversarial Network (GAN) named:"
    ],
    [
        "Discriminator The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images",
        ""
    ],
    [
        "What Is an Auto-encoder",
        "This Neural Network has three layers in which the input neurons are equal to the output neurons. The network's target outside is the same as the input. It uses dimensionality reduction to restructure the input. It works by compressing the image input to a latent space representation and then reconstructing the output from this representation."
    ],
    [
        "What Are Bagging and Boosting",
        "Bagging and Boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call. With Bagging, we take a dataset and split it into training data and test data. Then we randomly select data to place into the bags and train the model separately. With Boosting, the emphasis is on selecting data points that give wrong output to improve accuracy."
    ],
    [
        "What is the significance of using the Fourier transform in Deep Learning tasks",
        "The Fourier transform function efficiently analyzes, maintains, and manages large datasets. You can use it to generate real-time array data that helps process multiple signals."
    ],
    [
        "What do you understand by transfer learning",
        "Name a few commonly used transfer learning models. Transfer learning is the process of transferring the learning from one model to another model without having to train it from scratch. It takes critical parts of a pre-trained model and applies them to solve new but similar machine learning problems. Some of the popular transfer learning models are:   VGG-"
    ],
    [
        "What is the difference between SAME and VALID padding in TensorFlow",
        "Using the TensorFlow library, tf.nn.max_pool performs the max-pooling operation. Tf.nn.max_pool has a padding argument that takes"
    ],
    [
        "values - SAME or VALID",
        "With padding == “SAME” ensures that the filter is applied to all the elements of the input. The input image gets fully covered by the filter and specified stride. The padding type is named SAME as the output size is the same as the input size (when stride=1). With padding == “VALID” implies there is no padding in the input image. The filter window always stays inside the input image. It assumes that all the dimensions are valid so that the input image gets fully covered by a filter and the stride defined by you."
    ],
    [
        "What are some of the uses of Autoencoders in Deep Learning",
        "Autoencoders are used to convert black and white images into colored images.   Autoencoder helps to extract features and hidden patterns in the data.  It is also used to reduce the dimensionality of data.   It can also be used to remove noises from images."
    ],
    [
        "What is the Swish Function",
        "Swish is an activation function proposed by Google which is an alternative to the ReLU activation function. It is represented as: f(x) = x * sigmoid(x). The Swish function works better than ReLU for a variety of deeper models. The derivative of Swish can be written as: y’ = y + sigmoid(x) * ("
    ],
    [
        "What are the reasons for the mini-batch gradient being so useful",
        "Mini-batch gradient is highly efficient compared to stochastic gradient descent.   It lets you attain generalization by finding the flat minima.   Mini-batch gradient helps avoid local minima to allow gradient approximation for the whole dataset."
    ],
    [
        "What do you understand by the Leaky ReLU activation function",
        "Leaky ReLU is an advanced version of the ReLU activation function. In general, the ReLU function defines the gradient to be"
    ],
    [
        "when all the values of inputs are less than zero",
        "This deactivates the neurons. To overcome this problem, Leaky ReLU activation functions are used. It has a very small slope for negative values instead of a flat slope."
    ],
    [
        "What is Data Augmentation in Deep Learning",
        "Data Augmentation is the process of creating new data by enhancing the size and quality of training datasets to ensure better models can be built using them. There are different techniques to augment data such as numerical data augmentation, image augmentation, GAN-based augmentation, and text augmentation."
    ],
    [
        "Explain the Adam optimization algorithm",
        "Adaptive Moment Estimation or Adam optimization is an extension of the stochastic gradient descent. This algorithm is useful when working with complex problems involving vast amounts of data or parameters. It needs less memory and is efficient. Adam optimization algorithm is a combination of two gradient descent methodologies - Momentum and Root Mean Square Propagation."
    ],
    [
        "Why is a convolutional neural network preferred over a dense neural network for an image classification task",
        "The number of parameters in a convolutional neural network is much more diminutive than that of a Dense Neural Network. Hence, a CNN is less likely to overfit.   CNN allows you to look at the weights of a filter and visualize what the network learned. So, this gives a better understanding of the model.   CNN trains models hierarchically, i.e., it learns the patterns by explaining complex patterns using simpler ones."
    ],
    [
        "Which strategy does not prevent a model from over-fitting to the training data",
        ""
    ],
    [
        "Early stopping Answer: b) Pooling - It’s a layer in CNN that performs a downsampling operation",
        ""
    ],
    [
        "Explain two ways to deal with the vanishing gradient problem in a deep neural network",
        "Use the ReLU activation function instead of the sigmoid function   Initialize neural networks using Xavier initialization that works with Tanh activation."
    ],
    [
        "Why is a deep neural network better than a shallow neural network",
        "Both deep and shallow neural networks can approximate the values of a function. But the deep neural network is more efficient as it learns something new in every layer. A shallow neural network has only one hidden layer. But a deep neural network has several hidden layers that create a deeper representation and computation capability."
    ],
    [
        "What is the need to add randomness in the weight initialization process",
        "If you set the weights to zero, then every neuron at each layer will produce the same result and the same gradient value during backpropagation. So, the neural network won’t be able to learn the function as there is no asymmetry between the neurons. Hence, randomness in the weight initialization process is crucial."
    ],
    [
        "How can you train hyperparameters in a neural network",
        "Hyperparameters in a neural network can be trained using four components: Batch size: Indicates the size of the input data. Epochs: Denotes the number of times the training data is visible to the neural network to train. Momentum: Used to get an idea of the next steps that occur with the data being executed. Learning rate: This represents the time required for the network to update the parameters and learn."
    ],
    [
        "What are the challenges of using NLP in multilingual contexts",
        "Using NLP in multilingual contexts presents challenges such as variations in syntax, grammar, and cultural nuances across languages. One major challenge is the lack of data for less-resourced languages, which can limit model accuracy. To address these challenges, we use cross-lingual transfer learning and multilingual embeddings, which allow models to share knowledge across languages and improve performance, even with limited data for certain languages."
    ],
    [
        "How do you ensure that NLP models do not perpetuate biases present in training data.",
        "To prevent NLP models from perpetuating biases, I first assess the training data for such language. During model training, I apply techniques like annotating or altogether removing prejudiced data points. Additionally, I conduct thorough evaluations to identify and correct biased outputs, ensuring that models are fair and objective. This process is especially critical in sensitive applications like hiring employees, law enforcement, and customer service, where biased decisions can have significant consequences."
    ],
    [
        "How can NLP be used for language translation.",
        "NLP can be used for language translation through systems like statistical machine translation and neural machine translation. NMT uses deep learning to understand and translate text with human-like fluency. These models are trained on large datasets to handle various languages and contexts, improving their ability to translate idiomatic expressions and maintain high translation quality."
    ],
    [
        "What is the role of Named Entity Recognition (NER) in NLP.",
        "Named entity recognition is an NLP technique that identifies and classifies named entities in text into categories like personal names, organizations, and locations. NER is crucial for information extraction—helping to organize and retrieve structured information from unstructured text. It's widely used in applications like content categorization, question answering, and improving search engine accuracy by understanding the specific entities mentioned in queries."
    ],
    [
        "How do you approach sentiment analysis in NLP.",
        "In sentiment analysis, we identify and classify the sentiment expressed in text, which can be positive, negative, or neutral. Lexicon-based approaches use predefined dictionaries to score sentiments, while machine learning models train on labeled data to learn sentiment patterns.Advanced techniques like transformers enhance accuracy by understanding the context and nuances, such as sarcasm or negation, which are crucial for accurate sentiment detection."
    ],
    [
        "How do transformers like BERT differ from traditional NLP models.",
        "Transformers, such as BERT, differ from traditional NLP models by employing a self-attention mechanism that allows them to understand the context of each word in a sentence from both directions (bidirectionally). This capability enables transformers to capture long-range dependencies more effectively and understand the nuance and context of language better. As a result, transformers excel in tasks like question answering, text classification, and machine translation, offering significant improvements over earlier models."
    ],
    [
        "Can you explain the difference between stemming and lemmatization.",
        "Stemming and lemmatization are both techniques used to reduce words to their base forms.Stemming is a rule-based approach that cuts off suffixes, often resulting in non-standard words, like ‘comput’ from ‘computing’ or ‘computer.’ Lemmatization, however, uses vocabulary and morphological analysis to return the base or dictionary form of a word, such as 'run' from 'running' and 'better' from 'best.' The choice between the two depends on the task and the need for linguistic accuracy."
    ],
    [
        "What is the significance of embeddings in NLP, and how do they work.",
        "Embeddings are dense vector representations of words that encapsulate their semantic meaning within a continuous vector space. Techniques like Word2Vec, GloVe, and BERT generate these embeddings by analyzing the context in which words appear. This process helps capture semantic similarities and relationships between words, which is essential for tasks such as text classification, sentiment analysis, and machine translation."
    ],
    [
        "How do you handle out-of-vocabulary (OOV) words in NLP models.",
        "Out-of-vocabulary words pose a challenge in NLP. To handle them, I use subword tokenization methods like byte pair encoding or WordPiece, which break down words into smaller, recognizable units. This approach allows the model to understand and generate embeddings for previously unseen words.Another strategy is using embeddings like FastText, which can create embeddings for OOV words by leveraging character n-grams."
    ],
    [
        "Can you explain what tokenization is in NLP and why it is important.",
        "Tokenization is the process of converting text into smaller units called tokens, which can be words, subwords, or characters. This step is fundamental in NLP because it enables models to work with manageable and meaningful units of language. For instance, word tokenization breaks down sentences into individual words, while subword tokenization can handle rare or complex words more effectively, improving model performance."
    ],
    [
        "What is LingoNet?",
        "LingoNet is a lightweight transformer‑based language model designed specifically for on‑device text autocompletion on mobile. During training, LingoNet was fine‑tuned on a multilingual corpus spanning English, Spanish, and Mandarin to support cross‑language suggestions."
    ],
    [
        "How does contrastive augmentation improve LingoNet?",
        "During cross‑lingual fine‑tuning on aligned English, Spanish, and Mandarin corpora, LingoNet integrates contrastive augmentation to align semantic representations across languages, yielding a 12% lift in multilingual next‑token accuracy compared to equally‑sized non‑augmented transformers."
    ],
    [
        "What efficiency techniques does LingoNet use?",
        "During cross‑lingual fine‑tuning on aligned English, Spanish, and Mandarin corpora, LingoNet integrates contrastive augmentation to align semantic representations across languages, yielding a 12% lift in multilingual next‑token accuracy compared to equally‑sized non‑augmented transformers."
    ],
    [
        "What is LingoPer?",
        "It is similar to LingoNet but additionally fine‑tuned on Persian language data."
    ]
]